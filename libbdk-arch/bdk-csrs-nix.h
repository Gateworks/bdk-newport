#ifndef __BDK_CSRS_NIX_H__
#define __BDK_CSRS_NIX_H__
/* This file is auto-generated. Do not edit */

/***********************license start***************
 * Copyright (c) 2003-2017  Cavium Inc. (support@cavium.com). All rights
 * reserved.
 *
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met:
 *
 *   * Redistributions of source code must retain the above copyright
 *     notice, this list of conditions and the following disclaimer.
 *
 *   * Redistributions in binary form must reproduce the above
 *     copyright notice, this list of conditions and the following
 *     disclaimer in the documentation and/or other materials provided
 *     with the distribution.

 *   * Neither the name of Cavium Inc. nor the names of
 *     its contributors may be used to endorse or promote products
 *     derived from this software without specific prior written
 *     permission.

 * This Software, including technical data, may be subject to U.S. export  control
 * laws, including the U.S. Export Administration Act and its  associated
 * regulations, and may be subject to export or import  regulations in other
 * countries.

 * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
 * AND WITH ALL FAULTS AND CAVIUM  NETWORKS MAKES NO PROMISES, REPRESENTATIONS OR
 * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
 * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
 * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
 * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
 * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
 * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
 * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
 * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
 ***********************license end**************************************/


/**
 * @file
 *
 * Configuration and status register (CSR) address and type definitions for
 * Cavium NIX.
 *
 * This file is auto generated. Do not edit.
 *
 */

/**
 * Enumeration nix_af_int_vec_e
 *
 * NIX Admin Function Interrupt Vector Enumeration
 * Enumerates the NIX AF MSI-X interrupt vectors.
 */
#define BDK_NIX_AF_INT_VEC_E_AF_ERR (3)
#define BDK_NIX_AF_INT_VEC_E_AQ_DONE (2)
#define BDK_NIX_AF_INT_VEC_E_GEN (1)
#define BDK_NIX_AF_INT_VEC_E_POISON (4)
#define BDK_NIX_AF_INT_VEC_E_RVU (0)

/**
 * Enumeration nix_aq_comp_e
 *
 * NIX Completion Enumeration
 * Enumerates the values of NIX_AQ_RES_S[COMPCODE].
 */
#define BDK_NIX_AQ_COMP_E_CTX_FAULT (4)
#define BDK_NIX_AQ_COMP_E_CTX_POISON (3)
#define BDK_NIX_AQ_COMP_E_GOOD (1)
#define BDK_NIX_AQ_COMP_E_LOCKERR (5)
#define BDK_NIX_AQ_COMP_E_NOTDONE (0)
#define BDK_NIX_AQ_COMP_E_SQB_ALLOC_FAIL (6)
#define BDK_NIX_AQ_COMP_E_SWERR (2)

/**
 * Enumeration nix_aq_ctype_e
 *
 * NIX Context Type Enumeration
 * Enumerates NIX_AQ_INST_S[CTYPE] values.
 */
#define BDK_NIX_AQ_CTYPE_E_CQ (2)
#define BDK_NIX_AQ_CTYPE_E_DYNO (5)
#define BDK_NIX_AQ_CTYPE_E_MCE (3)
#define BDK_NIX_AQ_CTYPE_E_RQ (0)
#define BDK_NIX_AQ_CTYPE_E_RSS (4)
#define BDK_NIX_AQ_CTYPE_E_SQ (1)

/**
 * Enumeration nix_aq_instop_e
 *
 * NIX Admin Queue Opcode Enumeration
 * Enumerates NIX_AQ_INST_S[OP] values.
 */
#define BDK_NIX_AQ_INSTOP_E_INIT (1)
#define BDK_NIX_AQ_INSTOP_E_LOCK (4)
#define BDK_NIX_AQ_INSTOP_E_NOP (0)
#define BDK_NIX_AQ_INSTOP_E_READ (3)
#define BDK_NIX_AQ_INSTOP_E_UNLOCK (5)
#define BDK_NIX_AQ_INSTOP_E_WRITE (2)

/**
 * Enumeration nix_chan_e
 *
 * NIX Channel Number Enumeration
 * Enumerates the receive and transmit channels, and values of
 * NIX_RX_PARSE_S[CHAN], NIX_SQ_CTX_S[DEFAULT_CHAN]. CNXXXX implements a subset of
 * these channels. Specifically, only channels for links enumerated by NIX_LINK_E
 * are implemented.
 *
 * Internal:
 * P2X/X2P channel enumeration for t9x.
 */
#define BDK_NIX_CHAN_E_CGXX_LMACX_CHX(a,b,c) (0x800 + 0x100 * (a) + 0x10 * (b) + (c))
#define BDK_NIX_CHAN_E_LBKX_CHX(a,b) (0 + 0x100 * (a) + (b))
#define BDK_NIX_CHAN_E_RX(a) (0 + 0x100 * (a))
#define BDK_NIX_CHAN_E_SDP_CHX(a) (0x700 + (a))

/**
 * Enumeration nix_colorresult_e
 *
 * NIX Color Result Enumeration
 * Enumerates the values of NIX_MEM_RESULT_S[COLOR], NIX_AF_TL1()_MD_DEBUG1[COLOR]
 * and NIX_AF_TL1()_MD_DEBUG2[COLOR].
 */
#define BDK_NIX_COLORRESULT_E_GREEN (0)
#define BDK_NIX_COLORRESULT_E_RED_DROP (3)
#define BDK_NIX_COLORRESULT_E_RED_SEND (2)
#define BDK_NIX_COLORRESULT_E_YELLOW (1)

/**
 * Enumeration nix_cqerrint_e
 *
 * NIX Completion Queue Interrupt Enumeration
 * Enumerates the bit index of NIX_CQ_CTX_S[CQ_ERR_INT,CQ_ERR_INT_ENA].
 */
#define BDK_NIX_CQERRINT_E_CQE_FAULT (2)
#define BDK_NIX_CQERRINT_E_DOOR_ERR (0)
#define BDK_NIX_CQERRINT_E_WR_FULL (1)

/**
 * Enumeration nix_lf_int_vec_e
 *
 * NIX Local Function Interrupt Vector Enumeration
 * Enumerates the NIX MSI-X interrupt vectors per LF.
 */
#define BDK_NIX_LF_INT_VEC_E_CINTX(a) (0x40 + (a))
#define BDK_NIX_LF_INT_VEC_E_ERR_INT (0x81)
#define BDK_NIX_LF_INT_VEC_E_GINT (0x80)
#define BDK_NIX_LF_INT_VEC_E_POISON (0x82)
#define BDK_NIX_LF_INT_VEC_E_QINTX(a) (0 + (a))

/**
 * Enumeration nix_link_e
 *
 * NIX Link Number Enumeration
 * Enumerates the receive and transmit links, and index {a} (LINK) of
 * NIX_AF_RX_LINK()_SL()_SPKT_CNT, NIX_AF_RX_LINK()_SL()_SXQE_CNT,
 * NIX_AF_TX_LINK()_NORM_CREDIT, NIX_AF_TX_LINK()_EXPR_CREDIT,
 * NIX_AF_TX_LINK()_HW_XOFF and NIX_AF_TX_LINK()_SW_XOFF.
 */
#define BDK_NIX_LINK_E_CGXX_LMACX(a,b) (0 + 4 * (a) + (b))
#define BDK_NIX_LINK_E_LBKX(a) (0xc + (a))
#define BDK_NIX_LINK_E_SDP (0xd)

/**
 * Enumeration nix_lsoalg_e
 *
 * NIX Large Send Offload Algorithm Enumeration
 * Enumerates NIX_AF_LSO_FORMAT()_FIELD()[ALG] values. Specifies algorithm for
 * modifying the associated LSO packet field.
 */
#define BDK_NIX_LSOALG_E_ADD_OFFSET (3)
#define BDK_NIX_LSOALG_E_ADD_PAYLEN (2)
#define BDK_NIX_LSOALG_E_ADD_SEGNUM (1)
#define BDK_NIX_LSOALG_E_NOP (0)
#define BDK_NIX_LSOALG_E_TCP_FLAGS (4)

/**
 * Enumeration nix_maxsqesz_e
 *
 * NIX Maximum SQE Size Enumeration
 * Enumerates the values of NIX_SQ_CTX_S[MAX_SQE_SIZE].
 */
#define BDK_NIX_MAXSQESZ_E_W16 (0)
#define BDK_NIX_MAXSQESZ_E_W8 (1)

/**
 * Enumeration nix_mdtype_e
 *
 * NIX Meta Descriptor Type Enumeration
 * Enumerates values of NIX_AF_MDQ()_MD_DEBUG[MD_TYPE].
 */
#define BDK_NIX_MDTYPE_E_FLUSH (1)
#define BDK_NIX_MDTYPE_E_PMD (2)
#define BDK_NIX_MDTYPE_E_RSVD (0)

/**
 * Enumeration nix_mnqerr_e
 *
 * NIX Meta-Descriptor Enqueue Error Enumeration
 * Enumerates NIX_LF_MNQ_ERR_DBG[ERRCODE] values.
 */
#define BDK_NIX_MNQERR_E_LSO_ERR (5)
#define BDK_NIX_MNQERR_E_SQB_FAULT (2)
#define BDK_NIX_MNQERR_E_SQB_POISON (3)
#define BDK_NIX_MNQERR_E_SQ_CTX_FAULT (0)
#define BDK_NIX_MNQERR_E_SQ_CTX_POISON (1)
#define BDK_NIX_MNQERR_E_TOTAL_ERR (4)

/**
 * Enumeration nix_re_opcode_e
 *
 * NIX Receive Error Opcode Enumeration
 * Enumerates NIX_RX_PARSE_S[ERRCODE] values when NIX_RX_PARSE_S[ERRLEV] =
 * NPC_ERRLEV_E::RE.
 */
#define BDK_NIX_RE_OPCODE_E_OL2_LENMISM (0x12)
#define BDK_NIX_RE_OPCODE_E_OVERSIZE (0x11)
#define BDK_NIX_RE_OPCODE_E_RE_DMAPKT (0xf)
#define BDK_NIX_RE_OPCODE_E_RE_FCS (7)
#define BDK_NIX_RE_OPCODE_E_RE_FCS_RCV (8)
#define BDK_NIX_RE_OPCODE_E_RE_JABBER (2)
#define BDK_NIX_RE_OPCODE_E_RE_NONE (0)
#define BDK_NIX_RE_OPCODE_E_RE_PARTIAL (1)
#define BDK_NIX_RE_OPCODE_E_RE_RX_CTL (0xb)
#define BDK_NIX_RE_OPCODE_E_RE_SKIP (0xc)
#define BDK_NIX_RE_OPCODE_E_RE_TERMINATE (9)
#define BDK_NIX_RE_OPCODE_E_UNDERSIZE (0x10)

/**
 * Enumeration nix_redalg_e
 *
 * NIX Red Algorithm Enumeration
 * Enumerates the different algorithms of NIX_SEND_EXT_S[SHP_RA].
 */
#define BDK_NIX_REDALG_E_DISCARD (3)
#define BDK_NIX_REDALG_E_SEND (1)
#define BDK_NIX_REDALG_E_STALL (2)
#define BDK_NIX_REDALG_E_STD (0)

/**
 * Enumeration nix_rqint_e
 *
 * NIX Receive Queue Interrupt Enumeration
 * Enumerates the bit index of NIX_RQ_CTX_S[RQ_INT,RQ_INT_ENA].
 */
#define BDK_NIX_RQINT_E_DROP (0)
#define BDK_NIX_RQINT_E_RED (1)
#define BDK_NIX_RQINT_E_WQE_ALLOC_FAIL (2)

/**
 * Enumeration nix_rx_actionop_e
 *
 * NIX Receive Action Opcode Enumeration
 * Enumerates the values of NIX_RX_ACTION_S[OP].
 */
#define BDK_NIX_RX_ACTIONOP_E_DROP (0)
#define BDK_NIX_RX_ACTIONOP_E_MCAST (3)
#define BDK_NIX_RX_ACTIONOP_E_MIRROR (6)
#define BDK_NIX_RX_ACTIONOP_E_PF_FUNC_DROP (5)
#define BDK_NIX_RX_ACTIONOP_E_RSS (4)
#define BDK_NIX_RX_ACTIONOP_E_UCAST (1)
#define BDK_NIX_RX_ACTIONOP_E_UCAST_IPSEC (2)

/**
 * Enumeration nix_rx_mcop_e
 *
 * NIX Receive Multicast/Mirror Opcode Enumeration
 * Enumerates the values of NIX_RX_MCE_S[OP].
 */
#define BDK_NIX_RX_MCOP_E_RQ (0)
#define BDK_NIX_RX_MCOP_E_RSS (1)

/**
 * Enumeration nix_rx_perrcode_e
 *
 * NIX Receive Protocol Error Code Enumeration
 * Enumerates NIX_RX_PARSE_S[ERRCODE] values when NIX_RX_PARSE_S[ERRLEV] =
 * NPC_ERRLEV_E::NIX.
 */
#define BDK_NIX_RX_PERRCODE_E_BUFS_OFLOW (0xa)
#define BDK_NIX_RX_PERRCODE_E_DATA_FAULT (8)
#define BDK_NIX_RX_PERRCODE_E_IL3_LEN (0x20)
#define BDK_NIX_RX_PERRCODE_E_IL4_CHK (0x22)
#define BDK_NIX_RX_PERRCODE_E_IL4_LEN (0x21)
#define BDK_NIX_RX_PERRCODE_E_IL4_PORT (0x23)
#define BDK_NIX_RX_PERRCODE_E_MCAST_FAULT (4)
#define BDK_NIX_RX_PERRCODE_E_MCAST_POISON (6)
#define BDK_NIX_RX_PERRCODE_E_MEMOUT (9)
#define BDK_NIX_RX_PERRCODE_E_MIRROR_FAULT (5)
#define BDK_NIX_RX_PERRCODE_E_MIRROR_POISON (7)
#define BDK_NIX_RX_PERRCODE_E_NPC_RESULT_ERR (2)
#define BDK_NIX_RX_PERRCODE_E_OL3_LEN (0x10)
#define BDK_NIX_RX_PERRCODE_E_OL4_CHK (0x12)
#define BDK_NIX_RX_PERRCODE_E_OL4_LEN (0x11)
#define BDK_NIX_RX_PERRCODE_E_OL4_PORT (0x13)

/**
 * Enumeration nix_send_status_e
 *
 * NIX Send Completion Status Enumeration
 * Enumerates values of NIX_SEND_COMP_S[STATUS] and NIX_LF_SEND_ERR_DBG[ERRCODE].
 * Internal:
 * TODO.
 */
#define BDK_NIX_SEND_STATUS_E_DATA_FAULT (0x16)
#define BDK_NIX_SEND_STATUS_E_DATA_POISON (0x17)
#define BDK_NIX_SEND_STATUS_E_GOOD (0)
#define BDK_NIX_SEND_STATUS_E_INVALID_SUBDC (0x14)
#define BDK_NIX_SEND_STATUS_E_JUMP_FAULT (7)
#define BDK_NIX_SEND_STATUS_E_JUMP_POISON (8)
#define BDK_NIX_SEND_STATUS_E_LOCK_VIOL (0x21)
#define BDK_NIX_SEND_STATUS_E_NPC_DROP_ACTION (0x20)
#define BDK_NIX_SEND_STATUS_E_NPC_MCAST_ABORT (0x24)
#define BDK_NIX_SEND_STATUS_E_NPC_MCAST_CHAN_ERR (0x23)
#define BDK_NIX_SEND_STATUS_E_NPC_UCAST_CHAN_ERR (0x22)
#define BDK_NIX_SEND_STATUS_E_NPC_VTAG_PTR_ERR (0x25)
#define BDK_NIX_SEND_STATUS_E_NPC_VTAG_SIZE_ERR (0x26)
#define BDK_NIX_SEND_STATUS_E_SEND_CRC_ERR (0x10)
#define BDK_NIX_SEND_STATUS_E_SEND_EXT_ERR (6)
#define BDK_NIX_SEND_STATUS_E_SEND_HDR_ERR (5)
#define BDK_NIX_SEND_STATUS_E_SEND_IMM_ERR (0x11)
#define BDK_NIX_SEND_STATUS_E_SEND_MEM_ERR (0x13)
#define BDK_NIX_SEND_STATUS_E_SEND_MEM_FAULT (0x27)
#define BDK_NIX_SEND_STATUS_E_SEND_SG_ERR (0x12)
#define BDK_NIX_SEND_STATUS_E_SQB_FAULT (3)
#define BDK_NIX_SEND_STATUS_E_SQB_POISON (4)
#define BDK_NIX_SEND_STATUS_E_SQ_CTX_FAULT (1)
#define BDK_NIX_SEND_STATUS_E_SQ_CTX_POISON (2)
#define BDK_NIX_SEND_STATUS_E_SUBDC_ORDER_ERR (0x15)

/**
 * Enumeration nix_sendcrcalg_e
 *
 * NIX Send CRC Algorithm Enumeration
 * Enumerates the CRC algorithm used, see NIX_SEND_CRC_S[ALG].
 */
#define BDK_NIX_SENDCRCALG_E_CRC32 (0)
#define BDK_NIX_SENDCRCALG_E_CRC32C (1)
#define BDK_NIX_SENDCRCALG_E_ONES16 (2)

/**
 * Enumeration nix_sendl3type_e
 *
 * NIX Send Layer 3 Header Type Enumeration
 * Enumerates values of NIX_SEND_HDR_S[OL3TYPE], NIX_SEND_HDR_S[IL3TYPE].
 * Internal:
 * Encoding matches DPDK TX IP types:
 * \<pre\>
 * PKT_TX_IP_CKSUM      (1ULL \<\< 54)
 * PKT_TX_IPV4          (1ULL \<\< 55)
 * PKT_TX_IPV6          (1ULL \<\< 56)
 *
 * PKT_TX_OUTER_IP_CKSUM(1ULL \<\< 58)
 * PKT_TX_OUTER_IPV4    (1ULL \<\< 59)
 * PKT_TX_OUTER_IPV6    (1ULL \<\< 60)
 * \</pre\>
 */
#define BDK_NIX_SENDL3TYPE_E_IP4 (2)
#define BDK_NIX_SENDL3TYPE_E_IP4_CKSUM (3)
#define BDK_NIX_SENDL3TYPE_E_IP6 (4)
#define BDK_NIX_SENDL3TYPE_E_NONE (0)

/**
 * Enumeration nix_sendl4type_e
 *
 * NIX Send Layer 4 Header Type Enumeration
 * Enumerates values of NIX_SEND_HDR_S[OL4TYPE], NIX_SEND_HDR_S[IL4TYPE].
 * Internal:
 * Encoding matches DPDK TX L4 types.
 * \<pre\>
 * PKT_TX_L4_NO_CKSUM   (0ULL \<\< 52)  // Disable L4 cksum of TX pkt.
 * PKT_TX_TCP_CKSUM     (1ULL \<\< 52)  // TCP cksum of TX pkt. computed by nic.
 * PKT_TX_SCTP_CKSUM    (2ULL \<\< 52)  // SCTP cksum of TX pkt. computed by nic.
 * PKT_TX_UDP_CKSUM     (3ULL \<\< 52)  // UDP cksum of TX pkt. computed by nic.
 * \</pre\>
 */
#define BDK_NIX_SENDL4TYPE_E_NONE (0)
#define BDK_NIX_SENDL4TYPE_E_SCTP_CKSUM (2)
#define BDK_NIX_SENDL4TYPE_E_TCP_CKSUM (1)
#define BDK_NIX_SENDL4TYPE_E_UDP_CKSUM (3)

/**
 * Enumeration nix_sendldtype_e
 *
 * NIX Send Load Type Enumeration
 * Enumerates the load transaction types for reading segment bytes specified by
 * NIX_SEND_SG_S[LD_TYPE] and NIX_SEND_JUMP_S[LD_TYPE].
 */
#define BDK_NIX_SENDLDTYPE_E_LDD (0)
#define BDK_NIX_SENDLDTYPE_E_LDT (1)
#define BDK_NIX_SENDLDTYPE_E_LDWB (2)

/**
 * Enumeration nix_sendmemalg_e
 *
 * NIX Memory Modify Algorithm Enumeration
 * Enumerates the different algorithms for modifying memory; see
 * NIX_SEND_MEM_S[ALG]. mbufs_freed is the number of gather buffers freed to NPA
 * for the send descriptor. See NIX_SEND_HDR_S[DF] and NIX_SEND_SG_S[I].
 */
#define BDK_NIX_SENDMEMALG_E_ADD (8)
#define BDK_NIX_SENDMEMALG_E_ADDLEN (0xa)
#define BDK_NIX_SENDMEMALG_E_ADDMBUF (0xc)
#define BDK_NIX_SENDMEMALG_E_SET (0)
#define BDK_NIX_SENDMEMALG_E_SETRSLT (2)
#define BDK_NIX_SENDMEMALG_E_SETTSTMP (1)
#define BDK_NIX_SENDMEMALG_E_SUB (9)
#define BDK_NIX_SENDMEMALG_E_SUBLEN (0xb)
#define BDK_NIX_SENDMEMALG_E_SUBMBUF (0xd)

/**
 * Enumeration nix_sendmemdsz_e
 *
 * NIX Memory Data Size Enumeration
 * Enumerates the datum size for modifying memory; see NIX_SEND_MEM_S[DSZ].
 */
#define BDK_NIX_SENDMEMDSZ_E_B16 (2)
#define BDK_NIX_SENDMEMDSZ_E_B32 (1)
#define BDK_NIX_SENDMEMDSZ_E_B64 (0)
#define BDK_NIX_SENDMEMDSZ_E_B8 (3)

/**
 * Enumeration nix_sqint_e
 *
 * NIX Send Queue Interrupt Enumeration
 * Enumerates the bit index of NIX_SQ_CTX_S[SQ_INT,SQ_INT_ENA].
 */
#define BDK_NIX_SQINT_E_LMT_ERR (0)
#define BDK_NIX_SQINT_E_MNQ_ERR (1)
#define BDK_NIX_SQINT_E_SEND_ERR (2)
#define BDK_NIX_SQINT_E_SQB_ALLOC_FAIL (3)

/**
 * Enumeration nix_sqoperr_e
 *
 * NIX SQ Operation Error Enumeration
 * Enumerates NIX_LF_SQ_OP_ERR_DBG[ERRCODE] values.
 */
#define BDK_NIX_SQOPERR_E_MAX_SQE_SIZE_ERR (4)
#define BDK_NIX_SQOPERR_E_SQB_FAULT (7)
#define BDK_NIX_SQOPERR_E_SQB_NULL (6)
#define BDK_NIX_SQOPERR_E_SQE_OFLOW (5)
#define BDK_NIX_SQOPERR_E_SQ_CTX_FAULT (1)
#define BDK_NIX_SQOPERR_E_SQ_CTX_POISON (2)
#define BDK_NIX_SQOPERR_E_SQ_DISABLED (3)
#define BDK_NIX_SQOPERR_E_SQ_OOR (0)

/**
 * Enumeration nix_stat_lf_rx_e
 *
 * NIX Local Function Receive Statistics Enumeration
 * Enumerates the last index of NIX_AF_LF()_RX_STAT() and NIX_LF_RX_STAT().
 */
#define BDK_NIX_STAT_LF_RX_E_RX_BCAST (2)
#define BDK_NIX_STAT_LF_RX_E_RX_DROP (4)
#define BDK_NIX_STAT_LF_RX_E_RX_DROP_OCTS (5)
#define BDK_NIX_STAT_LF_RX_E_RX_DRP_BCAST (8)
#define BDK_NIX_STAT_LF_RX_E_RX_DRP_L3BCAST (0xa)
#define BDK_NIX_STAT_LF_RX_E_RX_DRP_L3MCAST (0xb)
#define BDK_NIX_STAT_LF_RX_E_RX_DRP_MCAST (9)
#define BDK_NIX_STAT_LF_RX_E_RX_ERR (7)
#define BDK_NIX_STAT_LF_RX_E_RX_FCS (6)
#define BDK_NIX_STAT_LF_RX_E_RX_MCAST (3)
#define BDK_NIX_STAT_LF_RX_E_RX_OCTS (0)
#define BDK_NIX_STAT_LF_RX_E_RX_UCAST (1)

/**
 * Enumeration nix_stat_lf_tx_e
 *
 * NIX Local Function Transmit Statistics Enumeration
 * Enumerates the index of NIX_AF_LF()_TX_STAT() and NIX_LF_TX_STAT().
 */
#define BDK_NIX_STAT_LF_TX_E_TX_BCAST (1)
#define BDK_NIX_STAT_LF_TX_E_TX_DROP (3)
#define BDK_NIX_STAT_LF_TX_E_TX_MCAST (2)
#define BDK_NIX_STAT_LF_TX_E_TX_OCTS (4)
#define BDK_NIX_STAT_LF_TX_E_TX_UCAST (0)

/**
 * Enumeration nix_stype_e
 *
 * NIX SQB Caching Type Enumeration
 * Enumerates the values of NIX_SQ_CTX_S[SQE_STYPE].
 */
#define BDK_NIX_STYPE_E_STF (0)
#define BDK_NIX_STYPE_E_STP (2)
#define BDK_NIX_STYPE_E_STT (1)

/**
 * Enumeration nix_subdc_e
 *
 * NIX Subdescriptor Operation Enumeration
 * Enumerates send and receive subdescriptor codes. The codes differentiate
 * subdescriptors within a NIX send or receive descriptor, excluding NIX_SEND_HDR_S for
 * send and NIX_CQE_HDR_S/NIX_WQE_HDR_S for receive, which are determined by their
 * position as the first subdescriptor, and NIX_RX_PARSE_S, which is determined by its
 * position as the second subdescriptor.
 */
#define BDK_NIX_SUBDC_E_CRC (2)
#define BDK_NIX_SUBDC_E_EXT (1)
#define BDK_NIX_SUBDC_E_IMM (3)
#define BDK_NIX_SUBDC_E_JUMP (6)
#define BDK_NIX_SUBDC_E_MEM (5)
#define BDK_NIX_SUBDC_E_SG (4)
#define BDK_NIX_SUBDC_E_WORK (7)

/**
 * Enumeration nix_tx_actionop_e
 *
 * NIX Transmit Action Opcode Enumeration
 * Enumerates the values of NIX_TX_ACTION_S[OP].
 */
#define BDK_NIX_TX_ACTIONOP_E_DROP (0)
#define BDK_NIX_TX_ACTIONOP_E_DROP_VIOL (5)
#define BDK_NIX_TX_ACTIONOP_E_MCAST (3)
#define BDK_NIX_TX_ACTIONOP_E_UCAST_CHAN (2)
#define BDK_NIX_TX_ACTIONOP_E_UCAST_DEFAULT (1)

/**
 * Enumeration nix_tx_vtagop_e
 *
 * NIX Transmit Vtag Opcode Enumeration
 * Enumerates the values of NIX_TX_VTAG_ACTION_S[VTAG0_OP,VTAG1_OP].
 */
#define BDK_NIX_TX_VTAGOP_E_INSERT (1)
#define BDK_NIX_TX_VTAGOP_E_NOP (0)
#define BDK_NIX_TX_VTAGOP_E_REPLACE (2)

/**
 * Enumeration nix_txlayer_e
 *
 * NIX Transmit Layer Enumeration
 * Enumerates the values of NIX_AF_LSO_FORMAT()_FIELD()[LAYER].
 */
#define BDK_NIX_TXLAYER_E_IL3 (2)
#define BDK_NIX_TXLAYER_E_IL4 (3)
#define BDK_NIX_TXLAYER_E_OL3 (0)
#define BDK_NIX_TXLAYER_E_OL4 (1)

/**
 * Enumeration nix_vtagsize_e
 *
 * NIX Vtag Size Enumeration
 * Enumerates the values of NIX_AF_TX_VTAG_DEF()_CTL[SIZE] and NIX_AF_LF()_RX_VTAG_TYPE()[SIZE].
 */
#define BDK_NIX_VTAGSIZE_E_T4 (0)
#define BDK_NIX_VTAGSIZE_E_T8 (1)

/**
 * Enumeration nix_xqe_type_e
 *
 * NIX WQE/CQE Type Enumeration
 * Enumerates the values of NIX_WQE_HDR_S[WQE_TYPE], NIX_CQE_HDR_S[CQE_TYPE].
 */
#define BDK_NIX_XQE_TYPE_E_INVALID (0)
#define BDK_NIX_XQE_TYPE_E_RX (1)
#define BDK_NIX_XQE_TYPE_E_RX_IPSECD (4)
#define BDK_NIX_XQE_TYPE_E_RX_IPSECH (3)
#define BDK_NIX_XQE_TYPE_E_RX_IPSECS (2)
#define BDK_NIX_XQE_TYPE_E_SEND (8)

/**
 * Enumeration nix_xqesz_e
 *
 * NIX WQE/CQE Size Enumeration
 * Enumerates the values of NIX_AF_LF()_CFG[XQE_SIZE].
 */
#define BDK_NIX_XQESZ_E_W16 (1)
#define BDK_NIX_XQESZ_E_W64 (0)

/**
 * Structure nix_aq_inst_s
 *
 * NIX Admin Queue Instruction Structure
 * This structure specifies the AQ instruction.
 * Instructions and associated structures are stored in memory as little-endian unless
 * NIX_AF_CFG[AF_BE] is set.
 *
 * Hardware reads of NIX_AQ_INST_S do not allocate into LLC.
 *
 * Hardware reads and writes of the context structure selected by [CTYPE], [LF]
 * and [CINDEX] use the NDC and LLC caching style configured for that context. For
 * example:
 * * When [CTYPE] = NIX_AQ_CTYPE_E::RQ: use NIX_AF_LF()_RQS_CFG[CACHING] and
 * NIX_AF_LF()_RQS_CFG[WAY_MASK].
 * * When [CTYPE] = NIX_AQ_CTYPE_E::MCE: use NIX_AF_RX_MCAST_CFG[CACHING] and
 * NIX_AF_RX_MCAST_CFG[WAY_MASK].
 */
union bdk_nix_aq_inst_s
{
    uint64_t u[2];
    struct bdk_nix_aq_inst_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t doneint               : 1;  /**< [ 63: 63] Done interrupt.
                                                                 0 = No interrupts related to this instruction.
                                                                 1 = When the instruction completes, NIX_AF_AQ_DONE[DONE] will be incremented,
                                                                 and based on the rules described there, an interrupt may occur. */
        uint64_t reserved_44_62        : 19;
        uint64_t cindex                : 20; /**< [ 43: 24] Context index. Index of context of type [CTYPE] within [LF]. For example,
                                                                 if [CTYPE] = NIX_AQ_CTYPE_E::RQ, this is the RQ index within the [LF].

                                                                 If [CTYPE] = NIX_AQ_CTYPE_E::DYNO, index to array of
                                                                 1 \<\< NIX_AF_CONST3[DYNO_ARRAY_LOG2COUNTERS]
                                                                 IPSEC dynamic ordering counters, starting at counter index
                                                                 (1 \<\< NIX_AF_CONST3[DYNO_ARRAY_LOG2COUNTERS]) * [CINDEX].
                                                                 See NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_ENA]. */
        uint64_t reserved_15_23        : 9;
        uint64_t lf                    : 7;  /**< [ 14:  8] Local function. Software must map the LF to a PF and function with
                                                                 NIX_PRIV_LF()_CFG[PF_FUNC] before issuing the AQ instruction.
                                                                 NIX_PRIV_LF()_CFG[ENA] is not required to be set when executing AQ
                                                                 instructions.

                                                                 Internal:
                                                                 Hardware uses PF(0)'s stream ID when accessing hardware context structures
                                                                 in LLC/DRAM, but NDC tracks the LF for context structures in its cache
                                                                 using the PF_FUNC's stream ID. */
        uint64_t ctype                 : 4;  /**< [  7:  4] Context type of instruction enumerated by NIX_AQ_CTYPE_E. */
        uint64_t op                    : 4;  /**< [  3:  0] Instruction op code enumerated by NIX_AQ_INSTOP_E. */
#else /* Word 0 - Little Endian */
        uint64_t op                    : 4;  /**< [  3:  0] Instruction op code enumerated by NIX_AQ_INSTOP_E. */
        uint64_t ctype                 : 4;  /**< [  7:  4] Context type of instruction enumerated by NIX_AQ_CTYPE_E. */
        uint64_t lf                    : 7;  /**< [ 14:  8] Local function. Software must map the LF to a PF and function with
                                                                 NIX_PRIV_LF()_CFG[PF_FUNC] before issuing the AQ instruction.
                                                                 NIX_PRIV_LF()_CFG[ENA] is not required to be set when executing AQ
                                                                 instructions.

                                                                 Internal:
                                                                 Hardware uses PF(0)'s stream ID when accessing hardware context structures
                                                                 in LLC/DRAM, but NDC tracks the LF for context structures in its cache
                                                                 using the PF_FUNC's stream ID. */
        uint64_t reserved_15_23        : 9;
        uint64_t cindex                : 20; /**< [ 43: 24] Context index. Index of context of type [CTYPE] within [LF]. For example,
                                                                 if [CTYPE] = NIX_AQ_CTYPE_E::RQ, this is the RQ index within the [LF].

                                                                 If [CTYPE] = NIX_AQ_CTYPE_E::DYNO, index to array of
                                                                 1 \<\< NIX_AF_CONST3[DYNO_ARRAY_LOG2COUNTERS]
                                                                 IPSEC dynamic ordering counters, starting at counter index
                                                                 (1 \<\< NIX_AF_CONST3[DYNO_ARRAY_LOG2COUNTERS]) * [CINDEX].
                                                                 See NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_ENA]. */
        uint64_t reserved_44_62        : 19;
        uint64_t doneint               : 1;  /**< [ 63: 63] Done interrupt.
                                                                 0 = No interrupts related to this instruction.
                                                                 1 = When the instruction completes, NIX_AF_AQ_DONE[DONE] will be incremented,
                                                                 and based on the rules described there, an interrupt may occur. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t res_addr              : 64; /**< [127: 64] Result IOVA. Specifies where to write NIX_AQ_RES_S.

                                                                 Bits \<3:0\> must be zero; address must be 16-byte aligned. Bits \<63:53\> are
                                                                 ignored by hardware; software should use a sign-extended bit \<52\> for forward
                                                                 compatibility.

                                                                 When [OP] = NIX_AQ_INSTOP_E::INIT, WRITE or READ,
                                                                 software must reserve space immediately after NIX_AQ_RES_S (starting at
                                                                 RVU PF(0)'S IOVA NIX_AQ_INST_S[RES_ADDR]+16) for one or two structures of the type
                                                                 selected by [CTYPE] that will be read of written by hardware. See
                                                                 NIX_AQ_INSTOP_E.

                                                                 Internal:
                                                                 Bits \<63:53\>, \<3:0\> are ignored by hardware, treated as always 0x0. */
#else /* Word 1 - Little Endian */
        uint64_t res_addr              : 64; /**< [127: 64] Result IOVA. Specifies where to write NIX_AQ_RES_S.

                                                                 Bits \<3:0\> must be zero; address must be 16-byte aligned. Bits \<63:53\> are
                                                                 ignored by hardware; software should use a sign-extended bit \<52\> for forward
                                                                 compatibility.

                                                                 When [OP] = NIX_AQ_INSTOP_E::INIT, WRITE or READ,
                                                                 software must reserve space immediately after NIX_AQ_RES_S (starting at
                                                                 RVU PF(0)'S IOVA NIX_AQ_INST_S[RES_ADDR]+16) for one or two structures of the type
                                                                 selected by [CTYPE] that will be read of written by hardware. See
                                                                 NIX_AQ_INSTOP_E.

                                                                 Internal:
                                                                 Bits \<63:53\>, \<3:0\> are ignored by hardware, treated as always 0x0. */
#endif /* Word 1 - End */
    } s;
    /* struct bdk_nix_aq_inst_s_s cn; */
};

/**
 * Structure nix_aq_res_s
 *
 * NIX Admin Queue Result Structure
 * NIX writes this structure after it completes the NIX_AQ_INST_S instruction.
 * The result structure is exactly 16 bytes, and each instruction completion produces
 * exactly one result structure.
 *
 * Results and associated structures are stored in memory as little-endian unless
 * NIX_AF_CFG[AF_BE] is set.
 *
 * When [OP] = NIX_AQ_INSTOP_E::INIT, WRITE or READ, this structure is
 * immediately followed by context read or write data. See NIX_AQ_INSTOP_E.
 *
 * Hardware writes of NIX_AQ_RES_S and context data always allocate into LLC.
 * Hardware reads of context data do not allocate into LLC.
 */
union bdk_nix_aq_res_s
{
    uint64_t u[2];
    struct bdk_nix_aq_res_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_17_63        : 47;
        uint64_t doneint               : 1;  /**< [ 16: 16] Done interrupt. This bit is copied from the corresponding instruction's
                                                                 NIX_AQ_INST_S[DONEINT]. */
        uint64_t compcode              : 8;  /**< [ 15:  8] Indicates completion/error status of the NIX coprocessor for the associated
                                                                 instruction, as enumerated by NIX_AQ_COMP_E. Core software may write the memory
                                                                 location containing [COMPCODE] to 0x0 before ringing the doorbell, and then poll
                                                                 for completion by checking for a nonzero value.

                                                                 Once the core observes a nonzero [COMPCODE] value in this case, NIX will have also
                                                                 completed LLC/DRAM reads and writes for the operation. */
        uint64_t ctype                 : 4;  /**< [  7:  4] Copy of NIX_AQ_INST_S[CTYPE] for the completed instruction; enumerated by
                                                                 NIX_AQ_CTYPE_E. */
        uint64_t op                    : 4;  /**< [  3:  0] Copy of NIX_AQ_INST_S[OP] for the completed instruction; enumerated by
                                                                 NIX_AQ_INSTOP_E. */
#else /* Word 0 - Little Endian */
        uint64_t op                    : 4;  /**< [  3:  0] Copy of NIX_AQ_INST_S[OP] for the completed instruction; enumerated by
                                                                 NIX_AQ_INSTOP_E. */
        uint64_t ctype                 : 4;  /**< [  7:  4] Copy of NIX_AQ_INST_S[CTYPE] for the completed instruction; enumerated by
                                                                 NIX_AQ_CTYPE_E. */
        uint64_t compcode              : 8;  /**< [ 15:  8] Indicates completion/error status of the NIX coprocessor for the associated
                                                                 instruction, as enumerated by NIX_AQ_COMP_E. Core software may write the memory
                                                                 location containing [COMPCODE] to 0x0 before ringing the doorbell, and then poll
                                                                 for completion by checking for a nonzero value.

                                                                 Once the core observes a nonzero [COMPCODE] value in this case, NIX will have also
                                                                 completed LLC/DRAM reads and writes for the operation. */
        uint64_t doneint               : 1;  /**< [ 16: 16] Done interrupt. This bit is copied from the corresponding instruction's
                                                                 NIX_AQ_INST_S[DONEINT]. */
        uint64_t reserved_17_63        : 47;
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t reserved_64_127       : 64;
#else /* Word 1 - Little Endian */
        uint64_t reserved_64_127       : 64;
#endif /* Word 1 - End */
    } s;
    /* struct bdk_nix_aq_res_s_s cn; */
};

/**
 * Structure nix_cint_hw_s
 *
 * NIX Completion Interrupt Context Hardware Structure
 * This structure contains context state maintained by hardware for each
 * completion interrupt (CINT) in NDC/LLC/DRAM. Software accesses this structure
 * with the NIX_LF_CINT()* registers.
 * Hardware maintains a table of NIX_AF_CONST2[CINTS] contiguous NIX_CINT_HW_S
 * structures per LF starting at IOVA NIX_AF_LF()_CINTS_BASE.
 */
union bdk_nix_cint_hw_s
{
    uint64_t u[2];
    struct bdk_nix_cint_hw_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_58_63        : 6;
        uint64_t timer_idx             : 8;  /**< [ 57: 50] Timer index of NIX_AF_CINT_TIMER(). Select the TIMER for the CINT. */
        uint64_t ena                   : 1;  /**< [ 49: 49] Interrupt enable. See also NIX_LF_CINT()_ENA_W1S and
                                                                 NIX_LF_CINT()_ENA_W1C. */
        uint64_t intr                  : 1;  /**< [ 48: 48] Interrupt status. See also NIX_LF_CINT()_INT and NIX_LF_CINT()_INT_W1S. */
        uint64_t qcount                : 16; /**< [ 47: 32] Active queue count. See NIX_LF_CINT()_CNT[QCOUNT]. */
        uint64_t ecount                : 32; /**< [ 31:  0] Entry count. See NIX_LF_CINT()_CNT[ECOUNT]. */
#else /* Word 0 - Little Endian */
        uint64_t ecount                : 32; /**< [ 31:  0] Entry count. See NIX_LF_CINT()_CNT[ECOUNT]. */
        uint64_t qcount                : 16; /**< [ 47: 32] Active queue count. See NIX_LF_CINT()_CNT[QCOUNT]. */
        uint64_t intr                  : 1;  /**< [ 48: 48] Interrupt status. See also NIX_LF_CINT()_INT and NIX_LF_CINT()_INT_W1S. */
        uint64_t ena                   : 1;  /**< [ 49: 49] Interrupt enable. See also NIX_LF_CINT()_ENA_W1S and
                                                                 NIX_LF_CINT()_ENA_W1C. */
        uint64_t timer_idx             : 8;  /**< [ 57: 50] Timer index of NIX_AF_CINT_TIMER(). Select the TIMER for the CINT. */
        uint64_t reserved_58_63        : 6;
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t reserved_120_127      : 8;
        uint64_t time_wait             : 8;  /**< [119:112] Time hold-off. See NIX_LF_CINT()_WAIT[TIME_WAIT]. */
        uint64_t qcount_wait           : 16; /**< [111: 96] Active queue count hold-off. See NIX_LF_CINT()_WAIT[QCOUNT_WAIT]. */
        uint64_t ecount_wait           : 32; /**< [ 95: 64] Entry count hold-off. See NIX_LF_CINT()_WAIT[ECOUNT_WAIT]. */
#else /* Word 1 - Little Endian */
        uint64_t ecount_wait           : 32; /**< [ 95: 64] Entry count hold-off. See NIX_LF_CINT()_WAIT[ECOUNT_WAIT]. */
        uint64_t qcount_wait           : 16; /**< [111: 96] Active queue count hold-off. See NIX_LF_CINT()_WAIT[QCOUNT_WAIT]. */
        uint64_t time_wait             : 8;  /**< [119:112] Time hold-off. See NIX_LF_CINT()_WAIT[TIME_WAIT]. */
        uint64_t reserved_120_127      : 8;
#endif /* Word 1 - End */
    } s;
    /* struct bdk_nix_cint_hw_s_s cn; */
};

/**
 * Structure nix_cq_ctx_s
 *
 * NIX Completion Queue Context Structure
 * This structure contains context state maintained by hardware for each CQ in
 * LLC/DRAM.
 * Software uses the same structure format to read and write an CQ context with
 * the NIX admin queue.
 */
union bdk_nix_cq_ctx_s
{
    uint64_t u[4];
    struct bdk_nix_cq_ctx_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t drop                  : 8;  /**< [ 63: 56] If [DROP_ENA] is set for a received packet, the packet will be
                                                                 dropped if the current 8-bit shifted count is equal to or greater than this
                                                                 value. */
        uint64_t reserved_53_55        : 3;
        uint64_t avg_con               : 9;  /**< [ 52: 44] This value controls how much of the present average resource level is used
                                                                 to calculate the new resource level. The value is a number from zero to 256,
                                                                 which represents [AVG_CON]/256 of the average resource level that will be
                                                                 used in the calculation.

                                                                 NIX updates the average resource level as follows whenever the immediate resource
                                                                 count changes:

                                                                 \<pre\>
                                                                 // Shifted 8-bit count (1/256 units of queue size); higher count indicates
                                                                 // more free resources:
                                                                 if ([QSIZE] \>= 2) {
                                                                   shifted_CNT = 255 - ((([TAIL] - [HEAD]) \>\> (2 * ([QSIZE] - 2))) % 256);
                                                                 } else {
                                                                   shifted_CNT = 255 - ((([TAIL] - [HEAD]) \<\< (2 * (2 - [QSIZE]))) % 256);
                                                                 }
                                                                 adjusted_CON = [AVG_CON] \>\> ceil(log2(NIX_AF_AVG_DELAY[AVG_TIMER] - [UPDATE_TIME]));
                                                                 [AVG_LEVEL] = (adjusted_CON * [AVG_LEVEL] + (256 - adjusted_CON)
                                                                               * shifted_CNT) / 256;
                                                                 [UPDATE_TIME] = NIX_AF_AVG_DELAY[AVG_TIMER];
                                                                 \</pre\>

                                                                 Note setting this value to zero will disable averaging, and always use the most
                                                                 immediate levels. NIX_AF_AVG_DELAY[AVG_DLY] controls the periodicity of the level
                                                                 calculations. */
        uint64_t reserved_43           : 1;
        uint64_t drop_ena              : 1;  /**< [ 42: 42] Enable RQ packet DROP based on the [DROP] level. */
        uint64_t cq_err                : 1;  /**< [ 41: 41] CQ error. Set along with the NIX_CQERRINT_E::WR_FULL or
                                                                 NIX_CQERRINT_E::CQE_FAULT bit in [CQ_ERR_INT] when the corresponding error
                                                                 is detected. The CQ is stopped and all new CQEs to be added to it are
                                                                 dropped. */
        uint64_t ena                   : 1;  /**< [ 40: 40] CQ enable. */
        uint64_t substream             : 20; /**< [ 39: 20] Substream ID used for writing CQEs to the CQ ring. */
        uint64_t wrptr                 : 20; /**< [ 19:  0] Internal pointer for writing to the CQ ring. */
#else /* Word 0 - Little Endian */
        uint64_t wrptr                 : 20; /**< [ 19:  0] Internal pointer for writing to the CQ ring. */
        uint64_t substream             : 20; /**< [ 39: 20] Substream ID used for writing CQEs to the CQ ring. */
        uint64_t ena                   : 1;  /**< [ 40: 40] CQ enable. */
        uint64_t cq_err                : 1;  /**< [ 41: 41] CQ error. Set along with the NIX_CQERRINT_E::WR_FULL or
                                                                 NIX_CQERRINT_E::CQE_FAULT bit in [CQ_ERR_INT] when the corresponding error
                                                                 is detected. The CQ is stopped and all new CQEs to be added to it are
                                                                 dropped. */
        uint64_t drop_ena              : 1;  /**< [ 42: 42] Enable RQ packet DROP based on the [DROP] level. */
        uint64_t reserved_43           : 1;
        uint64_t avg_con               : 9;  /**< [ 52: 44] This value controls how much of the present average resource level is used
                                                                 to calculate the new resource level. The value is a number from zero to 256,
                                                                 which represents [AVG_CON]/256 of the average resource level that will be
                                                                 used in the calculation.

                                                                 NIX updates the average resource level as follows whenever the immediate resource
                                                                 count changes:

                                                                 \<pre\>
                                                                 // Shifted 8-bit count (1/256 units of queue size); higher count indicates
                                                                 // more free resources:
                                                                 if ([QSIZE] \>= 2) {
                                                                   shifted_CNT = 255 - ((([TAIL] - [HEAD]) \>\> (2 * ([QSIZE] - 2))) % 256);
                                                                 } else {
                                                                   shifted_CNT = 255 - ((([TAIL] - [HEAD]) \<\< (2 * (2 - [QSIZE]))) % 256);
                                                                 }
                                                                 adjusted_CON = [AVG_CON] \>\> ceil(log2(NIX_AF_AVG_DELAY[AVG_TIMER] - [UPDATE_TIME]));
                                                                 [AVG_LEVEL] = (adjusted_CON * [AVG_LEVEL] + (256 - adjusted_CON)
                                                                               * shifted_CNT) / 256;
                                                                 [UPDATE_TIME] = NIX_AF_AVG_DELAY[AVG_TIMER];
                                                                 \</pre\>

                                                                 Note setting this value to zero will disable averaging, and always use the most
                                                                 immediate levels. NIX_AF_AVG_DELAY[AVG_DLY] controls the periodicity of the level
                                                                 calculations. */
        uint64_t reserved_53_55        : 3;
        uint64_t drop                  : 8;  /**< [ 63: 56] If [DROP_ENA] is set for a received packet, the packet will be
                                                                 dropped if the current 8-bit shifted count is equal to or greater than this
                                                                 value. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t bp_ena                : 1;  /**< [127:127] Enable CQ backpressure based on [BP] level. */
        uint64_t bp                    : 8;  /**< [126:119] Backpressure is asserted if [BP_ENA] bit is set and the current eight-bit
                                                                 shifted count is greater than or equal to this value. */
        uint64_t bpid                  : 9;  /**< [118:110] Backpressure ID (index {a} of NIX_AF_RX_BPID()_STATUS) to which
                                                                 backpressure is asserted when [BP_ENA] bit is set. */
        uint64_t state                 : 6;  /**< [109:104] CQ state (TBD). */
        uint64_t cq_err_int_ena        : 8;  /**< [103: 96] Error interrupt enables. Bits enumerated by NIX_CQERRINT_E. Software can read,
                                                                 set or clear these bits with NIX_LF_CQ_OP_INT. */
        uint64_t cq_err_int            : 8;  /**< [ 95: 88] Error interrupts. Bits enumerated by NIX_CQERRINT_E, which also defines when
                                                                 hardware sets each bit. Software can read, set or clear these bits with
                                                                 NIX_LF_CQ_OP_INT. */
        uint64_t update_time           : 16; /**< [ 87: 72] NIX_AF_AVG_DELAY[AVG_TIMER] value captured when [AVG_LEVEL] is updated. */
        uint64_t avg_level             : 8;  /**< [ 71: 64] Current moving average of the eight-bit shifted count. The higher [AVG_LEVEL]
                                                                 is, the more free resources. The lower levels indicate buffer exhaustion.
                                                                 See [AVG_CON].

                                                                 NIX uses [AVG_LEVEL] in receive queue QOS calculations. See
                                                                 NIX_RQ_CTX_S[XQE_DROP]. */
#else /* Word 1 - Little Endian */
        uint64_t avg_level             : 8;  /**< [ 71: 64] Current moving average of the eight-bit shifted count. The higher [AVG_LEVEL]
                                                                 is, the more free resources. The lower levels indicate buffer exhaustion.
                                                                 See [AVG_CON].

                                                                 NIX uses [AVG_LEVEL] in receive queue QOS calculations. See
                                                                 NIX_RQ_CTX_S[XQE_DROP]. */
        uint64_t update_time           : 16; /**< [ 87: 72] NIX_AF_AVG_DELAY[AVG_TIMER] value captured when [AVG_LEVEL] is updated. */
        uint64_t cq_err_int            : 8;  /**< [ 95: 88] Error interrupts. Bits enumerated by NIX_CQERRINT_E, which also defines when
                                                                 hardware sets each bit. Software can read, set or clear these bits with
                                                                 NIX_LF_CQ_OP_INT. */
        uint64_t cq_err_int_ena        : 8;  /**< [103: 96] Error interrupt enables. Bits enumerated by NIX_CQERRINT_E. Software can read,
                                                                 set or clear these bits with NIX_LF_CQ_OP_INT. */
        uint64_t state                 : 6;  /**< [109:104] CQ state (TBD). */
        uint64_t bpid                  : 9;  /**< [118:110] Backpressure ID (index {a} of NIX_AF_RX_BPID()_STATUS) to which
                                                                 backpressure is asserted when [BP_ENA] bit is set. */
        uint64_t bp                    : 8;  /**< [126:119] Backpressure is asserted if [BP_ENA] bit is set and the current eight-bit
                                                                 shifted count is greater than or equal to this value. */
        uint64_t bp_ena                : 1;  /**< [127:127] Enable CQ backpressure based on [BP] level. */
#endif /* Word 1 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 2 - Big Endian */
        uint64_t reserved_191          : 1;
        uint64_t cint_idx              : 7;  /**< [190:184] Completion interrupt index. Select the CINT within LF (index {a} of
                                                                 NIX_LF_CINT()*) which receives completion events for
                                                                 this CQ. */
        uint64_t reserved_183          : 1;
        uint64_t qint_idx              : 7;  /**< [182:176] Error queue interrupt index. Select the QINT within LF (index {a} of
                                                                 NIX_LF_QINT()*) which receives [CQ_ERR_INT] events. */
        uint64_t qsize                 : 4;  /**< [175:172] Specifies CQ ring size in number of CQEs:
                                                                 0x0 = 16 CQEs.
                                                                 0x1 = 64 CQEs.
                                                                 0x2 = 256 CQEs.
                                                                 0x3 = 1K CQEs.
                                                                 0x4 = 4K CQEs.
                                                                 0x5 = 16K CQEs.
                                                                 0x6 = 64K CQEs.
                                                                 0x7 = 256K CQEs.
                                                                 0x8 = 1M CQEs.
                                                                 0x9-0xF = Reserved.

                                                                 The CQE size is selected by NIX_AF_LF()_CFG[XQE_SIZE].

                                                                 Note that the usable size of the ring is the specified size minus one
                                                                 ([HEAD]==[TAIL] always means empty). */
        uint64_t reserved_169_171      : 3;
        uint64_t caching               : 1;  /**< [168:168] Selects the style of write to the LLC.
                                                                 0 = Writes of CQE data will not allocate into the LLC.
                                                                 1 = Writes of CQE data are allocated into the LLC. */
        uint64_t tail                  : 20; /**< [167:148] Tail CQE pointer.
                                                                 Hardware advances [TAIL] when an entry written to the CQ is committed and
                                                                 visible to software.
                                                                 The tail IOVA is [BASE] + ([TAIL] * 512) when NIX_AF_LF()_CFG[XQE_SIZE] = NIX_XQESZ_E::W64,
                                                                 [BASE] + ([TAIL] * 128) when NIX_AF_LF()_CFG[XQE_SIZE] = NIX_XQESZ_E::W16. */
        uint64_t head                  : 20; /**< [147:128] Head CQE pointer.
                                                                 Hardware advances [HEAD] when software writes to NIX_LF_CQ_OP_DOOR for
                                                                 this CQ.
                                                                 The head IOVA is [BASE] + ([HEAD] * 512) when NIX_AF_LF()_CFG[XQE_SIZE] = NIX_XQESZ_E::W64,
                                                                 [BASE] + ([HEAD] * 128) when NIX_AF_LF()_CFG[XQE_SIZE] = NIX_XQESZ_E::W16. */
#else /* Word 2 - Little Endian */
        uint64_t head                  : 20; /**< [147:128] Head CQE pointer.
                                                                 Hardware advances [HEAD] when software writes to NIX_LF_CQ_OP_DOOR for
                                                                 this CQ.
                                                                 The head IOVA is [BASE] + ([HEAD] * 512) when NIX_AF_LF()_CFG[XQE_SIZE] = NIX_XQESZ_E::W64,
                                                                 [BASE] + ([HEAD] * 128) when NIX_AF_LF()_CFG[XQE_SIZE] = NIX_XQESZ_E::W16. */
        uint64_t tail                  : 20; /**< [167:148] Tail CQE pointer.
                                                                 Hardware advances [TAIL] when an entry written to the CQ is committed and
                                                                 visible to software.
                                                                 The tail IOVA is [BASE] + ([TAIL] * 512) when NIX_AF_LF()_CFG[XQE_SIZE] = NIX_XQESZ_E::W64,
                                                                 [BASE] + ([TAIL] * 128) when NIX_AF_LF()_CFG[XQE_SIZE] = NIX_XQESZ_E::W16. */
        uint64_t caching               : 1;  /**< [168:168] Selects the style of write to the LLC.
                                                                 0 = Writes of CQE data will not allocate into the LLC.
                                                                 1 = Writes of CQE data are allocated into the LLC. */
        uint64_t reserved_169_171      : 3;
        uint64_t qsize                 : 4;  /**< [175:172] Specifies CQ ring size in number of CQEs:
                                                                 0x0 = 16 CQEs.
                                                                 0x1 = 64 CQEs.
                                                                 0x2 = 256 CQEs.
                                                                 0x3 = 1K CQEs.
                                                                 0x4 = 4K CQEs.
                                                                 0x5 = 16K CQEs.
                                                                 0x6 = 64K CQEs.
                                                                 0x7 = 256K CQEs.
                                                                 0x8 = 1M CQEs.
                                                                 0x9-0xF = Reserved.

                                                                 The CQE size is selected by NIX_AF_LF()_CFG[XQE_SIZE].

                                                                 Note that the usable size of the ring is the specified size minus one
                                                                 ([HEAD]==[TAIL] always means empty). */
        uint64_t qint_idx              : 7;  /**< [182:176] Error queue interrupt index. Select the QINT within LF (index {a} of
                                                                 NIX_LF_QINT()*) which receives [CQ_ERR_INT] events. */
        uint64_t reserved_183          : 1;
        uint64_t cint_idx              : 7;  /**< [190:184] Completion interrupt index. Select the CINT within LF (index {a} of
                                                                 NIX_LF_CINT()*) which receives completion events for
                                                                 this CQ. */
        uint64_t reserved_191          : 1;
#endif /* Word 2 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 3 - Big Endian */
        uint64_t base                  : 64; /**< [255:192] Base IOVA of CQ ring in LLC/DRAM.

                                                                 Bits \<8:0\> must be zero; address must be 512-byte aligned.
                                                                 Bits \<63:53\> are ignored by hardware; software should use a sign-extended
                                                                 bit \<52\> for forward compatibility.

                                                                 Internal:
                                                                 Bits \<63:53\>, \<6:0\> are ignored by hardware, treated as always 0x0. */
#else /* Word 3 - Little Endian */
        uint64_t base                  : 64; /**< [255:192] Base IOVA of CQ ring in LLC/DRAM.

                                                                 Bits \<8:0\> must be zero; address must be 512-byte aligned.
                                                                 Bits \<63:53\> are ignored by hardware; software should use a sign-extended
                                                                 bit \<52\> for forward compatibility.

                                                                 Internal:
                                                                 Bits \<63:53\>, \<6:0\> are ignored by hardware, treated as always 0x0. */
#endif /* Word 3 - End */
    } s;
    /* struct bdk_nix_cq_ctx_s_s cn; */
};

/**
 * Structure nix_cqe_hdr_s
 *
 * NIX Completion Queue Entry Header Structure
 * This 64-bit structure defines the first word of every CQE. It is immediately
 * followed by NIX_RX_PARSE_S in a receive CQE, and by NIX_SEND_COMP_S in a send
 * completion CQE.
 * Stored in memory as little-endian unless NIX_AF_LF()_CFG[BE] is set.
 */
union bdk_nix_cqe_hdr_s
{
    uint64_t u;
    struct bdk_nix_cqe_hdr_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t cqe_type              : 4;  /**< [ 63: 60] Completion queue entry type. Enumerated by NIX_XQE_TYPE_E. */
        uint64_t node                  : 2;  /**< [ 59: 58] Node number on which the packet was received or transmitted.
                                                                 Internal:
                                                                 This is needed by software; do not remove on single-node parts. */
        uint64_t reserved_52_57        : 6;
        uint64_t q                     : 20; /**< [ 51: 32] RQ or SQ within VF/PF. */
        uint64_t tag                   : 32; /**< [ 31:  0] Tag computed for the RX packet. Valid for receive descriptor only.
                                                                 See pseudocode in NIX_RQ_CTX_S[LTAG]. */
#else /* Word 0 - Little Endian */
        uint64_t tag                   : 32; /**< [ 31:  0] Tag computed for the RX packet. Valid for receive descriptor only.
                                                                 See pseudocode in NIX_RQ_CTX_S[LTAG]. */
        uint64_t q                     : 20; /**< [ 51: 32] RQ or SQ within VF/PF. */
        uint64_t reserved_52_57        : 6;
        uint64_t node                  : 2;  /**< [ 59: 58] Node number on which the packet was received or transmitted.
                                                                 Internal:
                                                                 This is needed by software; do not remove on single-node parts. */
        uint64_t cqe_type              : 4;  /**< [ 63: 60] Completion queue entry type. Enumerated by NIX_XQE_TYPE_E. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_cqe_hdr_s_s cn; */
};

/**
 * Structure nix_inst_hdr_s
 *
 * NIX Instruction Header Structure
 * This structure defines the instruction header that precedes the packet header
 * supplied to NPC for packets to be transmitted by NIX.
 */
union bdk_nix_inst_hdr_s
{
    uint64_t u;
    struct bdk_nix_inst_hdr_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_36_63        : 28;
        uint64_t sq                    : 20; /**< [ 35: 16] Send queue within [PF_FUNC]. */
        uint64_t pf_func               : 16; /**< [ 15:  0] PF and function transmitting the packet. Format specified by
                                                                 RVU_PF_FUNC_S. */
#else /* Word 0 - Little Endian */
        uint64_t pf_func               : 16; /**< [ 15:  0] PF and function transmitting the packet. Format specified by
                                                                 RVU_PF_FUNC_S. */
        uint64_t sq                    : 20; /**< [ 35: 16] Send queue within [PF_FUNC]. */
        uint64_t reserved_36_63        : 28;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_inst_hdr_s_s cn; */
};

/**
 * Structure nix_iova_s
 *
 * NIX I/O Virtual Address Structure
 */
union bdk_nix_iova_s
{
    uint64_t u;
    struct bdk_nix_iova_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t addr                  : 64; /**< [ 63:  0] I/O virtual address. Bits \<63:53\> are ignored by hardware; software should use a
                                                                 sign-extended bit \<52\> for forward compatibility. */
#else /* Word 0 - Little Endian */
        uint64_t addr                  : 64; /**< [ 63:  0] I/O virtual address. Bits \<63:53\> are ignored by hardware; software should use a
                                                                 sign-extended bit \<52\> for forward compatibility. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_iova_s_s cn; */
};

/**
 * Structure nix_mem_result_s
 *
 * NIX Memory Value Structure
 * When NIX_SEND_MEM_S[ALG]=NIX_SENDMEMALG_E::SETRSLT, the value written to memory is formed with
 * this structure.
 */
union bdk_nix_mem_result_s
{
    uint64_t u;
    struct bdk_nix_mem_result_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_3_63         : 61;
        uint64_t color                 : 2;  /**< [  2:  1] Final color of the packet. Enumerated by NIX_COLORRESULT_E. */
        uint64_t v                     : 1;  /**< [  0:  0] Valid. Always set by hardware so software can distinguish from data that was (presumed to
                                                                 be) zeroed by software before the send operation. */
#else /* Word 0 - Little Endian */
        uint64_t v                     : 1;  /**< [  0:  0] Valid. Always set by hardware so software can distinguish from data that was (presumed to
                                                                 be) zeroed by software before the send operation. */
        uint64_t color                 : 2;  /**< [  2:  1] Final color of the packet. Enumerated by NIX_COLORRESULT_E. */
        uint64_t reserved_3_63         : 61;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_mem_result_s_s cn; */
};

/**
 * Structure nix_op_q_wdata_s
 *
 * NIX Statistics Operation Write Data Structure
 * This structure specifies the write data format of an atomic 64-bit load-and-add of
 * NIX_LF_RQ_OP_OCTS,
 * NIX_LF_RQ_OP_PKTS,
 * NIX_LF_SQ_OP_OCTS,
 * NIX_LF_SQ_OP_PKTS.
 */
union bdk_nix_op_q_wdata_s
{
    uint64_t u;
    struct bdk_nix_op_q_wdata_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_52_63        : 12;
        uint64_t q                     : 20; /**< [ 51: 32] Queue within VF/PF (RQ or SQ). */
        uint64_t reserved_0_31         : 32;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_31         : 32;
        uint64_t q                     : 20; /**< [ 51: 32] Queue within VF/PF (RQ or SQ). */
        uint64_t reserved_52_63        : 12;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_op_q_wdata_s_s cn; */
};

/**
 * Structure nix_qint_hw_s
 *
 * NIX Queue Interrupt Context Hardware Structure
 * This structure contains context state maintained by hardware for each queue
 * interrupt (QINT) in NDC/LLC/DRAM. Software accesses this structure with the
 * NIX_LF_QINT()* registers.
 * Hardware maintains a table of NIX_AF_CONST2[QINTS] contiguous NIX_QINT_HW_S
 * structures per LF starting at IOVA NIX_AF_LF()_QINTS_BASE.
 */
union bdk_nix_qint_hw_s
{
    uint32_t u;
    struct bdk_nix_qint_hw_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint32_t ena                   : 1;  /**< [ 31: 31] Interrupt enable. See also NIX_LF_QINT()_ENA_W1S[INTR] and
                                                                 NIX_LF_QINT()_ENA_W1C[INTR]. */
        uint32_t reserved_22_30        : 9;
        uint32_t count                 : 22; /**< [ 21:  0] Interrupt count. See NIX_LF_QINT()_CNT[COUNT]. */
#else /* Word 0 - Little Endian */
        uint32_t count                 : 22; /**< [ 21:  0] Interrupt count. See NIX_LF_QINT()_CNT[COUNT]. */
        uint32_t reserved_22_30        : 9;
        uint32_t ena                   : 1;  /**< [ 31: 31] Interrupt enable. See also NIX_LF_QINT()_ENA_W1S[INTR] and
                                                                 NIX_LF_QINT()_ENA_W1C[INTR]. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_qint_hw_s_s cn; */
};

/**
 * Structure nix_rq_ctx_s
 *
 * NIX Receive Queue Context Structure
 * This structure contains context state maintained by hardware for each RQ in
 * LLC/DRAM.
 * Software uses the same structure format to read and write an RQ context with
 * the NIX admin queue.
 *
 * Internal:
 * FIXME -- extend structure to 16 words (128 bytes), matching
 * NIX_AF_CONST3[RQ_CTX_LOG2BYTES].
 */
union bdk_nix_rq_ctx_s
{
    uint64_t u[8];
    struct bdk_nix_rq_ctx_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t wqe_aura              : 20; /**< [ 63: 44] WQE aura. Aura within NIX_AF_LF()_CFG[NPA_PF_FUNC] for allocating SSO
                                                                 work-queue entry buffers.
                                                                 Valid when [SSO_ENA] is set and [ENA_WQWD] is clear. */
        uint64_t substream             : 20; /**< [ 43: 24] Substream ID of IOVA pointers allocated from [WQE_AURA,SPB_AURA,LPB_AURA]. */
        uint64_t cq                    : 20; /**< [ 23:  4] Completion Queue for this SQ. */
        uint64_t ena_wqwd              : 1;  /**< [  3:  3] Enable WQE with data. Not used when [SSO_ENA] is clear.

                                                                 When [SSO_ENA] and [ENA_WQWD] are both set, the WQE is written at the
                                                                 beginning of the packet's first buffer allocated from [LPB_AURA], and the
                                                                 packet data starts at word offset [FIRST_SKIP] in the buffer.

                                                                 When [SSO_ENA] is set and [ENA_WQWD] is clear, the WQE is written to a
                                                                 dedicated buffer allocated from [WQE_AURA]. */
        uint64_t ipsech_ena            : 1;  /**< [  2:  2] IPSEC hardware fast-path enable. When set along with [SSO_ENA], packets
                                                                 with NIX_RX_ACTION_S[OP] = NIX_RX_ACTIONOP_E::UCAST_IPSEC may use the IPSEC
                                                                 hardware fast-path subject to other packet checks. */
        uint64_t sso_ena               : 1;  /**< [  1:  1] WQE enable. Selects the receive descriptor type and destination generated for the LF.
                                                                 0 = The descriptor type is a CQE written to the CQ ring selected by NIX_RQ_CTX_S[CQ].
                                                                 1 = The descriptor type is a WQE sent to SSO. */
        uint64_t ena                   : 1;  /**< [  0:  0] RQ enable. */
#else /* Word 0 - Little Endian */
        uint64_t ena                   : 1;  /**< [  0:  0] RQ enable. */
        uint64_t sso_ena               : 1;  /**< [  1:  1] WQE enable. Selects the receive descriptor type and destination generated for the LF.
                                                                 0 = The descriptor type is a CQE written to the CQ ring selected by NIX_RQ_CTX_S[CQ].
                                                                 1 = The descriptor type is a WQE sent to SSO. */
        uint64_t ipsech_ena            : 1;  /**< [  2:  2] IPSEC hardware fast-path enable. When set along with [SSO_ENA], packets
                                                                 with NIX_RX_ACTION_S[OP] = NIX_RX_ACTIONOP_E::UCAST_IPSEC may use the IPSEC
                                                                 hardware fast-path subject to other packet checks. */
        uint64_t ena_wqwd              : 1;  /**< [  3:  3] Enable WQE with data. Not used when [SSO_ENA] is clear.

                                                                 When [SSO_ENA] and [ENA_WQWD] are both set, the WQE is written at the
                                                                 beginning of the packet's first buffer allocated from [LPB_AURA], and the
                                                                 packet data starts at word offset [FIRST_SKIP] in the buffer.

                                                                 When [SSO_ENA] is set and [ENA_WQWD] is clear, the WQE is written to a
                                                                 dedicated buffer allocated from [WQE_AURA]. */
        uint64_t cq                    : 20; /**< [ 23:  4] Completion Queue for this SQ. */
        uint64_t substream             : 20; /**< [ 43: 24] Substream ID of IOVA pointers allocated from [WQE_AURA,SPB_AURA,LPB_AURA]. */
        uint64_t wqe_aura              : 20; /**< [ 63: 44] WQE aura. Aura within NIX_AF_LF()_CFG[NPA_PF_FUNC] for allocating SSO
                                                                 work-queue entry buffers.
                                                                 Valid when [SSO_ENA] is set and [ENA_WQWD] is clear. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t reserved_122_127      : 6;
        uint64_t lpb_drop_ena          : 1;  /**< [121:121] Request NPA to do DROP processing on [LPB_AURA] if an LPB is requested.
                                                                 See NPA_AURA_S[AURA_DROP] and NPA_AURA_S[POOL_DROP]. */
        uint64_t spb_drop_ena          : 1;  /**< [120:120] Request NPA to do DROP processing on [SPB_AURA] if an SPB is requested.
                                                                 See NPA_AURA_S[AURA_DROP] and NPA_AURA_S[POOL_DROP]. */
        uint64_t xqe_drop_ena          : 1;  /**< [119:119] WQE/CQE drop enable. When [SSO_ENA] is set and [ENA_WQWD] is clear, request
                                                                 NPA to do DROP processing on [WQE_AURA]; see NPA_AURA_S[AURA_DROP] and
                                                                 NPA_AURA_S[POOL_DROP]. When [SSO_ENA] is clear, request CQ DROP processing;
                                                                 see NIX_CQ_CTX_S[DROP,DROP_ENA]. */
        uint64_t wqe_caching           : 1;  /**< [118:118] WQE caching. Selects the style of work-queue entry write to LLC/DRAM.
                                                                 0 = Writes of WQE data will not allocate into LLC.
                                                                 1 = Writes of WQE data are allocated into LLC.

                                                                 Valid when [SSO_ENA] is set. */
        uint64_t pb_caching            : 2;  /**< [117:116] Packet buffer caching. Selects the style of packet buffer write to LLC/DRAM packet.
                                                                 0x0 = Writes of SPB/LPB data will not allocate into the LLC.
                                                                 0x1 = All writes of SPB/LPB data are allocated into the LLC.
                                                                 0x2 = First aligned cache block is allocated into the LLC. All remaining cache
                                                                 blocks are not allocated.
                                                                 0x3 = First two aligned cache blocks are allocated into the LLC. All remaining
                                                                 cache blocks are not allocated. */
        uint64_t sso_tt                : 2;  /**< [115:114] SSO tag type for the packet's SSO ADD_WORK and to store in NIX_WQE_HDR_S[TT].
                                                                 Enumerated by SSO_TT_E. Valid when [SSO_ENA] is set. */
        uint64_t sso_grp               : 10; /**< [113:104] SSO group for the packet's SSO ADD_WORK, and to store in NIX_WQE_HDR_S[GRP]. Valid
                                                                 when [SSO_ENA] is set.
                                                                 Bits \<9..8\> must be zero. */
        uint64_t lpb_aura              : 20; /**< [103: 84] Large packet buffer aura. See [SPB_AURA]. */
        uint64_t spb_aura              : 20; /**< [ 83: 64] Small packet buffer aura. Aura within NIX_AF_LF()_CFG[NPA_PF_FUNC] for
                                                                 allocating data buffers for small packets.
                                                                 Packet data is written to WQE/CQE, SPB and/or LPBs as
                                                                 described by the following pseudocode:

                                                                 \<pre\>
                                                                 pkt_bytes = NIX_RX_PARSE_S[PKT_LENM1] + 1;
                                                                 spb_bytes = 8*([SPB_SIZEM1] + 1 - [FIRST_SKIP]);
                                                                 imm_write = False;
                                                                 spb_write = False;
                                                                 lpb_write = False;

                                                                 if ([XQE_HDR_SPLIT]) {
                                                                    imm_bytes = min(NIX_RX_PARSE_S[EOH_PTR], 8*[XQE_IMM_SIZE]);
                                                                 } else {
                                                                    imm_bytes = 8*[XQE_IMM_SIZE];
                                                                 }

                                                                 if (imm_bytes == 0) {
                                                                    if ([SPB_ENA] && (pkt_bytes \<= spb_bytes)) {
                                                                       spb_write = True;  // Write packet to an SPB
                                                                    } else {
                                                                       lpb_write = True;  // Write packet to one or more LPBs
                                                                    }
                                                                 }
                                                                 else { // imm_bytes \> 0
                                                                    // Write first imm_bytes of packet
                                                                    // (or entire packet if smaller) to WQE/CQE
                                                                    imm_write = True;

                                                                    if ([XQE_IMM_COPY]) {
                                                                       // Include copy of first imm_bytes in SPB or LPB.
                                                                       if (pkt_bytes \<= spb_bytes) {
                                                                          spb_write = True;  // Write packet to an SPB
                                                                       } else {
                                                                          lpb_write = True;  // Write packet to one or more LPBs
                                                                       }
                                                                    }
                                                                    else {
                                                                       if (pkt_bytes \<= imm_bytes) {
                                                                          ;  // No remaining packet data. Done.
                                                                       } else if ((pkt_bytes - imm_bytes) \<= spb_bytes) {
                                                                          // Write remaining packet data to an SPB
                                                                          spb_write = True;
                                                                       } else {
                                                                          // Write remaining packet data to one or more LPBs
                                                                          lpb_write = True;
                                                                       }
                                                                    }
                                                                 }
                                                                 \</pre\> */
#else /* Word 1 - Little Endian */
        uint64_t spb_aura              : 20; /**< [ 83: 64] Small packet buffer aura. Aura within NIX_AF_LF()_CFG[NPA_PF_FUNC] for
                                                                 allocating data buffers for small packets.
                                                                 Packet data is written to WQE/CQE, SPB and/or LPBs as
                                                                 described by the following pseudocode:

                                                                 \<pre\>
                                                                 pkt_bytes = NIX_RX_PARSE_S[PKT_LENM1] + 1;
                                                                 spb_bytes = 8*([SPB_SIZEM1] + 1 - [FIRST_SKIP]);
                                                                 imm_write = False;
                                                                 spb_write = False;
                                                                 lpb_write = False;

                                                                 if ([XQE_HDR_SPLIT]) {
                                                                    imm_bytes = min(NIX_RX_PARSE_S[EOH_PTR], 8*[XQE_IMM_SIZE]);
                                                                 } else {
                                                                    imm_bytes = 8*[XQE_IMM_SIZE];
                                                                 }

                                                                 if (imm_bytes == 0) {
                                                                    if ([SPB_ENA] && (pkt_bytes \<= spb_bytes)) {
                                                                       spb_write = True;  // Write packet to an SPB
                                                                    } else {
                                                                       lpb_write = True;  // Write packet to one or more LPBs
                                                                    }
                                                                 }
                                                                 else { // imm_bytes \> 0
                                                                    // Write first imm_bytes of packet
                                                                    // (or entire packet if smaller) to WQE/CQE
                                                                    imm_write = True;

                                                                    if ([XQE_IMM_COPY]) {
                                                                       // Include copy of first imm_bytes in SPB or LPB.
                                                                       if (pkt_bytes \<= spb_bytes) {
                                                                          spb_write = True;  // Write packet to an SPB
                                                                       } else {
                                                                          lpb_write = True;  // Write packet to one or more LPBs
                                                                       }
                                                                    }
                                                                    else {
                                                                       if (pkt_bytes \<= imm_bytes) {
                                                                          ;  // No remaining packet data. Done.
                                                                       } else if ((pkt_bytes - imm_bytes) \<= spb_bytes) {
                                                                          // Write remaining packet data to an SPB
                                                                          spb_write = True;
                                                                       } else {
                                                                          // Write remaining packet data to one or more LPBs
                                                                          lpb_write = True;
                                                                       }
                                                                    }
                                                                 }
                                                                 \</pre\> */
        uint64_t lpb_aura              : 20; /**< [103: 84] Large packet buffer aura. See [SPB_AURA]. */
        uint64_t sso_grp               : 10; /**< [113:104] SSO group for the packet's SSO ADD_WORK, and to store in NIX_WQE_HDR_S[GRP]. Valid
                                                                 when [SSO_ENA] is set.
                                                                 Bits \<9..8\> must be zero. */
        uint64_t sso_tt                : 2;  /**< [115:114] SSO tag type for the packet's SSO ADD_WORK and to store in NIX_WQE_HDR_S[TT].
                                                                 Enumerated by SSO_TT_E. Valid when [SSO_ENA] is set. */
        uint64_t pb_caching            : 2;  /**< [117:116] Packet buffer caching. Selects the style of packet buffer write to LLC/DRAM packet.
                                                                 0x0 = Writes of SPB/LPB data will not allocate into the LLC.
                                                                 0x1 = All writes of SPB/LPB data are allocated into the LLC.
                                                                 0x2 = First aligned cache block is allocated into the LLC. All remaining cache
                                                                 blocks are not allocated.
                                                                 0x3 = First two aligned cache blocks are allocated into the LLC. All remaining
                                                                 cache blocks are not allocated. */
        uint64_t wqe_caching           : 1;  /**< [118:118] WQE caching. Selects the style of work-queue entry write to LLC/DRAM.
                                                                 0 = Writes of WQE data will not allocate into LLC.
                                                                 1 = Writes of WQE data are allocated into LLC.

                                                                 Valid when [SSO_ENA] is set. */
        uint64_t xqe_drop_ena          : 1;  /**< [119:119] WQE/CQE drop enable. When [SSO_ENA] is set and [ENA_WQWD] is clear, request
                                                                 NPA to do DROP processing on [WQE_AURA]; see NPA_AURA_S[AURA_DROP] and
                                                                 NPA_AURA_S[POOL_DROP]. When [SSO_ENA] is clear, request CQ DROP processing;
                                                                 see NIX_CQ_CTX_S[DROP,DROP_ENA]. */
        uint64_t spb_drop_ena          : 1;  /**< [120:120] Request NPA to do DROP processing on [SPB_AURA] if an SPB is requested.
                                                                 See NPA_AURA_S[AURA_DROP] and NPA_AURA_S[POOL_DROP]. */
        uint64_t lpb_drop_ena          : 1;  /**< [121:121] Request NPA to do DROP processing on [LPB_AURA] if an LPB is requested.
                                                                 See NPA_AURA_S[AURA_DROP] and NPA_AURA_S[POOL_DROP]. */
        uint64_t reserved_122_127      : 6;
#endif /* Word 1 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 2 - Big Endian */
        uint64_t xqe_hdr_split         : 1;  /**< [191:191] WQE/CQE header split.

                                                                 0 = The first 8*[XQE_IMM_SIZE] bytes (or all bytes if the packet is smaller) are
                                                                 written to the WQE/CQE irrespective of the parsed header size.

                                                                 1 = Only parsed header bytes (first NIX_RX_PARSE_S[EOH_PTR] bytes of packet) may
                                                                 be written to the WQE/CQE. The actual number of header bytes written to WQE/CQE
                                                                 is the smaller of NIX_RX_PARSE_S[EOH_PTR] or 8*[XQE_IMM_SIZE]. */
        uint64_t xqe_imm_copy          : 1;  /**< [190:190] WQE/CQE immediate data copy. When set, all packet data is written to one or
                                                                 more buffers from [SPB_AURA] or [LPB_AURA], and initial data bytes,
                                                                 including initial data bytes written to the WQE/CQE, if any. See also
                                                                 [XQE_IMM_SIZE] and [XQE_HDR_SPLIT]. */
        uint64_t reserved_184_189      : 6;
        uint64_t xqe_imm_size          : 6;  /**< [183:178] WQE/CQE immediate size. Must not be greater than 32, and must be 0 when
                                                                 NIX_AF_LF()_CFG[XQE_SIZE] = NIX_XQESZ_E::W16.

                                                                 When nonzero, the
                                                                 maximum number of starting eight-byte words of packet data written with
                                                                 NIX_RX_IMM_S in the receive descriptor (CQE or WQE). See also
                                                                 [XQE_HDR_SPLIT]. Remaining packet data (if any), or all packet data if
                                                                 [XQE_IMM_COPY] is set, is written to one or more buffers from [SPB_AURA] or
                                                                 [LPB_AURA].

                                                                 When zero, packet data is not written in the WQE/CQE; all packet data is
                                                                 written to buffers from [SPB_AURA] or [LPB_AURA].

                                                                 See pseudocode at [SPB_AURA]. */
        uint64_t later_skip            : 6;  /**< [177:172] Later buffer start offset. The number of eight-byte words from the
                                                                 [LPB_AURA] buffer pointer (other than the packet's first buffer)
                                                                 the first byte stored in the buffer. Must not be greater than
                                                                 [LPB_SIZEM1]. */
        uint64_t reserved_171          : 1;
        uint64_t first_skip            : 7;  /**< [170:164] First buffer start offset. The number of eight-byte words from the
                                                                 [SPB_AURA] or first [LPB_AURA] buffer pointer to the first byte stored in
                                                                 the buffer. Must not be greater than [LPB_SIZEM1], and when [SPB_ENA] is
                                                                 set not greater than [SPB_SIZEM1].

                                                                 When [SSO_ENA] and [ENA_WQWD] are both set, must be large enough to ensure
                                                                 that the WQE does not overlap with packet data (must be \>= 64 when
                                                                 NIX_AF_LF()_CFG[XQE_SIZE] == NIX_XQESZ_E::W64, \>= 16 when
                                                                 NIX_AF_LF()_CFG[XQE_SIZE] == NIX_XQESZ_E::W16). */
        uint64_t lpb_sizem1            : 12; /**< [163:152] Large packet buffer size minus one. The number of eight-byte words (minus
                                                                 one) between the start of a buffer from [LPB_AURA] and the last word that
                                                                 NIX may write into that buffer. Must be greater than or equal to
                                                                 [SPB_SIZEM1] when [SPB_ENA] is set.

                                                                 See [SPB_AURA]. */
        uint64_t spb_ena               : 1;  /**< [151:151] Small packet buffer enable:

                                                                 0 = Do not use small packet buffers. All receive packets are stored in
                                                                 buffers from [LPB_AURA].

                                                                 1 = Use a single small packet buffer from [SPB_AURA] when a receive packet
                                                                 fits within that buffer.

                                                                 Must be clear when [ENA_WQWD] is set.
                                                                 See [SPB_AURA]. */
        uint64_t reserved_146_150      : 5;
        uint64_t spb_sizem1            : 6;  /**< [145:140] Small packet buffer size minus one. The number of eight-byte words (minus one)
                                                                 between the start of a buffer from [SPB_AURA] and the last word that NIX may
                                                                 write into that buffer. See [SPB_AURA].

                                                                 Internal:
                                                                 Limited to 6 bits (512 bytes) to enable early SPB/LBP decision and avoid
                                                                 store-and-forward of larger packets. */
        uint64_t reserved_128_139      : 12;
#else /* Word 2 - Little Endian */
        uint64_t reserved_128_139      : 12;
        uint64_t spb_sizem1            : 6;  /**< [145:140] Small packet buffer size minus one. The number of eight-byte words (minus one)
                                                                 between the start of a buffer from [SPB_AURA] and the last word that NIX may
                                                                 write into that buffer. See [SPB_AURA].

                                                                 Internal:
                                                                 Limited to 6 bits (512 bytes) to enable early SPB/LBP decision and avoid
                                                                 store-and-forward of larger packets. */
        uint64_t reserved_146_150      : 5;
        uint64_t spb_ena               : 1;  /**< [151:151] Small packet buffer enable:

                                                                 0 = Do not use small packet buffers. All receive packets are stored in
                                                                 buffers from [LPB_AURA].

                                                                 1 = Use a single small packet buffer from [SPB_AURA] when a receive packet
                                                                 fits within that buffer.

                                                                 Must be clear when [ENA_WQWD] is set.
                                                                 See [SPB_AURA]. */
        uint64_t lpb_sizem1            : 12; /**< [163:152] Large packet buffer size minus one. The number of eight-byte words (minus
                                                                 one) between the start of a buffer from [LPB_AURA] and the last word that
                                                                 NIX may write into that buffer. Must be greater than or equal to
                                                                 [SPB_SIZEM1] when [SPB_ENA] is set.

                                                                 See [SPB_AURA]. */
        uint64_t first_skip            : 7;  /**< [170:164] First buffer start offset. The number of eight-byte words from the
                                                                 [SPB_AURA] or first [LPB_AURA] buffer pointer to the first byte stored in
                                                                 the buffer. Must not be greater than [LPB_SIZEM1], and when [SPB_ENA] is
                                                                 set not greater than [SPB_SIZEM1].

                                                                 When [SSO_ENA] and [ENA_WQWD] are both set, must be large enough to ensure
                                                                 that the WQE does not overlap with packet data (must be \>= 64 when
                                                                 NIX_AF_LF()_CFG[XQE_SIZE] == NIX_XQESZ_E::W64, \>= 16 when
                                                                 NIX_AF_LF()_CFG[XQE_SIZE] == NIX_XQESZ_E::W16). */
        uint64_t reserved_171          : 1;
        uint64_t later_skip            : 6;  /**< [177:172] Later buffer start offset. The number of eight-byte words from the
                                                                 [LPB_AURA] buffer pointer (other than the packet's first buffer)
                                                                 the first byte stored in the buffer. Must not be greater than
                                                                 [LPB_SIZEM1]. */
        uint64_t xqe_imm_size          : 6;  /**< [183:178] WQE/CQE immediate size. Must not be greater than 32, and must be 0 when
                                                                 NIX_AF_LF()_CFG[XQE_SIZE] = NIX_XQESZ_E::W16.

                                                                 When nonzero, the
                                                                 maximum number of starting eight-byte words of packet data written with
                                                                 NIX_RX_IMM_S in the receive descriptor (CQE or WQE). See also
                                                                 [XQE_HDR_SPLIT]. Remaining packet data (if any), or all packet data if
                                                                 [XQE_IMM_COPY] is set, is written to one or more buffers from [SPB_AURA] or
                                                                 [LPB_AURA].

                                                                 When zero, packet data is not written in the WQE/CQE; all packet data is
                                                                 written to buffers from [SPB_AURA] or [LPB_AURA].

                                                                 See pseudocode at [SPB_AURA]. */
        uint64_t reserved_184_189      : 6;
        uint64_t xqe_imm_copy          : 1;  /**< [190:190] WQE/CQE immediate data copy. When set, all packet data is written to one or
                                                                 more buffers from [SPB_AURA] or [LPB_AURA], and initial data bytes,
                                                                 including initial data bytes written to the WQE/CQE, if any. See also
                                                                 [XQE_IMM_SIZE] and [XQE_HDR_SPLIT]. */
        uint64_t xqe_hdr_split         : 1;  /**< [191:191] WQE/CQE header split.

                                                                 0 = The first 8*[XQE_IMM_SIZE] bytes (or all bytes if the packet is smaller) are
                                                                 written to the WQE/CQE irrespective of the parsed header size.

                                                                 1 = Only parsed header bytes (first NIX_RX_PARSE_S[EOH_PTR] bytes of packet) may
                                                                 be written to the WQE/CQE. The actual number of header bytes written to WQE/CQE
                                                                 is the smaller of NIX_RX_PARSE_S[EOH_PTR] or 8*[XQE_IMM_SIZE]. */
#endif /* Word 2 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 3 - Big Endian */
        uint64_t spb_pool_pass         : 8;  /**< [255:248] [SPB_AURA]'s average pool level pass threshold for RED.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t spb_pool_drop         : 8;  /**< [247:240] [SPB_AURA]'s average pool level drop threshold for RED.

                                                                 Software can set [SPB_POOL_DROP] = [SPB_POOL_PASS] = 0 to disable this
                                                                 level check in the RQ RED algorithm.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t spb_aura_pass         : 8;  /**< [239:232] [SPB_AURA]'s average aura level pass threshold for RED.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t spb_aura_drop         : 8;  /**< [231:224] [SPB_AURA]'s average aura level drop threshold for RED.

                                                                 Software can set [SPB_AURA_DROP] = [SPB_AURA_PASS] = 0 to disable this
                                                                 level check in the RQ RED algorithm.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t wqe_pool_pass         : 8;  /**< [223:216] [WQE_AURA]'s average pool level pass threshold for RED. Valid when
                                                                 [SSO_ENA] is set and [ENA_WQWD] is clear.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t wqe_pool_drop         : 8;  /**< [215:208] [WQE_AURA]'s average pool level drop threshold for RED. Valid when
                                                                 [SSO_ENA] is set and [ENA_WQWD] is clear.

                                                                 Software can set [WQE_POOL_DROP] = [WQE_POOL_PASS] = 0 to disable this
                                                                 level check in the RQ RED algorithm.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t xqe_pass              : 8;  /**< [207:200] WQE/CQE pass level for RED:
                                                                 * When [SSO_ENA] is set and [ENA_WQWD] is clear, [WQE_AURA]'s aura and pool
                                                                 average level pass threshold for RED.
                                                                 * When [SSO_ENA] is clear, [CQ]'S average level pass threshold for RED.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t xqe_drop              : 8;  /**< [199:192] WQE/CQE drop level for RED:
                                                                 * When [SSO_ENA] is set and [ENA_WQWD] is clear, [WQE_AURA]'s average aura
                                                                 level drop threshold for RED.
                                                                 * When [SSO_ENA] is clear, [CQ]'S average level drop threshold for RED.

                                                                 Software can set [XQE_DROP] = [XQE_PASS] = 0 to disable this level check in
                                                                 the RQ RED algorithm.

                                                                 RQ RED algorithm pseudocode:

                                                                 \<pre\>
                                                                   // See [SPB_AURA] for computation of spb_write and lpb_write
                                                                   // variables.

                                                                   int get_drop_prob(int level, int pass, int drop) {
                                                                      if (level \>= pass) { return 0; }
                                                                      else if (level \< drop) { return 255; }
                                                                      else { return (255 - 255 * (level - drop) / (pass - drop)); }
                                                                   }

                                                                   drop_prob = 0;

                                                                   if ([SSO_ENA] && ![ENA_WQWD]) {
                                                                      aura_level = [WQE_AURA]'s NPA_AURA_S[AVG_LEVEL];
                                                                      pool_level = [WQE_AURA]'s NPA_POOL_S[AVG_LEVEL];
                                                                      drop_prob = max(drop_prob,
                                                                                      get_drop_prob(aura_level, [XQE_PASS], [XQE_DROP]),
                                                                                      get_drop_prob(pool_level, [WQE_POOL_PASS], [WQE_POOL_DROP]);
                                                                   } else {
                                                                      cq_level = [CQ]'s NIX_CQ_CTX_S[AVG_LEVEL];
                                                                      drop_prob = max(drop_prob,
                                                                                      get_drop_prob(cq_level, [XQE_PASS], [XQE_DROP]);
                                                                   }

                                                                   if (lpb_write) { // First LPB is requested
                                                                      aura_level = [LPB_AURA]'s NPA_AURA_S[AVG_LEVEL];
                                                                      pool_level = [LPB_AURA]'s NPA_POOL_S[AVG_LEVEL];
                                                                      drop_prob = max(drop_prob,
                                                                                      get_drop_prob(aura_level, [LPB_AURA_PASS], [LPB_AURA_DROP]),
                                                                                      get_drop_prob(pool_level, [LPB_POOL_PASS], [LPB_POOL_DROP]);
                                                                   }
                                                                   else if (spb_write) { // SPB is requested
                                                                      aura_level = [SPB_AURA]'s NPA_AURA_S[AVG_LEVEL];
                                                                      pool_level = [SPB_AURA]'s NPA_POOL_S[AVG_LEVEL];
                                                                      drop_prob = max(drop_prob,
                                                                                      get_drop_prob(aura_level, [SPB_AURA_PASS], [SPB_AURA_DROP]),
                                                                                      get_drop_prob(pool_level, [SPB_POOL_PASS], [SPB_POOL_DROP]);
                                                                   }
                                                                 \</pre\> */
#else /* Word 3 - Little Endian */
        uint64_t xqe_drop              : 8;  /**< [199:192] WQE/CQE drop level for RED:
                                                                 * When [SSO_ENA] is set and [ENA_WQWD] is clear, [WQE_AURA]'s average aura
                                                                 level drop threshold for RED.
                                                                 * When [SSO_ENA] is clear, [CQ]'S average level drop threshold for RED.

                                                                 Software can set [XQE_DROP] = [XQE_PASS] = 0 to disable this level check in
                                                                 the RQ RED algorithm.

                                                                 RQ RED algorithm pseudocode:

                                                                 \<pre\>
                                                                   // See [SPB_AURA] for computation of spb_write and lpb_write
                                                                   // variables.

                                                                   int get_drop_prob(int level, int pass, int drop) {
                                                                      if (level \>= pass) { return 0; }
                                                                      else if (level \< drop) { return 255; }
                                                                      else { return (255 - 255 * (level - drop) / (pass - drop)); }
                                                                   }

                                                                   drop_prob = 0;

                                                                   if ([SSO_ENA] && ![ENA_WQWD]) {
                                                                      aura_level = [WQE_AURA]'s NPA_AURA_S[AVG_LEVEL];
                                                                      pool_level = [WQE_AURA]'s NPA_POOL_S[AVG_LEVEL];
                                                                      drop_prob = max(drop_prob,
                                                                                      get_drop_prob(aura_level, [XQE_PASS], [XQE_DROP]),
                                                                                      get_drop_prob(pool_level, [WQE_POOL_PASS], [WQE_POOL_DROP]);
                                                                   } else {
                                                                      cq_level = [CQ]'s NIX_CQ_CTX_S[AVG_LEVEL];
                                                                      drop_prob = max(drop_prob,
                                                                                      get_drop_prob(cq_level, [XQE_PASS], [XQE_DROP]);
                                                                   }

                                                                   if (lpb_write) { // First LPB is requested
                                                                      aura_level = [LPB_AURA]'s NPA_AURA_S[AVG_LEVEL];
                                                                      pool_level = [LPB_AURA]'s NPA_POOL_S[AVG_LEVEL];
                                                                      drop_prob = max(drop_prob,
                                                                                      get_drop_prob(aura_level, [LPB_AURA_PASS], [LPB_AURA_DROP]),
                                                                                      get_drop_prob(pool_level, [LPB_POOL_PASS], [LPB_POOL_DROP]);
                                                                   }
                                                                   else if (spb_write) { // SPB is requested
                                                                      aura_level = [SPB_AURA]'s NPA_AURA_S[AVG_LEVEL];
                                                                      pool_level = [SPB_AURA]'s NPA_POOL_S[AVG_LEVEL];
                                                                      drop_prob = max(drop_prob,
                                                                                      get_drop_prob(aura_level, [SPB_AURA_PASS], [SPB_AURA_DROP]),
                                                                                      get_drop_prob(pool_level, [SPB_POOL_PASS], [SPB_POOL_DROP]);
                                                                   }
                                                                 \</pre\> */
        uint64_t xqe_pass              : 8;  /**< [207:200] WQE/CQE pass level for RED:
                                                                 * When [SSO_ENA] is set and [ENA_WQWD] is clear, [WQE_AURA]'s aura and pool
                                                                 average level pass threshold for RED.
                                                                 * When [SSO_ENA] is clear, [CQ]'S average level pass threshold for RED.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t wqe_pool_drop         : 8;  /**< [215:208] [WQE_AURA]'s average pool level drop threshold for RED. Valid when
                                                                 [SSO_ENA] is set and [ENA_WQWD] is clear.

                                                                 Software can set [WQE_POOL_DROP] = [WQE_POOL_PASS] = 0 to disable this
                                                                 level check in the RQ RED algorithm.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t wqe_pool_pass         : 8;  /**< [223:216] [WQE_AURA]'s average pool level pass threshold for RED. Valid when
                                                                 [SSO_ENA] is set and [ENA_WQWD] is clear.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t spb_aura_drop         : 8;  /**< [231:224] [SPB_AURA]'s average aura level drop threshold for RED.

                                                                 Software can set [SPB_AURA_DROP] = [SPB_AURA_PASS] = 0 to disable this
                                                                 level check in the RQ RED algorithm.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t spb_aura_pass         : 8;  /**< [239:232] [SPB_AURA]'s average aura level pass threshold for RED.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t spb_pool_drop         : 8;  /**< [247:240] [SPB_AURA]'s average pool level drop threshold for RED.

                                                                 Software can set [SPB_POOL_DROP] = [SPB_POOL_PASS] = 0 to disable this
                                                                 level check in the RQ RED algorithm.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t spb_pool_pass         : 8;  /**< [255:248] [SPB_AURA]'s average pool level pass threshold for RED.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
#endif /* Word 3 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 4 - Big Endian */
        uint64_t reserved_315_319      : 5;
        uint64_t qint_idx              : 7;  /**< [314:308] Queue interrupt index. Select the QINT within LF (index {a} of
                                                                 NIX_LF_QINT()*) which receives [RQ_INT] events. */
        uint64_t rq_int_ena            : 8;  /**< [307:300] RQ interrupt enables. Bits enumerated by NIX_RQINT_E. */
        uint64_t rq_int                : 8;  /**< [299:292] RQ interrupts. Bits enumerated by NIX_RQINT_E. */
        uint64_t reserved_288_291      : 4;
        uint64_t lpb_pool_pass         : 8;  /**< [287:280] [LPB_AURA]'s average pool level pass threshold for RED.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t lpb_pool_drop         : 8;  /**< [279:272] [LPB_AURA]'s average pool level drop threshold for RED.

                                                                 Software can set [LPB_POOL_DROP] = [LPB_POOL_PASS] = 0 to disable this
                                                                 level check in the RQ RED algorithm.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t lpb_aura_pass         : 8;  /**< [271:264] [LPB_AURA]'s average aura level pass threshold for RED.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t lpb_aura_drop         : 8;  /**< [263:256] [LPB_AURA]'s average aura level drop threshold for RED.

                                                                 Software can set [LPB_AURA_DROP] = [LPB_AURA_PASS] = 0 to disable this
                                                                 level check in the RQ RED algorithm.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
#else /* Word 4 - Little Endian */
        uint64_t lpb_aura_drop         : 8;  /**< [263:256] [LPB_AURA]'s average aura level drop threshold for RED.

                                                                 Software can set [LPB_AURA_DROP] = [LPB_AURA_PASS] = 0 to disable this
                                                                 level check in the RQ RED algorithm.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t lpb_aura_pass         : 8;  /**< [271:264] [LPB_AURA]'s average aura level pass threshold for RED.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t lpb_pool_drop         : 8;  /**< [279:272] [LPB_AURA]'s average pool level drop threshold for RED.

                                                                 Software can set [LPB_POOL_DROP] = [LPB_POOL_PASS] = 0 to disable this
                                                                 level check in the RQ RED algorithm.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t lpb_pool_pass         : 8;  /**< [287:280] [LPB_AURA]'s average pool level pass threshold for RED.

                                                                 See RQ RED algorithm pseudocode in [XQE_DROP]. */
        uint64_t reserved_288_291      : 4;
        uint64_t rq_int                : 8;  /**< [299:292] RQ interrupts. Bits enumerated by NIX_RQINT_E. */
        uint64_t rq_int_ena            : 8;  /**< [307:300] RQ interrupt enables. Bits enumerated by NIX_RQINT_E. */
        uint64_t qint_idx              : 7;  /**< [314:308] Queue interrupt index. Select the QINT within LF (index {a} of
                                                                 NIX_LF_QINT()*) which receives [RQ_INT] events. */
        uint64_t reserved_315_319      : 5;
#endif /* Word 4 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 5 - Big Endian */
        uint64_t reserved_366_383      : 18;
        uint64_t flow_tagw             : 6;  /**< [365:360] Flow tag width. Number of lower bits of WQE/CQE tag taken from packet's
                                                                 FLOW_TAG (see NIX_LF_RX_SECRET()). When greater than or equal to 32, the
                                                                 WQE/CQE tag equals FLOW_TAG.
                                                                 See pseudocode in [LTAG]. */
        uint64_t bad_utag              : 8;  /**< [359:352] Upper WQE/CQE tag bits for a non-IPSEC packet received with error,
                                                                 conditionally selected by [FLOW_TAGW].
                                                                 See pseudocode in [LTAG]. */
        uint64_t good_utag             : 8;  /**< [351:344] Upper WQE/CQE tag bits for a non-IPSEC packet received without error,
                                                                 conditionally selected by [FLOW_TAGW].
                                                                 See pseudocode in [LTAG]. */
        uint64_t ltag                  : 24; /**< [343:320] Lower WQE/CQE tag bits for a non-IPSEC receive packet, conditionally
                                                                 selected by [FLOW_TAGW].

                                                                 Pseudocode:
                                                                 \<pre\>
                                                                 rq_tag\<31:24\> = (NIX_RX_PARSE_S[ERRLEV] == 0 && NIX_RX_PARSE_S[ERRCODE] == 0)
                                                                      ? [GOOD_UTAG] : [BAD_UTAG];
                                                                 rq_tag\<23:0\> = [LTAG];
                                                                 flow_tag_mask\<31:0\> = (1 \<\< [FLOW_TAGW]) - 1;
                                                                 xqe_type = [SSO_ENA] ? NIX_WQE_HDR_S[WQE_TYPE] ? NIX_CQE_HDR_S[CQE_TYPE];

                                                                 if ((xqe_type == NIX_XQE_TYPE_E::RX)
                                                                     || (xqe_type == NIX_XQE_TYPE_E::RX_IPSECS))
                                                                 {
                                                                    // FLOW_TAG\<31:0\> computation is defined in NIX_LF_RX_SECRET()
                                                                    tag\<31:0\> = (~flow_tag_mask & rq_tag) | (flow_tag_mask & FLOW_TAG);
                                                                    if ([SSO_ENA]) NIX_WQE_HDR_S[TAG] = tag;
                                                                    else           NIX_CQE_HDR_S[TAG] = tag;
                                                                 }
                                                                 else { // NIX_XQE_TYPE_E::RX_IPSECH or IPSECD, only valid when [SSO_ENA]==1
                                                                    // SA_index computation is defined in NIX_AF_LF()_RX_IPSEC_SA_CFG[SA_IDX_W]
                                                                    NIX_WQE_HDR_S[TAG] = SA_index | (NIX_AF_LF()_RX_IPSEC_CFG[TAG_CONST] \<\< 8);
                                                                 }
                                                                 \</pre\> */
#else /* Word 5 - Little Endian */
        uint64_t ltag                  : 24; /**< [343:320] Lower WQE/CQE tag bits for a non-IPSEC receive packet, conditionally
                                                                 selected by [FLOW_TAGW].

                                                                 Pseudocode:
                                                                 \<pre\>
                                                                 rq_tag\<31:24\> = (NIX_RX_PARSE_S[ERRLEV] == 0 && NIX_RX_PARSE_S[ERRCODE] == 0)
                                                                      ? [GOOD_UTAG] : [BAD_UTAG];
                                                                 rq_tag\<23:0\> = [LTAG];
                                                                 flow_tag_mask\<31:0\> = (1 \<\< [FLOW_TAGW]) - 1;
                                                                 xqe_type = [SSO_ENA] ? NIX_WQE_HDR_S[WQE_TYPE] ? NIX_CQE_HDR_S[CQE_TYPE];

                                                                 if ((xqe_type == NIX_XQE_TYPE_E::RX)
                                                                     || (xqe_type == NIX_XQE_TYPE_E::RX_IPSECS))
                                                                 {
                                                                    // FLOW_TAG\<31:0\> computation is defined in NIX_LF_RX_SECRET()
                                                                    tag\<31:0\> = (~flow_tag_mask & rq_tag) | (flow_tag_mask & FLOW_TAG);
                                                                    if ([SSO_ENA]) NIX_WQE_HDR_S[TAG] = tag;
                                                                    else           NIX_CQE_HDR_S[TAG] = tag;
                                                                 }
                                                                 else { // NIX_XQE_TYPE_E::RX_IPSECH or IPSECD, only valid when [SSO_ENA]==1
                                                                    // SA_index computation is defined in NIX_AF_LF()_RX_IPSEC_SA_CFG[SA_IDX_W]
                                                                    NIX_WQE_HDR_S[TAG] = SA_index | (NIX_AF_LF()_RX_IPSEC_CFG[TAG_CONST] \<\< 8);
                                                                 }
                                                                 \</pre\> */
        uint64_t good_utag             : 8;  /**< [351:344] Upper WQE/CQE tag bits for a non-IPSEC packet received without error,
                                                                 conditionally selected by [FLOW_TAGW].
                                                                 See pseudocode in [LTAG]. */
        uint64_t bad_utag              : 8;  /**< [359:352] Upper WQE/CQE tag bits for a non-IPSEC packet received with error,
                                                                 conditionally selected by [FLOW_TAGW].
                                                                 See pseudocode in [LTAG]. */
        uint64_t flow_tagw             : 6;  /**< [365:360] Flow tag width. Number of lower bits of WQE/CQE tag taken from packet's
                                                                 FLOW_TAG (see NIX_LF_RX_SECRET()). When greater than or equal to 32, the
                                                                 WQE/CQE tag equals FLOW_TAG.
                                                                 See pseudocode in [LTAG]. */
        uint64_t reserved_366_383      : 18;
#endif /* Word 5 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 6 - Big Endian */
        uint64_t reserved_432_447      : 16;
        uint64_t octs                  : 48; /**< [431:384] Number of octets received. Includes frame minimum size pad bytes and
                                                                 excludes FCS bytes. */
#else /* Word 6 - Little Endian */
        uint64_t octs                  : 48; /**< [431:384] Number of octets received. Includes frame minimum size pad bytes and
                                                                 excludes FCS bytes. */
        uint64_t reserved_432_447      : 16;
#endif /* Word 6 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 7 - Big Endian */
        uint64_t reserved_496_511      : 16;
        uint64_t pkts                  : 48; /**< [495:448] Number of packets successfully received. */
#else /* Word 7 - Little Endian */
        uint64_t pkts                  : 48; /**< [495:448] Number of packets successfully received. */
        uint64_t reserved_496_511      : 16;
#endif /* Word 7 - End */
    } s;
    /* struct bdk_nix_rq_ctx_s_s cn; */
};

/**
 * Structure nix_rsse_s
 *
 * NIX Receive Side Scaling Entry Structure
 * This structure specifies the format of each hardware entry in the NIX RSS
 * tables in LLC/DRAM. See NIX_AF_LF()_RSS_BASE and NIX_AF_LF()_RSS_GRP().
 * Software uses the same structure format to read and write an RSS table entry
 * with the NIX admin queue.
 */
union bdk_nix_rsse_s
{
    uint32_t u;
    struct bdk_nix_rsse_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint32_t reserved_20_31        : 12;
        uint32_t rq                    : 20; /**< [ 19:  0] Receive queue index within LF to which the packet is directed. */
#else /* Word 0 - Little Endian */
        uint32_t rq                    : 20; /**< [ 19:  0] Receive queue index within LF to which the packet is directed. */
        uint32_t reserved_20_31        : 12;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_rsse_s_s cn; */
};

/**
 * Structure nix_rx_action_s
 *
 * NIX Receive Action Structure
 * This structure defines the format of NPC_RESULT_S[ACTION] for a receive packet.
 */
union bdk_nix_rx_action_s
{
    uint64_t u;
    struct bdk_nix_rx_action_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_61_63        : 3;
        uint64_t flow_key_alg          : 5;  /**< [ 60: 56] Flow key algorithm. Index {a} (ALG) of NIX_AF_RX_FLOW_KEY_ALG()_FIELD(). */
        uint64_t match_id              : 16; /**< [ 55: 40] Software defined match identifier. */
        uint64_t index                 : 20; /**< [ 39: 20] Receive queue or table index in NIX. The index type
                                                                 is selected as follows:
                                                                 _ if [OP] = NIX_RX_ACTIONOP_E::UCAST or NIX_RX_ACTIONOP_E::UCAST_IPSEC, RQ
                                                                 index within [PF_FUNC].
                                                                 _ if [OP] = NIX_RX_ACTIONOP_E::RSS, RSS group index within [PF_FUNC].
                                                                 _ if [OP] = NIX_RX_ACTIONOP_E::MCAST, index of first NIX_RX_MCE_S of the
                                                                 multicast replication list in the NIX RX multicast/mirror table.
                                                                 _ if [OP] = NIX_RX_ACTIONOP_E::MIRROR, index of first NIX_RX_MCE_S of the
                                                                 mirror replication list in the NIX RX multicast/mirror table.
                                                                 _ otherwise, not used. */
        uint64_t pf_func               : 16; /**< [ 19:  4] PF and function to which the packet is directed if unicast.
                                                                 Format specified by RVU_PF_FUNC_S. Not meaningful when
                                                                 [OP] = NIX_RX_ACTIONOP_E::MCAST or NIX_RX_ACTIONOP_E::MIRROR. */
        uint64_t op                    : 4;  /**< [  3:  0] Action op code enumerated by NIX_RX_ACTIONOP_E. */
#else /* Word 0 - Little Endian */
        uint64_t op                    : 4;  /**< [  3:  0] Action op code enumerated by NIX_RX_ACTIONOP_E. */
        uint64_t pf_func               : 16; /**< [ 19:  4] PF and function to which the packet is directed if unicast.
                                                                 Format specified by RVU_PF_FUNC_S. Not meaningful when
                                                                 [OP] = NIX_RX_ACTIONOP_E::MCAST or NIX_RX_ACTIONOP_E::MIRROR. */
        uint64_t index                 : 20; /**< [ 39: 20] Receive queue or table index in NIX. The index type
                                                                 is selected as follows:
                                                                 _ if [OP] = NIX_RX_ACTIONOP_E::UCAST or NIX_RX_ACTIONOP_E::UCAST_IPSEC, RQ
                                                                 index within [PF_FUNC].
                                                                 _ if [OP] = NIX_RX_ACTIONOP_E::RSS, RSS group index within [PF_FUNC].
                                                                 _ if [OP] = NIX_RX_ACTIONOP_E::MCAST, index of first NIX_RX_MCE_S of the
                                                                 multicast replication list in the NIX RX multicast/mirror table.
                                                                 _ if [OP] = NIX_RX_ACTIONOP_E::MIRROR, index of first NIX_RX_MCE_S of the
                                                                 mirror replication list in the NIX RX multicast/mirror table.
                                                                 _ otherwise, not used. */
        uint64_t match_id              : 16; /**< [ 55: 40] Software defined match identifier. */
        uint64_t flow_key_alg          : 5;  /**< [ 60: 56] Flow key algorithm. Index {a} (ALG) of NIX_AF_RX_FLOW_KEY_ALG()_FIELD(). */
        uint64_t reserved_61_63        : 3;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_rx_action_s_s cn; */
};

/**
 * Structure nix_rx_imm_s
 *
 * NIX Receive Immediate Subdescriptor Structure
 * The receive immediate subdescriptor indicates that bytes immediately following this
 * NIX_RX_IMM_S (after skipping [APAD] bytes) were saved from the received packet. The
 * next subdescriptor following this NIX_RX_IMM_S (when one exists) will follow the
 * immediate bytes, after rounding up the address to a multiple of 16 bytes.
 */
union bdk_nix_rx_imm_s
{
    uint64_t u;
    struct bdk_nix_rx_imm_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates immediate. Enumerated by NIX_SUBDC_E::IMM. */
        uint64_t reserved_19_59        : 41;
        uint64_t apad                  : 3;  /**< [ 18: 16] Alignment pad. Number of bytes to skip following this 64-bit structure before
                                                                 the first byte of packet data. */
        uint64_t size                  : 16; /**< [ 15:  0] Size of immediate data (in bytes) that immediately follows this 64-bit
                                                                 structure. [SIZE] will be between 1 and 256 bytes. The next subdescriptor
                                                                 follows [APAD]+[SIZE] bytes later in the descriptor, rounded up to the next
                                                                 16-byte aligned address. */
#else /* Word 0 - Little Endian */
        uint64_t size                  : 16; /**< [ 15:  0] Size of immediate data (in bytes) that immediately follows this 64-bit
                                                                 structure. [SIZE] will be between 1 and 256 bytes. The next subdescriptor
                                                                 follows [APAD]+[SIZE] bytes later in the descriptor, rounded up to the next
                                                                 16-byte aligned address. */
        uint64_t apad                  : 3;  /**< [ 18: 16] Alignment pad. Number of bytes to skip following this 64-bit structure before
                                                                 the first byte of packet data. */
        uint64_t reserved_19_59        : 41;
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates immediate. Enumerated by NIX_SUBDC_E::IMM. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_rx_imm_s_s cn; */
};

/**
 * Structure nix_rx_mce_s
 *
 * NIX Receive Multicast/Mirror Entry Structure
 * This structure specifies the format of entries in the NIX receive
 * multicast/mirror table maintained by hardware in LLC/DRAM. See
 * NIX_AF_RX_MCAST_BASE and NIX_AF_RX_MCAST_CFG. Note the table may contain both
 * multicast and mirror replication lists.
 * Software uses the same structure format to read and write a multicast/mirror
 * table entry with the NIX admin queue.
 */
union bdk_nix_rx_mce_s
{
    uint64_t u;
    struct bdk_nix_rx_mce_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t next                  : 16; /**< [ 63: 48] Index of next NIX_RX_MCE_S structure in the multicast/mirror replication
                                                                 list. Valid when [EOL] is clear. */
        uint64_t pf_func               : 16; /**< [ 47: 32] PF and function to which the packet is directed. Format specified by RVU_PF_FUNC_S. */
        uint64_t reserved_24_31        : 8;
        uint64_t index                 : 20; /**< [ 23:  4] Receive queue or RSS index within [PF_FUNC] to which the packet is directed. The
                                                                 index type is selected as follows:
                                                                 _ if [OP] = NIX_RX_MCOP_E::RQ, RQ index within [PF_FUNC].
                                                                 _ if [OP] = NIX_RX_MCOP_E::RSS, RSS group index within [PF_FUNC]. */
        uint64_t eol                   : 1;  /**< [  3:  3] End of multicast/mirror replication list. */
        uint64_t reserved_2            : 1;
        uint64_t op                    : 2;  /**< [  1:  0] Destination type within [PF_FUNC]; enumerated by NIX_RX_MCOP_E. */
#else /* Word 0 - Little Endian */
        uint64_t op                    : 2;  /**< [  1:  0] Destination type within [PF_FUNC]; enumerated by NIX_RX_MCOP_E. */
        uint64_t reserved_2            : 1;
        uint64_t eol                   : 1;  /**< [  3:  3] End of multicast/mirror replication list. */
        uint64_t index                 : 20; /**< [ 23:  4] Receive queue or RSS index within [PF_FUNC] to which the packet is directed. The
                                                                 index type is selected as follows:
                                                                 _ if [OP] = NIX_RX_MCOP_E::RQ, RQ index within [PF_FUNC].
                                                                 _ if [OP] = NIX_RX_MCOP_E::RSS, RSS group index within [PF_FUNC]. */
        uint64_t reserved_24_31        : 8;
        uint64_t pf_func               : 16; /**< [ 47: 32] PF and function to which the packet is directed. Format specified by RVU_PF_FUNC_S. */
        uint64_t next                  : 16; /**< [ 63: 48] Index of next NIX_RX_MCE_S structure in the multicast/mirror replication
                                                                 list. Valid when [EOL] is clear. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_rx_mce_s_s cn; */
};

/**
 * Structure nix_rx_parse_s
 *
 * NIX Receive Parse Structure
 * This structure contains the receive packet parse result. It immediately follows
 * NIX_CQE_HDR_S in a receive CQE, or NIX_WQE_HDR_S in a receive WQE.
 * Stored in memory as little-endian unless NIX_AF_LF()_CFG[BE] is set.
 *
 * Header layers are always 2-byte aligned, so all header pointers in this
 * structure ([EOH_PTR], [LAPTR] through [LHPTR], [VTAG*_PTR]) are even.
 */
union bdk_nix_rx_parse_s
{
    uint64_t u[7];
    struct bdk_nix_rx_parse_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t lhtype                : 4;  /**< [ 63: 60] Software defined layer H type from NPC_RESULT_S[LH[LTYPE]]. */
        uint64_t lgtype                : 4;  /**< [ 59: 56] Software defined layer G type from NPC_RESULT_S[LG[LTYPE]]. */
        uint64_t lftype                : 4;  /**< [ 55: 52] Software defined layer F type from NPC_RESULT_S[LF[LTYPE]]. */
        uint64_t letype                : 4;  /**< [ 51: 48] Software defined layer E type from NPC_RESULT_S[LE[LTYPE]]. */
        uint64_t ldtype                : 4;  /**< [ 47: 44] Software defined layer D type from NPC_RESULT_S[LD[LTYPE]]. */
        uint64_t lctype                : 4;  /**< [ 43: 40] Software defined layer C type from NPC_RESULT_S[LC[LTYPE]]. */
        uint64_t lbtype                : 4;  /**< [ 39: 36] Software defined layer B type from NPC_RESULT_S[LB[LTYPE]]. */
        uint64_t latype                : 4;  /**< [ 35: 32] Software defined layer A type from NPC_RESULT_S[LA[LTYPE]]. */
        uint64_t errcode               : 8;  /**< [ 31: 24] When zero, indicates no error. When nonzero, contains opcode identifying
                                                                 the error reason, [ERRLEV] specifies the lowest protocol layer containing
                                                                 the eror, and software should ignore all parse information for layers
                                                                 higher than [ERRLEV], e.g. ignore [LF*], [LG*] and [LH*] when [ERRCODE] is
                                                                 nonzero and [ERRLEV]=NPC_ERRLEV_E::LE.

                                                                 Values defined as follows:
                                                                 * When [ERRLEV] = NPC_ERRLEV_E::RE, [ERRCODE] values are enumerated by
                                                                 NIX_RE_OPCODE_E.
                                                                 * When [ERRLEV] = NPC_ERRLEV_E::NIX, [ERRCODE] values are enumerated by
                                                                 NIX_RX_PERRCODE_E.
                                                                 * For all other [ERRLEV] values, [ERRCODE] values are software defined in
                                                                 NPC. */
        uint64_t errlev                : 4;  /**< [ 23: 20] Normally zero, but when errors are detected contains the lowest protocol layer
                                                                 containing an error, and [ERRCODE] will indicate the precise error
                                                                 reason. Enumerated by NPC_ERRLEV_E. */
        uint64_t wqwd                  : 1;  /**< [ 19: 19] WQE with data. Valid when NIX_RX_PARSE_S is included in a WQE, always clear in
                                                                 a CQE. Value from NIX_RQ_CTX_S[ENA_WQWD]. When set, indicates that the
                                                                 packet data starts in the same buffer as the WQE, i.e. the first NIX_IOVA_S
                                                                 of the first NIX_RX_SG_S in the receive descriptor points to an address
                                                                 within the WQE's buffer. */
        uint64_t express               : 1;  /**< [ 18: 18] Express packet.
                                                                 0 = Normal (potentially preemptable) packet.
                                                                 1 = Express packet. */
        uint64_t reserved_17           : 1;
        uint64_t desc_sizem1           : 5;  /**< [ 16: 12] Number of 128-bit words minus one in receive descriptor following NIX_RX_PARSE_S,
                                                                 i.e. size (minus one) of all NIX_RX_IMM_S, NIX_RX_SG_S and associated immediate
                                                                 data and IOVAs in the descriptor. */
        uint64_t chan                  : 12; /**< [ 11:  0] The logical channel that the packet arrived from, enumerated by NIX_CHAN_E. */
#else /* Word 0 - Little Endian */
        uint64_t chan                  : 12; /**< [ 11:  0] The logical channel that the packet arrived from, enumerated by NIX_CHAN_E. */
        uint64_t desc_sizem1           : 5;  /**< [ 16: 12] Number of 128-bit words minus one in receive descriptor following NIX_RX_PARSE_S,
                                                                 i.e. size (minus one) of all NIX_RX_IMM_S, NIX_RX_SG_S and associated immediate
                                                                 data and IOVAs in the descriptor. */
        uint64_t reserved_17           : 1;
        uint64_t express               : 1;  /**< [ 18: 18] Express packet.
                                                                 0 = Normal (potentially preemptable) packet.
                                                                 1 = Express packet. */
        uint64_t wqwd                  : 1;  /**< [ 19: 19] WQE with data. Valid when NIX_RX_PARSE_S is included in a WQE, always clear in
                                                                 a CQE. Value from NIX_RQ_CTX_S[ENA_WQWD]. When set, indicates that the
                                                                 packet data starts in the same buffer as the WQE, i.e. the first NIX_IOVA_S
                                                                 of the first NIX_RX_SG_S in the receive descriptor points to an address
                                                                 within the WQE's buffer. */
        uint64_t errlev                : 4;  /**< [ 23: 20] Normally zero, but when errors are detected contains the lowest protocol layer
                                                                 containing an error, and [ERRCODE] will indicate the precise error
                                                                 reason. Enumerated by NPC_ERRLEV_E. */
        uint64_t errcode               : 8;  /**< [ 31: 24] When zero, indicates no error. When nonzero, contains opcode identifying
                                                                 the error reason, [ERRLEV] specifies the lowest protocol layer containing
                                                                 the eror, and software should ignore all parse information for layers
                                                                 higher than [ERRLEV], e.g. ignore [LF*], [LG*] and [LH*] when [ERRCODE] is
                                                                 nonzero and [ERRLEV]=NPC_ERRLEV_E::LE.

                                                                 Values defined as follows:
                                                                 * When [ERRLEV] = NPC_ERRLEV_E::RE, [ERRCODE] values are enumerated by
                                                                 NIX_RE_OPCODE_E.
                                                                 * When [ERRLEV] = NPC_ERRLEV_E::NIX, [ERRCODE] values are enumerated by
                                                                 NIX_RX_PERRCODE_E.
                                                                 * For all other [ERRLEV] values, [ERRCODE] values are software defined in
                                                                 NPC. */
        uint64_t latype                : 4;  /**< [ 35: 32] Software defined layer A type from NPC_RESULT_S[LA[LTYPE]]. */
        uint64_t lbtype                : 4;  /**< [ 39: 36] Software defined layer B type from NPC_RESULT_S[LB[LTYPE]]. */
        uint64_t lctype                : 4;  /**< [ 43: 40] Software defined layer C type from NPC_RESULT_S[LC[LTYPE]]. */
        uint64_t ldtype                : 4;  /**< [ 47: 44] Software defined layer D type from NPC_RESULT_S[LD[LTYPE]]. */
        uint64_t letype                : 4;  /**< [ 51: 48] Software defined layer E type from NPC_RESULT_S[LE[LTYPE]]. */
        uint64_t lftype                : 4;  /**< [ 55: 52] Software defined layer F type from NPC_RESULT_S[LF[LTYPE]]. */
        uint64_t lgtype                : 4;  /**< [ 59: 56] Software defined layer G type from NPC_RESULT_S[LG[LTYPE]]. */
        uint64_t lhtype                : 4;  /**< [ 63: 60] Software defined layer H type from NPC_RESULT_S[LH[LTYPE]]. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t vtag1_tci             : 16; /**< [127:112] Vtag 1 tag control information. See [VTAG0_TCI]. */
        uint64_t vtag0_tci             : 16; /**< [111: 96] Vtag 0 tag control information. First two bytes of Vtag's TCI field from the
                                                                 packet header.
                                                                 Valid when [VTAG0_VALID] is set. */
        uint64_t reserved_94_95        : 2;
        uint64_t pkind                 : 6;  /**< [ 93: 88] Port kind supplied by CGX or LBK for received packet. */
        uint64_t vtag1_gone            : 1;  /**< [ 87: 87] Vtag 1 gone. See [VTAG0_GONE]. */
        uint64_t vtag1_valid           : 1;  /**< [ 86: 86] Vtag 1 valid. See [VTAG0_VALID]. */
        uint64_t vtag0_gone            : 1;  /**< [ 85: 85] Vtag 0 gone. Valid when [VTAG0_VALID] is set. Indicates Vtag 0 was stripped from
                                                                 the packet data. Set when corresponding
                                                                 NIX_AF_LF()_RX_VTAG_TYPE()[CAPTURE,STRIP] are set. */
        uint64_t vtag0_valid           : 1;  /**< [ 84: 84] Vtag 0 valid. Set when NPC_RESULT_S[NIX_RX_VTAG_ACTION_S[VTAG0_VALID]] and
                                                                 corresponding NIX_AF_LF()_RX_VTAG_TYPE()[CAPTURE] are set. */
        uint64_t l3b                   : 1;  /**< [ 83: 83] Outer IP broadcast. See NPC_RESULT_S[L3B]. */
        uint64_t l3m                   : 1;  /**< [ 82: 82] Outer IP multicast. See NPC_RESULT_S[L3M]. */
        uint64_t l2b                   : 1;  /**< [ 81: 81] Outer L2 broadcast. See NPC_RESULT_S[L2B]. */
        uint64_t l2m                   : 1;  /**< [ 80: 80] Outer L2 multicast. See NPC_RESULT_S[L2M]. */
        uint64_t pkt_lenm1             : 16; /**< [ 79: 64] Packet length in bytes minus one. */
#else /* Word 1 - Little Endian */
        uint64_t pkt_lenm1             : 16; /**< [ 79: 64] Packet length in bytes minus one. */
        uint64_t l2m                   : 1;  /**< [ 80: 80] Outer L2 multicast. See NPC_RESULT_S[L2M]. */
        uint64_t l2b                   : 1;  /**< [ 81: 81] Outer L2 broadcast. See NPC_RESULT_S[L2B]. */
        uint64_t l3m                   : 1;  /**< [ 82: 82] Outer IP multicast. See NPC_RESULT_S[L3M]. */
        uint64_t l3b                   : 1;  /**< [ 83: 83] Outer IP broadcast. See NPC_RESULT_S[L3B]. */
        uint64_t vtag0_valid           : 1;  /**< [ 84: 84] Vtag 0 valid. Set when NPC_RESULT_S[NIX_RX_VTAG_ACTION_S[VTAG0_VALID]] and
                                                                 corresponding NIX_AF_LF()_RX_VTAG_TYPE()[CAPTURE] are set. */
        uint64_t vtag0_gone            : 1;  /**< [ 85: 85] Vtag 0 gone. Valid when [VTAG0_VALID] is set. Indicates Vtag 0 was stripped from
                                                                 the packet data. Set when corresponding
                                                                 NIX_AF_LF()_RX_VTAG_TYPE()[CAPTURE,STRIP] are set. */
        uint64_t vtag1_valid           : 1;  /**< [ 86: 86] Vtag 1 valid. See [VTAG0_VALID]. */
        uint64_t vtag1_gone            : 1;  /**< [ 87: 87] Vtag 1 gone. See [VTAG0_GONE]. */
        uint64_t pkind                 : 6;  /**< [ 93: 88] Port kind supplied by CGX or LBK for received packet. */
        uint64_t reserved_94_95        : 2;
        uint64_t vtag0_tci             : 16; /**< [111: 96] Vtag 0 tag control information. First two bytes of Vtag's TCI field from the
                                                                 packet header.
                                                                 Valid when [VTAG0_VALID] is set. */
        uint64_t vtag1_tci             : 16; /**< [127:112] Vtag 1 tag control information. See [VTAG0_TCI]. */
#endif /* Word 1 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 2 - Big Endian */
        uint64_t lhflags               : 8;  /**< [191:184] Software defined layer H flags from NPC_RESULT_S[LH[FLAGS]]. */
        uint64_t lgflags               : 8;  /**< [183:176] Software defined layer G flags from NPC_RESULT_S[LG[FLAGS]]. */
        uint64_t lfflags               : 8;  /**< [175:168] Software defined layer F flags from NPC_RESULT_S[LF[FLAGS]]. */
        uint64_t leflags               : 8;  /**< [167:160] Software defined layer E flags from NPC_RESULT_S[LE[FLAGS]]. */
        uint64_t ldflags               : 8;  /**< [159:152] Software defined layer D flags from NPC_RESULT_S[LD[FLAGS]]. */
        uint64_t lcflags               : 8;  /**< [151:144] Software defined layer C flags from NPC_RESULT_S[LC[FLAGS]]. */
        uint64_t lbflags               : 8;  /**< [143:136] Software defined layer B flags from NPC_RESULT_S[LB[FLAGS]]. */
        uint64_t laflags               : 8;  /**< [135:128] Software defined layer A flags from NPC_RESULT_S[LA[FLAGS]]. */
#else /* Word 2 - Little Endian */
        uint64_t laflags               : 8;  /**< [135:128] Software defined layer A flags from NPC_RESULT_S[LA[FLAGS]]. */
        uint64_t lbflags               : 8;  /**< [143:136] Software defined layer B flags from NPC_RESULT_S[LB[FLAGS]]. */
        uint64_t lcflags               : 8;  /**< [151:144] Software defined layer C flags from NPC_RESULT_S[LC[FLAGS]]. */
        uint64_t ldflags               : 8;  /**< [159:152] Software defined layer D flags from NPC_RESULT_S[LD[FLAGS]]. */
        uint64_t leflags               : 8;  /**< [167:160] Software defined layer E flags from NPC_RESULT_S[LE[FLAGS]]. */
        uint64_t lfflags               : 8;  /**< [175:168] Software defined layer F flags from NPC_RESULT_S[LF[FLAGS]]. */
        uint64_t lgflags               : 8;  /**< [183:176] Software defined layer G flags from NPC_RESULT_S[LG[FLAGS]]. */
        uint64_t lhflags               : 8;  /**< [191:184] Software defined layer H flags from NPC_RESULT_S[LH[FLAGS]]. */
#endif /* Word 2 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 3 - Big Endian */
        uint64_t match_id              : 16; /**< [255:240] Software defined match identifier from NIX_RX_ACTION_S[MATCH_ID]. */
        uint64_t pb_aura               : 20; /**< [239:220] Packet buffer aura. Valid when the receive descriptor contains at least one
                                                                 NIX_RX_SG_S. Aura within NIX_AF_LF()_CFG[NPA_PF_FUNC] from which the
                                                                 packet buffer pointers in NIX_RX_SG_S are allocated. */
        uint64_t wqe_aura              : 20; /**< [219:200] WQE aura. Valid when NIX_RX_PARSE_S is included in a WQE. Not valid when
                                                                 included in a CQE. Aura within NIX_AF_LF()_CFG[NPA_PF_FUNC] from which this
                                                                 WQE buffer was allocated. */
        uint64_t eoh_ptr               : 8;  /**< [199:192] End of header pointer. Byte offset from packet start to first byte after
                                                                 the last parsed header layer. */
#else /* Word 3 - Little Endian */
        uint64_t eoh_ptr               : 8;  /**< [199:192] End of header pointer. Byte offset from packet start to first byte after
                                                                 the last parsed header layer. */
        uint64_t wqe_aura              : 20; /**< [219:200] WQE aura. Valid when NIX_RX_PARSE_S is included in a WQE. Not valid when
                                                                 included in a CQE. Aura within NIX_AF_LF()_CFG[NPA_PF_FUNC] from which this
                                                                 WQE buffer was allocated. */
        uint64_t pb_aura               : 20; /**< [239:220] Packet buffer aura. Valid when the receive descriptor contains at least one
                                                                 NIX_RX_SG_S. Aura within NIX_AF_LF()_CFG[NPA_PF_FUNC] from which the
                                                                 packet buffer pointers in NIX_RX_SG_S are allocated. */
        uint64_t match_id              : 16; /**< [255:240] Software defined match identifier from NIX_RX_ACTION_S[MATCH_ID]. */
#endif /* Word 3 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 4 - Big Endian */
        uint64_t lhptr                 : 8;  /**< [319:312] Layer H pointer. Byte offset from packet start to first byte of layer H. */
        uint64_t lgptr                 : 8;  /**< [311:304] Layer G pointer. Byte offset from packet start to first byte of layer G. */
        uint64_t lfptr                 : 8;  /**< [303:296] Layer F pointer. Byte offset from packet start to first byte of layer F. */
        uint64_t leptr                 : 8;  /**< [295:288] Layer E pointer. Byte offset from packet start to first byte of layer E. */
        uint64_t ldptr                 : 8;  /**< [287:280] Layer D pointer. Byte offset from packet start to first byte of layer D. */
        uint64_t lcptr                 : 8;  /**< [279:272] Layer C pointer. Byte offset from packet start to first byte of layer C. */
        uint64_t lbptr                 : 8;  /**< [271:264] Layer B pointer. Byte offset from packet start to first byte of layer B. */
        uint64_t laptr                 : 8;  /**< [263:256] Layer A pointer. Byte offset from packet start to first byte of layer A. */
#else /* Word 4 - Little Endian */
        uint64_t laptr                 : 8;  /**< [263:256] Layer A pointer. Byte offset from packet start to first byte of layer A. */
        uint64_t lbptr                 : 8;  /**< [271:264] Layer B pointer. Byte offset from packet start to first byte of layer B. */
        uint64_t lcptr                 : 8;  /**< [279:272] Layer C pointer. Byte offset from packet start to first byte of layer C. */
        uint64_t ldptr                 : 8;  /**< [287:280] Layer D pointer. Byte offset from packet start to first byte of layer D. */
        uint64_t leptr                 : 8;  /**< [295:288] Layer E pointer. Byte offset from packet start to first byte of layer E. */
        uint64_t lfptr                 : 8;  /**< [303:296] Layer F pointer. Byte offset from packet start to first byte of layer F. */
        uint64_t lgptr                 : 8;  /**< [311:304] Layer G pointer. Byte offset from packet start to first byte of layer G. */
        uint64_t lhptr                 : 8;  /**< [319:312] Layer H pointer. Byte offset from packet start to first byte of layer H. */
#endif /* Word 4 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 5 - Big Endian */
        uint64_t reserved_341_383      : 43;
        uint64_t flow_key_alg          : 5;  /**< [340:336] Flow key algorithm used to generate NIX_CQE_HDR_S[TAG]. Index {a}
                                                                 (ALG) of NIX_AF_RX_FLOW_KEY_ALG()_FIELD(). */
        uint64_t vtag1_ptr             : 8;  /**< [335:328] Vtag 1 pointer. See [VTAG0_PTR]. */
        uint64_t vtag0_ptr             : 8;  /**< [327:320] Vtag 0 pointer. Byte offset from packet start to first byte of Vtag 0.
                                                                 Valid when [VTAG0_VALID] is set. */
#else /* Word 5 - Little Endian */
        uint64_t vtag0_ptr             : 8;  /**< [327:320] Vtag 0 pointer. Byte offset from packet start to first byte of Vtag 0.
                                                                 Valid when [VTAG0_VALID] is set. */
        uint64_t vtag1_ptr             : 8;  /**< [335:328] Vtag 1 pointer. See [VTAG0_PTR]. */
        uint64_t flow_key_alg          : 5;  /**< [340:336] Flow key algorithm used to generate NIX_CQE_HDR_S[TAG]. Index {a}
                                                                 (ALG) of NIX_AF_RX_FLOW_KEY_ALG()_FIELD(). */
        uint64_t reserved_341_383      : 43;
#endif /* Word 5 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 6 - Big Endian */
        uint64_t reserved_384_447      : 64;
#else /* Word 6 - Little Endian */
        uint64_t reserved_384_447      : 64;
#endif /* Word 6 - End */
    } s;
    /* struct bdk_nix_rx_parse_s_s cn; */
};

/**
 * Structure nix_rx_sg_s
 *
 * NIX Receive Scatter/Gather Subdescriptor Structure
 * The receive scatter/gather subdescriptor specifies one to three segments of packet data bytes.
 * There may be multiple NIX_RX_SG_Ss in each NIX receive descriptor.
 *
 * NIX_RX_SG_S is immediately followed by one NIX_IOVA_S word when [SEGS] = 1,
 * three NIX_IOVA_S words when [SEGS] \>= 2. Each NIX_IOVA_S word specifies the
 * IOVA of first packet data byte in the corresponding segment; first NIX_IOVA_S
 * word for segment 1, second word for segment 2, third word for segment 3. Note
 * the third word is present when [SEGS] \>= 2 but only valid when [SEGS] = 3.
 */
union bdk_nix_rx_sg_s
{
    uint64_t u;
    struct bdk_nix_rx_sg_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates scatter/gather. Enumerated by NIX_SUBDC_E::SG. */
        uint64_t reserved_50_59        : 10;
        uint64_t segs                  : 2;  /**< [ 49: 48] Number of valid segments. Must be nonzero. */
        uint64_t seg3_size             : 16; /**< [ 47: 32] Size of segment 3 in bytes. Valid when [SEGS] = 3. */
        uint64_t seg2_size             : 16; /**< [ 31: 16] Size of segment 2 in bytes. Valid when [SEGS] \>= 2. */
        uint64_t seg1_size             : 16; /**< [ 15:  0] Size of segment 1 in bytes. */
#else /* Word 0 - Little Endian */
        uint64_t seg1_size             : 16; /**< [ 15:  0] Size of segment 1 in bytes. */
        uint64_t seg2_size             : 16; /**< [ 31: 16] Size of segment 2 in bytes. Valid when [SEGS] \>= 2. */
        uint64_t seg3_size             : 16; /**< [ 47: 32] Size of segment 3 in bytes. Valid when [SEGS] = 3. */
        uint64_t segs                  : 2;  /**< [ 49: 48] Number of valid segments. Must be nonzero. */
        uint64_t reserved_50_59        : 10;
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates scatter/gather. Enumerated by NIX_SUBDC_E::SG. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_rx_sg_s_s cn; */
};

/**
 * Structure nix_rx_vtag_action_s
 *
 * NIX Receive Vtag Action Structure
 * This structure defines the format of NPC_RESULT_S[VTAG_ACTION] for a receive packet.
 * It specifies up to two Vtags (e.g. C-VLAN/S-VLAN tags, 802.1BR E-TAG) for optional
 * capture and/or stripping.
 */
union bdk_nix_rx_vtag_action_s
{
    uint64_t u;
    struct bdk_nix_rx_vtag_action_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t vtag1_valid           : 1;  /**< [ 47: 47] Vtag 1 valid. Remaining [VTAG1_*] fields are valid when set. */
        uint64_t vtag1_type            : 3;  /**< [ 46: 44] Vtag 1 type. See [VTAG0_TYPE]. */
        uint64_t reserved_43           : 1;
        uint64_t vtag1_lid             : 3;  /**< [ 42: 40] Vtag 1 layer ID enumerated by NPC_LID_E. */
        uint64_t vtag1_relptr          : 8;  /**< [ 39: 32] Vtag 1 relative pointer. See [VTAG0_RELPTR]. */
        uint64_t reserved_16_31        : 16;
        uint64_t vtag0_valid           : 1;  /**< [ 15: 15] Vtag 0 valid. Remaining [VTAG0_*] fields are valid when set. */
        uint64_t vtag0_type            : 3;  /**< [ 14: 12] Vtag 0 type. Index to NIX_AF_LF()_RX_VTAG_TYPE() entry for the receive
                                                                 packet's VF/PF. The selected entry specifies the tag size and optional tag
                                                                 strip/capture actions.

                                                                 The VF/PF is specified by NIX_RX_ACTION_S[PF_FUNC] when
                                                                 NIX_RX_ACTION_S[OP] != NIX_RX_ACTIONOP_E::MCAST or
                                                                 NIX_RX_ACTIONOP_E::MIRROR, and by the NIX RX multicast/mirror replication
                                                                 list entries otherwise. */
        uint64_t reserved_11           : 1;
        uint64_t vtag0_lid             : 3;  /**< [ 10:  8] Vtag 0 layer ID enumerated by NPC_LID_E. */
        uint64_t vtag0_relptr          : 8;  /**< [  7:  0] Vtag 0 relative pointer. Byte offset from start of selected layer to first
                                                                 tag 0 byte. Must be even. For example, if [VTAG0_LID] = NPC_LID_E::LB, then
                                                                 the byte offset from packet start to the first tag 0 byte is
                                                                 NPC_RESULT_S[LB[LPTR]] + [VTAG0_RELPTR]. */
#else /* Word 0 - Little Endian */
        uint64_t vtag0_relptr          : 8;  /**< [  7:  0] Vtag 0 relative pointer. Byte offset from start of selected layer to first
                                                                 tag 0 byte. Must be even. For example, if [VTAG0_LID] = NPC_LID_E::LB, then
                                                                 the byte offset from packet start to the first tag 0 byte is
                                                                 NPC_RESULT_S[LB[LPTR]] + [VTAG0_RELPTR]. */
        uint64_t vtag0_lid             : 3;  /**< [ 10:  8] Vtag 0 layer ID enumerated by NPC_LID_E. */
        uint64_t reserved_11           : 1;
        uint64_t vtag0_type            : 3;  /**< [ 14: 12] Vtag 0 type. Index to NIX_AF_LF()_RX_VTAG_TYPE() entry for the receive
                                                                 packet's VF/PF. The selected entry specifies the tag size and optional tag
                                                                 strip/capture actions.

                                                                 The VF/PF is specified by NIX_RX_ACTION_S[PF_FUNC] when
                                                                 NIX_RX_ACTION_S[OP] != NIX_RX_ACTIONOP_E::MCAST or
                                                                 NIX_RX_ACTIONOP_E::MIRROR, and by the NIX RX multicast/mirror replication
                                                                 list entries otherwise. */
        uint64_t vtag0_valid           : 1;  /**< [ 15: 15] Vtag 0 valid. Remaining [VTAG0_*] fields are valid when set. */
        uint64_t reserved_16_31        : 16;
        uint64_t vtag1_relptr          : 8;  /**< [ 39: 32] Vtag 1 relative pointer. See [VTAG0_RELPTR]. */
        uint64_t vtag1_lid             : 3;  /**< [ 42: 40] Vtag 1 layer ID enumerated by NPC_LID_E. */
        uint64_t reserved_43           : 1;
        uint64_t vtag1_type            : 3;  /**< [ 46: 44] Vtag 1 type. See [VTAG0_TYPE]. */
        uint64_t vtag1_valid           : 1;  /**< [ 47: 47] Vtag 1 valid. Remaining [VTAG1_*] fields are valid when set. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_rx_vtag_action_s_s cn; */
};

/**
 * Structure nix_send_comp_s
 *
 * NIX Send Completion Structure
 * This structure immediately follows NIX_CQE_HDR_S in a send completion CQE.
 */
union bdk_nix_send_comp_s
{
    uint64_t u;
    struct bdk_nix_send_comp_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_24_63        : 40;
        uint64_t sqe_id                : 16; /**< [ 23:  8] SQE identifier from NIX_SEND_HDR_S[SQE_ID]. */
        uint64_t status                : 8;  /**< [  7:  0] Send completion status enumerated by NIX_SEND_STATUS_E. */
#else /* Word 0 - Little Endian */
        uint64_t status                : 8;  /**< [  7:  0] Send completion status enumerated by NIX_SEND_STATUS_E. */
        uint64_t sqe_id                : 16; /**< [ 23:  8] SQE identifier from NIX_SEND_HDR_S[SQE_ID]. */
        uint64_t reserved_24_63        : 40;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_send_comp_s_s cn; */
};

/**
 * Structure nix_send_crc_s
 *
 * NIX Send CRC Subdescriptor Structure
 * The send CRC subdescriptor specifies a CRC calculation be performed during
 * transmission. There may be up to two NIX_SEND_CRC_Ss per send descriptor.
 *
 * NIX_SEND_CRC_S constraints:
 * * When present, NIX_SEND_CRC_S subdescriptors must precede all NIX_SEND_SG_S,
 * NIX_SEND_IMM_S and NIX_SEND_MEM_S subdescriptors in the send descriptor.
 * * NIX_SEND_CRC_S subdescriptors must follow the same order as their checksum
 * and insert regions in the packet, i.e. the checksum and insert regions of a
 * NIX_SEND_CRC_S must come after the checksum and insert regions of a preceding
 * NIX_SEND_CRC_S. There must be no overlap between any NIX_SEND_CRC_S checksum
 * and insert regions.
 * * If either NIX_SEND_HDR_S[OL4TYPE,IL4TYPE] = NIX_SENDL4TYPE_E::SCTP_CKSUM, the
 * SCTP checksum region and NIX_SEND_CRC_S insert region must not overlap, and
 * likewise the NIX_SEND_CRC_S checksum region and SCTP insert region must not
 * overlap.
 * * Any checksums inserted by NIX_SEND_HDR_S[OL3TYPE,OL4TYPE,IL3TYPE,IL4TYPE]
 * must be outside of the NIX_SEND_CRC_S checksum and insert regions.
 */
union bdk_nix_send_crc_s
{
    uint64_t u[2];
    struct bdk_nix_send_crc_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates send CRC. Enumerated by NIX_SUBDC_E::CRC. */
        uint64_t alg                   : 2;  /**< [ 59: 58] CRC algorithm enumerated by NIX_SENDCRCALG_E. */
        uint64_t reserved_48_57        : 10;
        uint64_t insert                : 16; /**< [ 47: 32] Byte position relative to the first packet byte at which to insert the first byte of the
                                                                 calculated CRC. NIX does not allocate bytes as it inserts the CRC result into the packet,
                                                                 it overwrites four pre-supplied packet bytes using NIX_SEND_SG_S or NIX_SEND_IMM_S.
                                                                 The insertion point may not be within the start/size region of this NIX_SEND_CRC_S or
                                                                 another NIX_SEND_CRC_S. */
        uint64_t start                 : 16; /**< [ 31: 16] Byte position relative to the first packet byte at which to start the checksum. */
        uint64_t size                  : 16; /**< [ 15:  0] Length of checksum region, must not be zero. The region is contiguous in packet bytes
                                                                 [START] through [START]+[SIZE]-1. Note that these covered packet bytes need not be
                                                                 contiguous in LLC/DRAM -- they can straddle any number of NIX_SEND_SG_S subdescriptors. */
#else /* Word 0 - Little Endian */
        uint64_t size                  : 16; /**< [ 15:  0] Length of checksum region, must not be zero. The region is contiguous in packet bytes
                                                                 [START] through [START]+[SIZE]-1. Note that these covered packet bytes need not be
                                                                 contiguous in LLC/DRAM -- they can straddle any number of NIX_SEND_SG_S subdescriptors. */
        uint64_t start                 : 16; /**< [ 31: 16] Byte position relative to the first packet byte at which to start the checksum. */
        uint64_t insert                : 16; /**< [ 47: 32] Byte position relative to the first packet byte at which to insert the first byte of the
                                                                 calculated CRC. NIX does not allocate bytes as it inserts the CRC result into the packet,
                                                                 it overwrites four pre-supplied packet bytes using NIX_SEND_SG_S or NIX_SEND_IMM_S.
                                                                 The insertion point may not be within the start/size region of this NIX_SEND_CRC_S or
                                                                 another NIX_SEND_CRC_S. */
        uint64_t reserved_48_57        : 10;
        uint64_t alg                   : 2;  /**< [ 59: 58] CRC algorithm enumerated by NIX_SENDCRCALG_E. */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates send CRC. Enumerated by NIX_SUBDC_E::CRC. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t reserved_96_127       : 32;
        uint64_t iv                    : 32; /**< [ 95: 64] Initial value of the checksum. If [ALG] = ONES16, then only bits \<15:0\> in
                                                                 big-endian format are valid. */
#else /* Word 1 - Little Endian */
        uint64_t iv                    : 32; /**< [ 95: 64] Initial value of the checksum. If [ALG] = ONES16, then only bits \<15:0\> in
                                                                 big-endian format are valid. */
        uint64_t reserved_96_127       : 32;
#endif /* Word 1 - End */
    } s;
    /* struct bdk_nix_send_crc_s_s cn; */
};

/**
 * Structure nix_send_ext_s
 *
 * NIX Send Extended Header Subdescriptor Structure
 * The send extended header specifies LSO, VLAN insertion, timestamp and/or
 * scheduling services on the packet. If present, it must immediately follow
 * NIX_SEND_HDR_S. All fields are assumed to be zero when this subdescriptor is not
 * present.
 */
union bdk_nix_send_ext_s
{
    uint64_t u[2];
    struct bdk_nix_send_ext_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates send extended header. Enumerated by NIX_SUBDC_E::EXT. */
        uint64_t mark_en               : 1;  /**< [ 59: 59] Enable for packet shaper marking. When one, NIX_COLORRESULT_E::YELLOW and
                                                                 NIX_COLORRESULT_E::RED_SEND packets will be marked as specified by
                                                                 [MARKFORM] and [MARKPTR].

                                                                 When [LSO] and [MARK_EN] are both set in the descriptor, NIX marks each LSO
                                                                 segment independently, using [MARKPTR] and [MARKFORM] for every LSO
                                                                 segment. */
        uint64_t markform              : 7;  /**< [ 58: 52] Mark Format. When [MARK_EN] is set, the NIX_AF_MARK_FORMAT()_CTL register
                                                                 which specifies how NIX will mark NIX_COLORRESULT_E::YELLOW and
                                                                 NIX_COLORRESULT_E::RED_SEND packets. [MARKFORM] must be less than the size
                                                                 of the NIX_AF_MARK_FORMAT()_CTL array. See also [MARK_EN]. */
        uint64_t markptr               : 8;  /**< [ 51: 44] Mark pointer. When [MARK_EN] is set, byte offset from packet start to byte
                                                                 to use for packet shaper marking. [MARKFORM] indirectly determines how this
                                                                 offset is used, including whether and how an L2 or L3 header is marked. See
                                                                 also [MARK_EN]. */
        uint64_t shp_ra                : 2;  /**< [ 43: 42] Red algorithm. Enumerated by NIX_REDALG_E. Specifies handling of a packet that
                                                                 traverses a RED MDQ through TL2 shaper. (A shaper is in RED state when
                                                                 NIX_AF_TL*()_SHAPE_STATE[COLOR]=0x2.) Has no effect when the packet traverses no
                                                                 shapers that are in the RED state. When [SHP_RA]!=STD, [SHP_RA] overrides the
                                                                 NIX_AF_TL*()_SHAPE[RED_ALGO] settings in all MDQ through TL2 shapers traversed
                                                                 by the packet. [SHP_RA] has no effect on the TL1 rate limiters. See
                                                                 NIX_AF_TL*()_MD_DEBUG*[RED_ALGO_OVERRIDE].

                                                                 When [LSO] is set in the descriptor, hardware applies [SHP_RA] to each LSO
                                                                 segment. */
        uint64_t shp_dis               : 1;  /**< [ 41: 41] Disables the shaper update and internal coloring algorithms used as
                                                                 the packet traverses MDQ through TL2 shapers. [SHP_DIS]
                                                                 has no effect on the L1 rate limiters.

                                                                 When [SHP_DIS] is 0 enabled CIR and PIR counters are used and adjusted
                                                                 per mode as the packet traverses through an enabled shaper. The
                                                                 internal color of a packet can be any of NIX_COLORRESULT_E::GREEN,
                                                                 NIX_COLORRESULT_E::YELLOW, NIX_COLORRESULT_E::RED_SEND, or
                                                                 NIX_COLORRESULT_E::RED_DROP after a shaper, depending on the shaper state
                                                                 and configuration.

                                                                 When [SHP_DIS] is 1 there is no packet coloring. No shaper can change
                                                                 the packet from its initial GREEN color. Neither the CIR nor PIR
                                                                 counters are used nor adjusted in any shaper as this packet traverses.
                                                                 Similar behavior to when both NIX_AF_TL*()_CIR[ENABLE] and
                                                                 NIX_AF_TL*()_PIR[ENABLE] are clear in all traversed shapers.

                                                                 See NIX_AF_TL*()_MD_DEBUG*[PIR_DIS,CIR_DIS]. When [SHP_DIS] is set,
                                                                 NIX_AF_TL*()_MD_DEBUG*[PIR_DIS,CIR_DIS] are both set. When [SHP_DIS] is
                                                                 clear, NIX_AF_TL*()_MD_DEBUG*[PIR_DIS,CIR_DIS] are both cleared and
                                                                 NIX_AF_TL*()_SHAPE[YELLOW_DISABLE,RED_DISABLE] determines the packet
                                                                 coloring of the shaper.

                                                                 When [LSO] is set, hardware applies [SHP_DIS] to each LSO segment. */
        uint64_t shp_chg               : 9;  /**< [ 40: 32] Signed packet size adjustment. The packet size used for shaper {a} (PIR_ACCUM and
                                                                 CIR_ACCUM) and DWRR scheduler {a} (RR_COUNT) calculations at level {b} is:

                                                                 _  (NIX_AF_{b}{a}_SHAPE[LENGTH_DISABLE] ? 0 : (NIX_AF_{b}{a}_MD*[LENGTH] + [SHP_CHG]))
                                                                        + NIX_AF_{b}{a}_SHAPE[ADJUST]

                                                                 where {b} = TL1, TL2, TL3, TL4, or MDQ and {a} selects one of the shapers
                                                                 at the level selected by {b}.

                                                                 [SHP_CHG] values -255 .. 255 are allowed. [SHP_CHG] value 0x100 (i.e. -256)
                                                                 is reserved and must never be used.

                                                                 [SHP_CHG] becomes NIX_AF_{b}m_MD*[ADJUST].

                                                                 When [LSO] is set, hardware applies [SHP_CHG] to each LSO segment. */
        uint64_t reserved_29_31        : 3;
        uint64_t lso_format            : 5;  /**< [ 28: 24] Large send offload format. Valid when [LSO] is set and selects index {a}
                                                                 (FORMAT) of NIX_AF_LSO_FORMAT()_FIELD(). */
        uint64_t lso_sb                : 8;  /**< [ 23: 16] Start bytes when [LSO] set. Location of the start byte of the TCP message
                                                                 payload, i.e. the size of the headers preceding the payload, excluding
                                                                 optional VLAN bytes inserted by [VLAN*] and Vtag bytes
                                                                 inserted by NIX_TX_VTAG_ACTION_S.

                                                                 Must be nonzero and less than NIX_SEND_HDR_S[TOTAL], else the send
                                                                 descriptor is treated as non-LSO. */
        uint64_t tstmp                 : 1;  /**< [ 15: 15] PTP timestamp. A timestamp is requested when this bit is set and all of the
                                                                 following are true:
                                                                 * NIX_AF_LF()_TX_CFG[SEND_TSTMP_ENA] is set.
                                                                 * [LSO] is clear.
                                                                 * A NIX_SEND_MEM_S is present in the descriptor with NIX_SEND_MEM_S[ALG] =
                                                                 NIX_SENDMEMALG_E::SETTSTMP.

                                                                 This bit is ignored if any of the above conditions is false.

                                                                 If timestamp is requested, hardware writes the packet's timestamp
                                                                 (MIO_PTP_CLOCK_HI) to IOVA NIX_SEND_MEM_S[ADDR] when the targeted CGX LMAC
                                                                 transmits the packet.

                                                                 Hardware writes 0x1 to IOVA NIX_SEND_MEM_S[ADDR] if the timestamp cannot be
                                                                 captured due to one of the following:
                                                                 * PTP packet is multicast: NIX_TX_ACTION_S[OP] = NIX_TX_ACTIONOP_E::MCAST.
                                                                 * PTP packet is sent to SDP or LBK, i.e. the transmit channel is a
                                                                 NIX_CHAN_E::LBK()_CH() or NIX_CHAN_E::SDP_CH() value.
                                                                 * PTP packet is dropped by NIX.
                                                                 * PTP packet is sent to a CGX LMAC but is not transmitted by the LMAC
                                                                 within the timeout period specified by NIX_AF_TX_TSTMP_CFG[TSTMP_WD_PERIOD].

                                                                 If NIX_SQ_CTX_S[CQ_ENA] and software wishes to receive a CQE on timestamp
                                                                 completion, it must set NIX_SEND_HDR_S[PNC] = 1 and NIX_SEND_MEM_S[WMEM] =
                                                                 1.

                                                                 If NIX_SQ_CTX_S[SSO_ENA] and software wishes to add work to SSO on
                                                                 timestamp completion, it must set NIX_SEND_MEM_S[WMEM] = 1 and include
                                                                 NIX_SEND_WORK_S in the descriptor. */
        uint64_t lso                   : 1;  /**< [ 14: 14] Large send offload. Ignored and treated as clear when
                                                                 NIX_AF_LSO_CFG[ENABLE] is clear. When set along with
                                                                 NIX_AF_LSO_CFG[ENABLE], the send descriptor is for one or more
                                                                 packets of a TCP flow, and the related [LSO_*] fields are valid. */
        uint64_t lso_mps               : 14; /**< [ 13:  0] When [LSO] set, maximum payload size in bytes per packet (e.g. maximum
                                                                 TCP segment size). Must be nonzero, else the send descriptor is treated as
                                                                 non-LSO.

                                                                 The maximum LSO packet size is [LSO_SB] + [LSO_MPS], plus optional VLAN
                                                                 bytes inserted by [VLAN*] and Vtag bytes inserted by
                                                                 NIX_TX_VTAG_ACTION_S. This must not exceed NIX_AF_SMQ()_CFG[MAXLEN].

                                                                 The number of LSO segments must be less than 256, thus [LSO_MPS] must be
                                                                 greater than 1024 to support maximum NIX_SEND_HDR_S[TOTAL] value of
                                                                 2**18 - 1. */
#else /* Word 0 - Little Endian */
        uint64_t lso_mps               : 14; /**< [ 13:  0] When [LSO] set, maximum payload size in bytes per packet (e.g. maximum
                                                                 TCP segment size). Must be nonzero, else the send descriptor is treated as
                                                                 non-LSO.

                                                                 The maximum LSO packet size is [LSO_SB] + [LSO_MPS], plus optional VLAN
                                                                 bytes inserted by [VLAN*] and Vtag bytes inserted by
                                                                 NIX_TX_VTAG_ACTION_S. This must not exceed NIX_AF_SMQ()_CFG[MAXLEN].

                                                                 The number of LSO segments must be less than 256, thus [LSO_MPS] must be
                                                                 greater than 1024 to support maximum NIX_SEND_HDR_S[TOTAL] value of
                                                                 2**18 - 1. */
        uint64_t lso                   : 1;  /**< [ 14: 14] Large send offload. Ignored and treated as clear when
                                                                 NIX_AF_LSO_CFG[ENABLE] is clear. When set along with
                                                                 NIX_AF_LSO_CFG[ENABLE], the send descriptor is for one or more
                                                                 packets of a TCP flow, and the related [LSO_*] fields are valid. */
        uint64_t tstmp                 : 1;  /**< [ 15: 15] PTP timestamp. A timestamp is requested when this bit is set and all of the
                                                                 following are true:
                                                                 * NIX_AF_LF()_TX_CFG[SEND_TSTMP_ENA] is set.
                                                                 * [LSO] is clear.
                                                                 * A NIX_SEND_MEM_S is present in the descriptor with NIX_SEND_MEM_S[ALG] =
                                                                 NIX_SENDMEMALG_E::SETTSTMP.

                                                                 This bit is ignored if any of the above conditions is false.

                                                                 If timestamp is requested, hardware writes the packet's timestamp
                                                                 (MIO_PTP_CLOCK_HI) to IOVA NIX_SEND_MEM_S[ADDR] when the targeted CGX LMAC
                                                                 transmits the packet.

                                                                 Hardware writes 0x1 to IOVA NIX_SEND_MEM_S[ADDR] if the timestamp cannot be
                                                                 captured due to one of the following:
                                                                 * PTP packet is multicast: NIX_TX_ACTION_S[OP] = NIX_TX_ACTIONOP_E::MCAST.
                                                                 * PTP packet is sent to SDP or LBK, i.e. the transmit channel is a
                                                                 NIX_CHAN_E::LBK()_CH() or NIX_CHAN_E::SDP_CH() value.
                                                                 * PTP packet is dropped by NIX.
                                                                 * PTP packet is sent to a CGX LMAC but is not transmitted by the LMAC
                                                                 within the timeout period specified by NIX_AF_TX_TSTMP_CFG[TSTMP_WD_PERIOD].

                                                                 If NIX_SQ_CTX_S[CQ_ENA] and software wishes to receive a CQE on timestamp
                                                                 completion, it must set NIX_SEND_HDR_S[PNC] = 1 and NIX_SEND_MEM_S[WMEM] =
                                                                 1.

                                                                 If NIX_SQ_CTX_S[SSO_ENA] and software wishes to add work to SSO on
                                                                 timestamp completion, it must set NIX_SEND_MEM_S[WMEM] = 1 and include
                                                                 NIX_SEND_WORK_S in the descriptor. */
        uint64_t lso_sb                : 8;  /**< [ 23: 16] Start bytes when [LSO] set. Location of the start byte of the TCP message
                                                                 payload, i.e. the size of the headers preceding the payload, excluding
                                                                 optional VLAN bytes inserted by [VLAN*] and Vtag bytes
                                                                 inserted by NIX_TX_VTAG_ACTION_S.

                                                                 Must be nonzero and less than NIX_SEND_HDR_S[TOTAL], else the send
                                                                 descriptor is treated as non-LSO. */
        uint64_t lso_format            : 5;  /**< [ 28: 24] Large send offload format. Valid when [LSO] is set and selects index {a}
                                                                 (FORMAT) of NIX_AF_LSO_FORMAT()_FIELD(). */
        uint64_t reserved_29_31        : 3;
        uint64_t shp_chg               : 9;  /**< [ 40: 32] Signed packet size adjustment. The packet size used for shaper {a} (PIR_ACCUM and
                                                                 CIR_ACCUM) and DWRR scheduler {a} (RR_COUNT) calculations at level {b} is:

                                                                 _  (NIX_AF_{b}{a}_SHAPE[LENGTH_DISABLE] ? 0 : (NIX_AF_{b}{a}_MD*[LENGTH] + [SHP_CHG]))
                                                                        + NIX_AF_{b}{a}_SHAPE[ADJUST]

                                                                 where {b} = TL1, TL2, TL3, TL4, or MDQ and {a} selects one of the shapers
                                                                 at the level selected by {b}.

                                                                 [SHP_CHG] values -255 .. 255 are allowed. [SHP_CHG] value 0x100 (i.e. -256)
                                                                 is reserved and must never be used.

                                                                 [SHP_CHG] becomes NIX_AF_{b}m_MD*[ADJUST].

                                                                 When [LSO] is set, hardware applies [SHP_CHG] to each LSO segment. */
        uint64_t shp_dis               : 1;  /**< [ 41: 41] Disables the shaper update and internal coloring algorithms used as
                                                                 the packet traverses MDQ through TL2 shapers. [SHP_DIS]
                                                                 has no effect on the L1 rate limiters.

                                                                 When [SHP_DIS] is 0 enabled CIR and PIR counters are used and adjusted
                                                                 per mode as the packet traverses through an enabled shaper. The
                                                                 internal color of a packet can be any of NIX_COLORRESULT_E::GREEN,
                                                                 NIX_COLORRESULT_E::YELLOW, NIX_COLORRESULT_E::RED_SEND, or
                                                                 NIX_COLORRESULT_E::RED_DROP after a shaper, depending on the shaper state
                                                                 and configuration.

                                                                 When [SHP_DIS] is 1 there is no packet coloring. No shaper can change
                                                                 the packet from its initial GREEN color. Neither the CIR nor PIR
                                                                 counters are used nor adjusted in any shaper as this packet traverses.
                                                                 Similar behavior to when both NIX_AF_TL*()_CIR[ENABLE] and
                                                                 NIX_AF_TL*()_PIR[ENABLE] are clear in all traversed shapers.

                                                                 See NIX_AF_TL*()_MD_DEBUG*[PIR_DIS,CIR_DIS]. When [SHP_DIS] is set,
                                                                 NIX_AF_TL*()_MD_DEBUG*[PIR_DIS,CIR_DIS] are both set. When [SHP_DIS] is
                                                                 clear, NIX_AF_TL*()_MD_DEBUG*[PIR_DIS,CIR_DIS] are both cleared and
                                                                 NIX_AF_TL*()_SHAPE[YELLOW_DISABLE,RED_DISABLE] determines the packet
                                                                 coloring of the shaper.

                                                                 When [LSO] is set, hardware applies [SHP_DIS] to each LSO segment. */
        uint64_t shp_ra                : 2;  /**< [ 43: 42] Red algorithm. Enumerated by NIX_REDALG_E. Specifies handling of a packet that
                                                                 traverses a RED MDQ through TL2 shaper. (A shaper is in RED state when
                                                                 NIX_AF_TL*()_SHAPE_STATE[COLOR]=0x2.) Has no effect when the packet traverses no
                                                                 shapers that are in the RED state. When [SHP_RA]!=STD, [SHP_RA] overrides the
                                                                 NIX_AF_TL*()_SHAPE[RED_ALGO] settings in all MDQ through TL2 shapers traversed
                                                                 by the packet. [SHP_RA] has no effect on the TL1 rate limiters. See
                                                                 NIX_AF_TL*()_MD_DEBUG*[RED_ALGO_OVERRIDE].

                                                                 When [LSO] is set in the descriptor, hardware applies [SHP_RA] to each LSO
                                                                 segment. */
        uint64_t markptr               : 8;  /**< [ 51: 44] Mark pointer. When [MARK_EN] is set, byte offset from packet start to byte
                                                                 to use for packet shaper marking. [MARKFORM] indirectly determines how this
                                                                 offset is used, including whether and how an L2 or L3 header is marked. See
                                                                 also [MARK_EN]. */
        uint64_t markform              : 7;  /**< [ 58: 52] Mark Format. When [MARK_EN] is set, the NIX_AF_MARK_FORMAT()_CTL register
                                                                 which specifies how NIX will mark NIX_COLORRESULT_E::YELLOW and
                                                                 NIX_COLORRESULT_E::RED_SEND packets. [MARKFORM] must be less than the size
                                                                 of the NIX_AF_MARK_FORMAT()_CTL array. See also [MARK_EN]. */
        uint64_t mark_en               : 1;  /**< [ 59: 59] Enable for packet shaper marking. When one, NIX_COLORRESULT_E::YELLOW and
                                                                 NIX_COLORRESULT_E::RED_SEND packets will be marked as specified by
                                                                 [MARKFORM] and [MARKPTR].

                                                                 When [LSO] and [MARK_EN] are both set in the descriptor, NIX marks each LSO
                                                                 segment independently, using [MARKPTR] and [MARKFORM] for every LSO
                                                                 segment. */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates send extended header. Enumerated by NIX_SUBDC_E::EXT. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t reserved_114_127      : 14;
        uint64_t vlan1_ins_ena         : 1;  /**< [113:113] VLAN 1 insert enable. See [VLAN0_INS_ENA]. */
        uint64_t vlan0_ins_ena         : 1;  /**< [112:112] VLAN 0 insert enable. If set, NIX inserts a VLAN tag at byte offset
                                                                 [VLAN0_INS_PTR] from the start of packet. The inserted VLAN tag consists of:
                                                                 * 16-bit Ethertype given by NIX_AF_LF()_TX_CFG[VLAN0_INS_ETYPE], followed by
                                                                 * 16-bit tag control information given by [VLAN0_INS_TCI].

                                                                 Up to two VLAN tags may be inserted in a packet due to [VLAN0_INS_ENA] and
                                                                 [VLAN1_INS_ENA]. If two VLAN tags are inserted, [VLAN0_INS_PTR] must
                                                                 be less than or equal to [VLAN1_INS_PTR]. VLAN 1 is always inserted after
                                                                 VLAN 0 in the packet's header, even if the two pointers are equal.

                                                                 A VLAN must not be inserted within an outer or inner L3/L4 header, but may be
                                                                 inserted within an outer L4 payload.

                                                                 The packet header is parsed by NPC after VLAN insertion. Note that the
                                                                 resulting NIX_TX_VTAG_ACTION_S[VTAG0_OP,VTAG1_OP] may replace or insert
                                                                 additional header bytes. Thus, Vtag may replace bytes that were inserted by
                                                                 [VLAN0_INS_*,VLAN1_INS_*]. */
        uint64_t vlan1_ins_tci         : 16; /**< [111: 96] VLAN 1 insert tag control information. See [VLAN1_INS_ENA]. */
        uint64_t vlan1_ins_ptr         : 8;  /**< [ 95: 88] VLAN 1 insert pointer. Byte offset from packet start to first inserted VLAN byte when
                                                                 [VLAN1_INS_ENA] is set. Must be even. */
        uint64_t vlan0_ins_tci         : 16; /**< [ 87: 72] VLAN 0 insert tag control information. See [VLAN0_INS_ENA]. */
        uint64_t vlan0_ins_ptr         : 8;  /**< [ 71: 64] VLAN 0 insert pointer. Byte offset from packet start to first inserted VLAN byte when
                                                                 [VLAN0_INS_ENA] is set. Must be even. */
#else /* Word 1 - Little Endian */
        uint64_t vlan0_ins_ptr         : 8;  /**< [ 71: 64] VLAN 0 insert pointer. Byte offset from packet start to first inserted VLAN byte when
                                                                 [VLAN0_INS_ENA] is set. Must be even. */
        uint64_t vlan0_ins_tci         : 16; /**< [ 87: 72] VLAN 0 insert tag control information. See [VLAN0_INS_ENA]. */
        uint64_t vlan1_ins_ptr         : 8;  /**< [ 95: 88] VLAN 1 insert pointer. Byte offset from packet start to first inserted VLAN byte when
                                                                 [VLAN1_INS_ENA] is set. Must be even. */
        uint64_t vlan1_ins_tci         : 16; /**< [111: 96] VLAN 1 insert tag control information. See [VLAN1_INS_ENA]. */
        uint64_t vlan0_ins_ena         : 1;  /**< [112:112] VLAN 0 insert enable. If set, NIX inserts a VLAN tag at byte offset
                                                                 [VLAN0_INS_PTR] from the start of packet. The inserted VLAN tag consists of:
                                                                 * 16-bit Ethertype given by NIX_AF_LF()_TX_CFG[VLAN0_INS_ETYPE], followed by
                                                                 * 16-bit tag control information given by [VLAN0_INS_TCI].

                                                                 Up to two VLAN tags may be inserted in a packet due to [VLAN0_INS_ENA] and
                                                                 [VLAN1_INS_ENA]. If two VLAN tags are inserted, [VLAN0_INS_PTR] must
                                                                 be less than or equal to [VLAN1_INS_PTR]. VLAN 1 is always inserted after
                                                                 VLAN 0 in the packet's header, even if the two pointers are equal.

                                                                 A VLAN must not be inserted within an outer or inner L3/L4 header, but may be
                                                                 inserted within an outer L4 payload.

                                                                 The packet header is parsed by NPC after VLAN insertion. Note that the
                                                                 resulting NIX_TX_VTAG_ACTION_S[VTAG0_OP,VTAG1_OP] may replace or insert
                                                                 additional header bytes. Thus, Vtag may replace bytes that were inserted by
                                                                 [VLAN0_INS_*,VLAN1_INS_*]. */
        uint64_t vlan1_ins_ena         : 1;  /**< [113:113] VLAN 1 insert enable. See [VLAN0_INS_ENA]. */
        uint64_t reserved_114_127      : 14;
#endif /* Word 1 - End */
    } s;
    /* struct bdk_nix_send_ext_s_s cn; */
};

/**
 * Structure nix_send_hdr_s
 *
 * NIX Send Header Subdescriptor Structure
 * The send header is the first subdescriptor of every send descriptor.
 */
union bdk_nix_send_hdr_s
{
    uint64_t u[2];
    struct bdk_nix_send_hdr_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t sq                    : 20; /**< [ 63: 44] Send queue within LF. Valid in the first NIX_SEND_HDR_S of an LMT store to
                                                                 NIX_LF_OP_SEND(). If multiple SQEs are enqueued by the LMT store,
                                                                 ignored in all NIX_SEND_HDR_S other than the first one.

                                                                 Internal:
                                                                 Included in LMTST, removed by hardware. */
        uint64_t pnc                   : 1;  /**< [ 43: 43] Post normal completion. If set along with NIX_SQ_CTX_S[CQ_ENA], a CQE is
                                                                 created with NIX_CQE_HDR_S[CQE_TYPE] = NIX_XQE_TYPE_E::SEND when the send
                                                                 descriptor's operation completes. If NIX_SEND_EXT_S[LSO] is set, a CQE is
                                                                 created when the send operation completes for the last LSO segment.

                                                                 If clear, no CQE is added on send completion.

                                                                 This bit is ignored when NIX_SQ_CTX_S[CQ_ENA] is clear.

                                                                 NIX will not add a CQE for this send descriptor until after it has
                                                                 completed all LLC/DRAM fetches that service all prior NIX_SEND_SG_S
                                                                 subdescriptors, and it has fetched all subdescriptors in the send
                                                                 descriptor. If NIX_SEND_MEM_S[WMEM]=1, NIX also will not post the CQE until
                                                                 all NIX_SEND_MEM_S subdescriptors in the descriptor complete and commit. */
        uint64_t sizem1                : 3;  /**< [ 42: 40] Number of 128-bit words in the SQE minus one. */
        uint64_t aura                  : 20; /**< [ 39: 20] Aura number. NPA aura to which buffers are optionally freed for packet segments from
                                                                 NIX_SEND_SG_S. See [DF] and NIX_SEND_SG_S[I]. */
        uint64_t df                    : 1;  /**< [ 19: 19] Don't free. If set, by default NIX will not free the surrounding buffer of
                                                                 a packet segment from NIX_SEND_SG_S. If clear, by default NIX will free the
                                                                 buffer. See NIX_SEND_SG_S[I]. */
        uint64_t reserved_18           : 1;
        uint64_t total                 : 18; /**< [ 17:  0] Total byte count to send, excluding optional VLAN bytes inserted by
                                                                 NIX_SEND_EXT_S[VLAN*] and Vtag bytes inserted by NIX_TX_VTAG_ACTION_S.

                                                                 For a non-LSO descriptor, total number of bytes, including any inserted
                                                                 VLAN and/or Vtag bytes, must not exceed NIX_AF_SMQ()_CFG[MAXLEN].

                                                                 For a LSO send descriptor (NIX_SEND_EXT_S[LSO] is present and set),
                                                                 specifies the total LSO payload size plus the size of the LSO header,
                                                                 excluding any inserted VLAN and/or Vtag bytes. In other words, the total
                                                                 LSO data payload size is [TOTAL] - NIX_SEND_EXT_S[LSO_SB].

                                                                 [TOTAL] does not include any of the outside FCS bytes that CGX may append
                                                                 to the packet(s). Hardware zero pads the packet when [TOTAL] is larger than
                                                                 the sum of all NIX_SEND_SG_S[SEG_SIZE*]s and NIX_SEND_IMM_S[SIZE]s in the
                                                                 descriptor. In addition, hardware zero pads the packet when
                                                                 NIX_AF_SMQ()_CFG[MINLEN] is larger than the sum of [TOTAL] and any inserted
                                                                 VLAN and Vtag bytes. */
#else /* Word 0 - Little Endian */
        uint64_t total                 : 18; /**< [ 17:  0] Total byte count to send, excluding optional VLAN bytes inserted by
                                                                 NIX_SEND_EXT_S[VLAN*] and Vtag bytes inserted by NIX_TX_VTAG_ACTION_S.

                                                                 For a non-LSO descriptor, total number of bytes, including any inserted
                                                                 VLAN and/or Vtag bytes, must not exceed NIX_AF_SMQ()_CFG[MAXLEN].

                                                                 For a LSO send descriptor (NIX_SEND_EXT_S[LSO] is present and set),
                                                                 specifies the total LSO payload size plus the size of the LSO header,
                                                                 excluding any inserted VLAN and/or Vtag bytes. In other words, the total
                                                                 LSO data payload size is [TOTAL] - NIX_SEND_EXT_S[LSO_SB].

                                                                 [TOTAL] does not include any of the outside FCS bytes that CGX may append
                                                                 to the packet(s). Hardware zero pads the packet when [TOTAL] is larger than
                                                                 the sum of all NIX_SEND_SG_S[SEG_SIZE*]s and NIX_SEND_IMM_S[SIZE]s in the
                                                                 descriptor. In addition, hardware zero pads the packet when
                                                                 NIX_AF_SMQ()_CFG[MINLEN] is larger than the sum of [TOTAL] and any inserted
                                                                 VLAN and Vtag bytes. */
        uint64_t reserved_18           : 1;
        uint64_t df                    : 1;  /**< [ 19: 19] Don't free. If set, by default NIX will not free the surrounding buffer of
                                                                 a packet segment from NIX_SEND_SG_S. If clear, by default NIX will free the
                                                                 buffer. See NIX_SEND_SG_S[I]. */
        uint64_t aura                  : 20; /**< [ 39: 20] Aura number. NPA aura to which buffers are optionally freed for packet segments from
                                                                 NIX_SEND_SG_S. See [DF] and NIX_SEND_SG_S[I]. */
        uint64_t sizem1                : 3;  /**< [ 42: 40] Number of 128-bit words in the SQE minus one. */
        uint64_t pnc                   : 1;  /**< [ 43: 43] Post normal completion. If set along with NIX_SQ_CTX_S[CQ_ENA], a CQE is
                                                                 created with NIX_CQE_HDR_S[CQE_TYPE] = NIX_XQE_TYPE_E::SEND when the send
                                                                 descriptor's operation completes. If NIX_SEND_EXT_S[LSO] is set, a CQE is
                                                                 created when the send operation completes for the last LSO segment.

                                                                 If clear, no CQE is added on send completion.

                                                                 This bit is ignored when NIX_SQ_CTX_S[CQ_ENA] is clear.

                                                                 NIX will not add a CQE for this send descriptor until after it has
                                                                 completed all LLC/DRAM fetches that service all prior NIX_SEND_SG_S
                                                                 subdescriptors, and it has fetched all subdescriptors in the send
                                                                 descriptor. If NIX_SEND_MEM_S[WMEM]=1, NIX also will not post the CQE until
                                                                 all NIX_SEND_MEM_S subdescriptors in the descriptor complete and commit. */
        uint64_t sq                    : 20; /**< [ 63: 44] Send queue within LF. Valid in the first NIX_SEND_HDR_S of an LMT store to
                                                                 NIX_LF_OP_SEND(). If multiple SQEs are enqueued by the LMT store,
                                                                 ignored in all NIX_SEND_HDR_S other than the first one.

                                                                 Internal:
                                                                 Included in LMTST, removed by hardware. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t sqe_id                : 16; /**< [127:112] Software defined SQE identifier copied to NIX_SEND_COMP_S[SQE_ID]. */
        uint64_t il4type               : 4;  /**< [111:108] Inner Layer 4 type enumerated by NIX_SENDL4TYPE_E. If checksum generation
                                                                 is specified, hardware includes all packet data following the inner layer 4
                                                                 header in the checksum calculation, excluding pad bytes added due to
                                                                 NIX_AF_SMQ()_CFG[MINLEN]. Hardware does not use IP or UDP length fields in
                                                                 the packet to determine the layer 4 checksum region. */
        uint64_t il3type               : 4;  /**< [107:104] Inner Layer 3 type enumerated by NIX_SENDL3TYPE_E. */
        uint64_t ol4type               : 4;  /**< [103:100] Outer Layer 4 type enumerated by NIX_SENDL4TYPE_E. If checksum generation
                                                                 is specified, hardware includes all packet data following the outer layer 4
                                                                 header in the checksum calculation, excluding pad bytes added due to
                                                                 NIX_AF_SMQ()_CFG[MINLEN]. Hardware does not use IP or UDP length fields in
                                                                 the packet to determine the layer 4 checksum region.

                                                                 When [OL4TYPE] = NIX_SENDL4TYPE_E::SCTP_CKSUM, [IL3TYPE] and [IL4TYPE] must
                                                                 not specify checksum generation. */
        uint64_t ol3type               : 4;  /**< [ 99: 96] Outer Layer 3 type enumerated by NIX_SENDL3TYPE_E. */
        uint64_t il4ptr                : 8;  /**< [ 95: 88] Inner Layer 4 pointer. Byte offset from packet start to first byte of
                                                                 inner L4 header, if present. Must be even. */
        uint64_t il3ptr                : 8;  /**< [ 87: 80] Inner Layer 3 pointer. Byte offset from packet start to first byte of
                                                                 inner L3 header, if present. Must be even. */
        uint64_t ol4ptr                : 8;  /**< [ 79: 72] Outer Layer 4 pointer. Byte offset from packet start to first byte of
                                                                 outer L4 header, if present. Must be even. */
        uint64_t ol3ptr                : 8;  /**< [ 71: 64] Outer Layer 3 pointer. Byte offset from packet start to first byte of
                                                                 outer L3 header, if present. Must be even. */
#else /* Word 1 - Little Endian */
        uint64_t ol3ptr                : 8;  /**< [ 71: 64] Outer Layer 3 pointer. Byte offset from packet start to first byte of
                                                                 outer L3 header, if present. Must be even. */
        uint64_t ol4ptr                : 8;  /**< [ 79: 72] Outer Layer 4 pointer. Byte offset from packet start to first byte of
                                                                 outer L4 header, if present. Must be even. */
        uint64_t il3ptr                : 8;  /**< [ 87: 80] Inner Layer 3 pointer. Byte offset from packet start to first byte of
                                                                 inner L3 header, if present. Must be even. */
        uint64_t il4ptr                : 8;  /**< [ 95: 88] Inner Layer 4 pointer. Byte offset from packet start to first byte of
                                                                 inner L4 header, if present. Must be even. */
        uint64_t ol3type               : 4;  /**< [ 99: 96] Outer Layer 3 type enumerated by NIX_SENDL3TYPE_E. */
        uint64_t ol4type               : 4;  /**< [103:100] Outer Layer 4 type enumerated by NIX_SENDL4TYPE_E. If checksum generation
                                                                 is specified, hardware includes all packet data following the outer layer 4
                                                                 header in the checksum calculation, excluding pad bytes added due to
                                                                 NIX_AF_SMQ()_CFG[MINLEN]. Hardware does not use IP or UDP length fields in
                                                                 the packet to determine the layer 4 checksum region.

                                                                 When [OL4TYPE] = NIX_SENDL4TYPE_E::SCTP_CKSUM, [IL3TYPE] and [IL4TYPE] must
                                                                 not specify checksum generation. */
        uint64_t il3type               : 4;  /**< [107:104] Inner Layer 3 type enumerated by NIX_SENDL3TYPE_E. */
        uint64_t il4type               : 4;  /**< [111:108] Inner Layer 4 type enumerated by NIX_SENDL4TYPE_E. If checksum generation
                                                                 is specified, hardware includes all packet data following the inner layer 4
                                                                 header in the checksum calculation, excluding pad bytes added due to
                                                                 NIX_AF_SMQ()_CFG[MINLEN]. Hardware does not use IP or UDP length fields in
                                                                 the packet to determine the layer 4 checksum region. */
        uint64_t sqe_id                : 16; /**< [127:112] Software defined SQE identifier copied to NIX_SEND_COMP_S[SQE_ID]. */
#endif /* Word 1 - End */
    } s;
    /* struct bdk_nix_send_hdr_s_s cn; */
};

/**
 * Structure nix_send_imm_s
 *
 * NIX Send Immediate Subdescriptor Structure
 * The send immediate subdescriptor requests that bytes immediately following this
 * NIX_SEND_IMM_S (after skipping [APAD] bytes) are to be included in the packet data.
 * The next subdescriptor following this NIX_SEND_IMM_S (when one exists) will
 * follow the immediate bytes, after rounding up the address to a multiple of 16 bytes.
 *
 * There may be multiple NIX_SEND_IMM_S in one NIX send descriptor. A
 * NIX_SEND_IMM_S is ignored in a NIX send descriptor if the sum of all prior
 * NIX_SEND_SG_S[SEG*_SIZE]s and NIX_SEND_IMM_S[SIZE]s meets or exceeds
 * NIX_SEND_HDR_S[TOTAL].
 *
 * When NIX_SEND_EXT_S[LSO] is set in the descriptor, all NIX_SEND_IMM_S
 * bytes must be included in the first NIX_SEND_EXT_S[LSO_SB] bytes of the
 * source packet.
 */
union bdk_nix_send_imm_s
{
    uint64_t u;
    struct bdk_nix_send_imm_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates immediate. Enumerated by NIX_SUBDC_E::IMM. */
        uint64_t reserved_19_59        : 41;
        uint64_t apad                  : 3;  /**< [ 18: 16] Alignment pad. Number of bytes to skip following this 64-bit structure before
                                                                 the first byte to be included in the packet data. */
        uint64_t size                  : 16; /**< [ 15:  0] Size of immediate data (in bytes) that follows this 64-bit structure after
                                                                 skipping [APAD] bytes. The next subdescriptor follows [APAD]+[SIZE] bytes
                                                                 later in the descriptor, rounded up to the next 16-byte aligned address.
                                                                 [SIZE] must be greater than 0, and [APAD]+[SIZE] must be less than or equal
                                                                 to 264 bytes. */
#else /* Word 0 - Little Endian */
        uint64_t size                  : 16; /**< [ 15:  0] Size of immediate data (in bytes) that follows this 64-bit structure after
                                                                 skipping [APAD] bytes. The next subdescriptor follows [APAD]+[SIZE] bytes
                                                                 later in the descriptor, rounded up to the next 16-byte aligned address.
                                                                 [SIZE] must be greater than 0, and [APAD]+[SIZE] must be less than or equal
                                                                 to 264 bytes. */
        uint64_t apad                  : 3;  /**< [ 18: 16] Alignment pad. Number of bytes to skip following this 64-bit structure before
                                                                 the first byte to be included in the packet data. */
        uint64_t reserved_19_59        : 41;
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates immediate. Enumerated by NIX_SUBDC_E::IMM. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_send_imm_s_s cn; */
};

/**
 * Structure nix_send_jump_s
 *
 * NIX Send Jump Subdescriptor Structure
 * The send jump subdescriptor selects a new address for fetching the remaining
 * subdescriptors of a send descriptor. This allows software to create a send
 * descriptor longer than SQE size selected by NIX_SQ_CTX_S[MAX_SQE_SIZE].
 *
 * There can be only one NIX_SEND_JUMP_S subdescriptor in a send descriptor. If
 * present, it must immediately follow NIX_SEND_HDR_S if NIX_SEND_EXT_S is not
 * present, else it must immediately follow NIX_SEND_EXT_S. In either case, it
 * must terminate the SQE enqueued by software.
 */
union bdk_nix_send_jump_s
{
    uint64_t u[2];
    struct bdk_nix_send_jump_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates send jump. Enumerated by NIX_SUBDC_E::JUMP. */
        uint64_t f                     : 1;  /**< [ 59: 59] Free.
                                                                 0 = Hardware will not free the buffer indicated by [ADDR].
                                                                 1 = Hardware will free the buffer indicated by [ADDR] to NPA after it has read all
                                                                 subdescriptors from it.

                                                                 NIX sends [ADDR] to NPA as part of the buffer free when [F] is set. Either an NPA
                                                                 naturally-aligned pool or opaque pool may be appropriate. Refer to the NPA chapter. */
        uint64_t reserved_36_58        : 23;
        uint64_t aura                  : 20; /**< [ 35: 16] Aura number. See [F]. */
        uint64_t ld_type               : 2;  /**< [ 15: 14] Specifies load transaction type to use for reading post-jump
                                                                 subdescriptors. Enumerated by NIX_SENDLDTYPE_E. */
        uint64_t reserved_7_13         : 7;
        uint64_t sizem1                : 7;  /**< [  6:  0] Number of 16-byte subdescriptor words (minus one) in the subdescriptor list that [ADDR]
                                                                 points to. */
#else /* Word 0 - Little Endian */
        uint64_t sizem1                : 7;  /**< [  6:  0] Number of 16-byte subdescriptor words (minus one) in the subdescriptor list that [ADDR]
                                                                 points to. */
        uint64_t reserved_7_13         : 7;
        uint64_t ld_type               : 2;  /**< [ 15: 14] Specifies load transaction type to use for reading post-jump
                                                                 subdescriptors. Enumerated by NIX_SENDLDTYPE_E. */
        uint64_t aura                  : 20; /**< [ 35: 16] Aura number. See [F]. */
        uint64_t reserved_36_58        : 23;
        uint64_t f                     : 1;  /**< [ 59: 59] Free.
                                                                 0 = Hardware will not free the buffer indicated by [ADDR].
                                                                 1 = Hardware will free the buffer indicated by [ADDR] to NPA after it has read all
                                                                 subdescriptors from it.

                                                                 NIX sends [ADDR] to NPA as part of the buffer free when [F] is set. Either an NPA
                                                                 naturally-aligned pool or opaque pool may be appropriate. Refer to the NPA chapter. */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates send jump. Enumerated by NIX_SUBDC_E::JUMP. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t addr                  : 64; /**< [127: 64] IOVA of the first byte of the next subdescriptor. See NIX_IOVA_S[ADDR]. Bits
                                                                 \<3:0\> are ignored; address must be 16-byte aligned.

                                                                 If NIX_AF_LF()_CFG[BE] is set for this LF (VF/PF), [ADDR] points to big-endian
                                                                 instructions, otherwise little-endian. */
#else /* Word 1 - Little Endian */
        uint64_t addr                  : 64; /**< [127: 64] IOVA of the first byte of the next subdescriptor. See NIX_IOVA_S[ADDR]. Bits
                                                                 \<3:0\> are ignored; address must be 16-byte aligned.

                                                                 If NIX_AF_LF()_CFG[BE] is set for this LF (VF/PF), [ADDR] points to big-endian
                                                                 instructions, otherwise little-endian. */
#endif /* Word 1 - End */
    } s;
    /* struct bdk_nix_send_jump_s_s cn; */
};

/**
 * Structure nix_send_mem_s
 *
 * NIX Send Memory Subdescriptor Structure
 * The send memory subdescriptor atomically sets, increments or decrements a memory location.
 *
 * NIX_SEND_MEM_S subdescriptors must follow all NIX_SEND_SG_S and NIX_SEND_IMM_S
 * subdescriptors in the NIX send descriptor. NIX will not initiate the memory
 * update for this subdescriptor until after it has completed all LLC/DRAM fetches
 * that service all prior NIX_SEND_SG_S subdescriptors.
 *
 * Performance is best if a memory decrement by one is used rather than any other memory
 * set/increment/decrement. (Less internal bus bandwidth is used with memory decrements by one.)
 *
 * When NIX_SEND_EXT_S[LSO] is set in the descriptor, NIX executes the
 * NIX_SEND_MEM_S work add only while processing the last LSO segment, after
 * processing prior segments.
 */
union bdk_nix_send_mem_s
{
    uint64_t u[2];
    struct bdk_nix_send_mem_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates send memory. Enumerated by NIX_SUBDC_E::MEM. */
        uint64_t alg                   : 4;  /**< [ 59: 56] Adder algorithm. How to modify the memory location, for example by setting or atomically
                                                                 incrementing. Enumerated by NIX_SENDMEMALG_E. */
        uint64_t dsz                   : 2;  /**< [ 55: 54] Memory data size. The size of the word in memory, enumerated by NIX_SENDMEMDSZ_E. */
        uint64_t wmem                  : 1;  /**< [ 53: 53] Wait for memory.
                                                                 0 = The memory operation may complete after the CQE is posted and/or ADD_WORK is
                                                                 initiated, and potentially after software has begun servicing the
                                                                 work/completion.
                                                                 1 = NIX will wait for this NIX_SEND_MEM_S requested memory operation to
                                                                 complete and commit before adding a send completion CQE for the send
                                                                 descriptor if NIX_SEND_HDR_S[PNC] is set, and before initiating SSO add
                                                                 work for any NIX_SEND_WORK_S in the descriptor. This may have reduced
                                                                 performance over not waiting. */
        uint64_t reserved_16_52        : 37;
        uint64_t offset                : 16; /**< [ 15:  0] Adder offset. Constant value to add or subtract or set. If the count being
                                                                 modified is to represent the true packet size, then the offset may
                                                                 represent the pad and FCS appended to the packet.

                                                                 Internal:
                                                                 Note IOB hardware has a special encoding for atomic decrement,
                                                                 therefore a change of minus one is twice as IOB bandwidth efficient as adding/subtracting
                                                                 other values or setting. */
#else /* Word 0 - Little Endian */
        uint64_t offset                : 16; /**< [ 15:  0] Adder offset. Constant value to add or subtract or set. If the count being
                                                                 modified is to represent the true packet size, then the offset may
                                                                 represent the pad and FCS appended to the packet.

                                                                 Internal:
                                                                 Note IOB hardware has a special encoding for atomic decrement,
                                                                 therefore a change of minus one is twice as IOB bandwidth efficient as adding/subtracting
                                                                 other values or setting. */
        uint64_t reserved_16_52        : 37;
        uint64_t wmem                  : 1;  /**< [ 53: 53] Wait for memory.
                                                                 0 = The memory operation may complete after the CQE is posted and/or ADD_WORK is
                                                                 initiated, and potentially after software has begun servicing the
                                                                 work/completion.
                                                                 1 = NIX will wait for this NIX_SEND_MEM_S requested memory operation to
                                                                 complete and commit before adding a send completion CQE for the send
                                                                 descriptor if NIX_SEND_HDR_S[PNC] is set, and before initiating SSO add
                                                                 work for any NIX_SEND_WORK_S in the descriptor. This may have reduced
                                                                 performance over not waiting. */
        uint64_t dsz                   : 2;  /**< [ 55: 54] Memory data size. The size of the word in memory, enumerated by NIX_SENDMEMDSZ_E. */
        uint64_t alg                   : 4;  /**< [ 59: 56] Adder algorithm. How to modify the memory location, for example by setting or atomically
                                                                 incrementing. Enumerated by NIX_SENDMEMALG_E. */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates send memory. Enumerated by NIX_SUBDC_E::MEM. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t addr                  : 64; /**< [127: 64] IOVA of the LLC/DRAM address to be modified.
                                                                 [ADDR] must be naturally aligned to the size specified in [DSZ].
                                                                 Bits \<63:53\> are ignored by hardware; software should use a sign-extended
                                                                 bit \<52\> for forward compatibility.
                                                                 If NIX_AF_LF()_CFG[BE] is set for this LF (VF/PF), [ADDR] is a big-endian byte
                                                                 pointer. Otherwise, [ADDR] is a little-endian byte pointer. */
#else /* Word 1 - Little Endian */
        uint64_t addr                  : 64; /**< [127: 64] IOVA of the LLC/DRAM address to be modified.
                                                                 [ADDR] must be naturally aligned to the size specified in [DSZ].
                                                                 Bits \<63:53\> are ignored by hardware; software should use a sign-extended
                                                                 bit \<52\> for forward compatibility.
                                                                 If NIX_AF_LF()_CFG[BE] is set for this LF (VF/PF), [ADDR] is a big-endian byte
                                                                 pointer. Otherwise, [ADDR] is a little-endian byte pointer. */
#endif /* Word 1 - End */
    } s;
    /* struct bdk_nix_send_mem_s_s cn; */
};

/**
 * Structure nix_send_sg_s
 *
 * NIX Send Scatter/Gather Subdescriptor Structure
 * The send scatter/gather subdescriptor requests one to three segments of packet data
 * bytes to be transmitted.
 * There may be multiple NIX_SEND_SG_Ss in each NIX send descriptor. A
 * NIX_SEND_SG_S is ignored in a NIX send descriptor if the sum of all prior
 * NIX_SEND_SG_S[SEG*_SIZE]s and NIX_SEND_IMM_S[SIZE]s meets or exceeds NIX_SEND_HDR_S[TOTAL].
 *
 * NIX_SEND_SG_S is immediately followed by one NIX_IOVA_S word when [SEGS] = 1,
 * three NIX_IOVA_S words when [SEGS] \>= 2. Each NIX_IOVA_S word specifies the
 * IOVA of first packet data byte in the corresponding segment; first NIX_IOVA_S
 * word for segment 1, second word for segment 2, third word for segment 3. Note
 * the third word is present when [SEGS] \>= 2 but only valid when [SEGS] = 3.
 */
union bdk_nix_send_sg_s
{
    uint64_t u;
    struct bdk_nix_send_sg_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates scatter/gather. Enumerated by NIX_SUBDC_E::SG. */
        uint64_t ld_type               : 2;  /**< [ 59: 58] Specifies load transaction type to use for reading segment bytes. Enumerated by
                                                                 NIX_SENDLDTYPE_E. */
        uint64_t i                     : 1;  /**< [ 57: 57] Invert free. NIX frees the surrounding buffer of each of the [SEGS] valid
                                                                 segments when:

                                                                 _  NIX_SEND_HDR_S[DF] == [I].

                                                                 NIX naturally aligns [SEG*_ADDR] to 128 bytes before sending it to NPA as part of
                                                                 the buffer free. An NPA naturally-aligned pool is recommended, though opaque
                                                                 pool mode may also be possible. Refer to the NPA chapter.

                                                                 NIX frees the buffer to NIX_SEND_HDR_S[AURA]. If any [SEG*_SIZE] is zero for any
                                                                 valid segment, NIX will not read any segment data and but will free the buffer for
                                                                 that segment. */
        uint64_t reserved_50_56        : 7;
        uint64_t segs                  : 2;  /**< [ 49: 48] Number of valid segments. Must be nonzero. */
        uint64_t seg3_size             : 16; /**< [ 47: 32] Size of segment 3 in bytes. Valid when [SEGS] = 3. */
        uint64_t seg2_size             : 16; /**< [ 31: 16] Size of segment 2 in bytes. Valid when [SEGS] \>= 2. */
        uint64_t seg1_size             : 16; /**< [ 15:  0] Size of segment 1 in bytes. */
#else /* Word 0 - Little Endian */
        uint64_t seg1_size             : 16; /**< [ 15:  0] Size of segment 1 in bytes. */
        uint64_t seg2_size             : 16; /**< [ 31: 16] Size of segment 2 in bytes. Valid when [SEGS] \>= 2. */
        uint64_t seg3_size             : 16; /**< [ 47: 32] Size of segment 3 in bytes. Valid when [SEGS] = 3. */
        uint64_t segs                  : 2;  /**< [ 49: 48] Number of valid segments. Must be nonzero. */
        uint64_t reserved_50_56        : 7;
        uint64_t i                     : 1;  /**< [ 57: 57] Invert free. NIX frees the surrounding buffer of each of the [SEGS] valid
                                                                 segments when:

                                                                 _  NIX_SEND_HDR_S[DF] == [I].

                                                                 NIX naturally aligns [SEG*_ADDR] to 128 bytes before sending it to NPA as part of
                                                                 the buffer free. An NPA naturally-aligned pool is recommended, though opaque
                                                                 pool mode may also be possible. Refer to the NPA chapter.

                                                                 NIX frees the buffer to NIX_SEND_HDR_S[AURA]. If any [SEG*_SIZE] is zero for any
                                                                 valid segment, NIX will not read any segment data and but will free the buffer for
                                                                 that segment. */
        uint64_t ld_type               : 2;  /**< [ 59: 58] Specifies load transaction type to use for reading segment bytes. Enumerated by
                                                                 NIX_SENDLDTYPE_E. */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates scatter/gather. Enumerated by NIX_SUBDC_E::SG. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_send_sg_s_s cn; */
};

/**
 * Structure nix_send_work_s
 *
 * NIX Send Work Subdescriptor Structure
 * This subdescriptor adds work to the SSO. At most one NIX_SEND_WORK_S subdescriptor
 * can exist in the NIX send descriptor. If a NIX_SEND_WORK_S exists in the
 * descriptor, it must be the last subdescriptor. NIX will not initiate the work add
 * for this subdescriptor until after (1) it has completed all LLC/DRAM fetches that
 * service all prior NIX_SEND_SG_S subdescriptors, (2) it has
 * fetched all subdescriptors in the descriptor, and (3) all NIX_SEND_MEM_S[WMEM]=1
 * LLC/DRAM updates have completed.
 *
 * Provided the path of descriptors from the SQ through NIX to an output FIFO is
 * unmodified between the descriptors (as should normally be the case, but it is
 * possible for software to change the path), NIX also (1) will submit
 * the SSO add works from all descriptors in the SQ in order, and
 * (2) will not submit an SSO work add until after all prior descriptors
 * in the SQ have completed their NIX_SEND_SG_S
 * processing, and (3) will not submit an SSO work add until after
 * it has fetched all subdescriptors from prior descriptors in the SQ.
 *
 * When NIX_SEND_EXT_S[LSO] is set in the descriptor, NIX executes the
 * NIX_SEND_WORK_S work add only while processing the last LSO segment, after
 * processing prior segments.
 *
 * Hardware ignores NIX_SEND_WORK_S when NIX_SQ_CTX_S[SSO_ENA] is clear.
 */
union bdk_nix_send_work_s
{
    uint64_t u[2];
    struct bdk_nix_send_work_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates send work. Enumerated by NIX_SUBDC_E::WORK. */
        uint64_t reserved_44_59        : 16;
        uint64_t grp                   : 10; /**< [ 43: 34] SSO group. The SSO group number to add work to. Note the upper two bits
                                                                 correspond to a node number. */
        uint64_t tt                    : 2;  /**< [ 33: 32] SSO tag type. The SSO tag type number to add work with. */
        uint64_t tag                   : 32; /**< [ 31:  0] The SSO tag to use when NIX submits work to SSO. */
#else /* Word 0 - Little Endian */
        uint64_t tag                   : 32; /**< [ 31:  0] The SSO tag to use when NIX submits work to SSO. */
        uint64_t tt                    : 2;  /**< [ 33: 32] SSO tag type. The SSO tag type number to add work with. */
        uint64_t grp                   : 10; /**< [ 43: 34] SSO group. The SSO group number to add work to. Note the upper two bits
                                                                 correspond to a node number. */
        uint64_t reserved_44_59        : 16;
        uint64_t subdc                 : 4;  /**< [ 63: 60] Subdescriptor code. Indicates send work. Enumerated by NIX_SUBDC_E::WORK. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t addr                  : 64; /**< [127: 64] IOVA of the work-queue entry to be submitted to the SSO. See NIX_IOVA_S[ADDR].
                                                                 Bits \<2:0\> are ignored; address must be eight-byte aligned. */
#else /* Word 1 - Little Endian */
        uint64_t addr                  : 64; /**< [127: 64] IOVA of the work-queue entry to be submitted to the SSO. See NIX_IOVA_S[ADDR].
                                                                 Bits \<2:0\> are ignored; address must be eight-byte aligned. */
#endif /* Word 1 - End */
    } s;
    /* struct bdk_nix_send_work_s_s cn; */
};

/**
 * Structure nix_sq_ctx_hw_s
 *
 * NIX SQ Context Hardware Structure
 * This structure contains context state maintained by hardware for each SQ in
 * in LLC/DRAM. Software uses the equivalent NIX_SQ_CTX_S structure format to read
 * and write an SQ context with the NIX admin queue.
 */
union bdk_nix_sq_ctx_hw_s
{
    uint64_t u[16];
    struct bdk_nix_sq_ctx_hw_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t gbl_rsvd1             : 5;  /**< [ 63: 59] Reserved. */
        uint64_t gbl_sqb_aura          : 20; /**< [ 58: 39] See NIX_SQ_CTX_S[SQB_AURA]. This is the last entry for the enqueue engine requests. */
        uint64_t gbl_sqe_way_mask      : 16; /**< [ 38: 23] See NIX_SQ_CTX_S[SQE_WAY_MASK]. */
        uint64_t gbl_max_sqe_size      : 2;  /**< [ 22: 21] See NIX_SQ_CTX_S[MAX_SQE_SIZE]. */
        uint64_t gbl_substream         : 20; /**< [ 20:  1] See NIX_SQ_CTX_S[SUBSTREAM]. */
        uint64_t gbl_ena               : 1;  /**< [  0:  0] See NIX_SQ_CTX_S[ENA]. */
#else /* Word 0 - Little Endian */
        uint64_t gbl_ena               : 1;  /**< [  0:  0] See NIX_SQ_CTX_S[ENA]. */
        uint64_t gbl_substream         : 20; /**< [ 20:  1] See NIX_SQ_CTX_S[SUBSTREAM]. */
        uint64_t gbl_max_sqe_size      : 2;  /**< [ 22: 21] See NIX_SQ_CTX_S[MAX_SQE_SIZE]. */
        uint64_t gbl_sqe_way_mask      : 16; /**< [ 38: 23] See NIX_SQ_CTX_S[SQE_WAY_MASK]. */
        uint64_t gbl_sqb_aura          : 20; /**< [ 58: 39] See NIX_SQ_CTX_S[SQB_AURA]. This is the last entry for the enqueue engine requests. */
        uint64_t gbl_rsvd1             : 5;  /**< [ 63: 59] Reserved. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t gbl_rsvd              : 12; /**< [127:116] Reserved. */
        uint64_t send_lso_segnum       : 8;  /**< [115:108] See NIX_SQ_CTX_S[SEND_LSO_SEGNUM] - NA. */
        uint64_t sq_int_ena            : 8;  /**< [107:100] See NIX_SQ_CTX_S[SQ_INT_ENA]. */
        uint64_t sq_int                : 8;  /**< [ 99: 92] See NIX_SQ_CTX_S[SQ_INT]. */
        uint64_t qint_idx              : 7;  /**< [ 91: 85] See NIX_SQ_CTX_S[QINT_IDX]. */
        uint64_t gbl_cq_ena            : 1;  /**< [ 84: 84] See NIX_SQ_CTX_S[CQ_ENA]. */
        uint64_t gbl_cq_id             : 20; /**< [ 83: 64] See NIX_SQ_CTX_S[CQ]. */
#else /* Word 1 - Little Endian */
        uint64_t gbl_cq_id             : 20; /**< [ 83: 64] See NIX_SQ_CTX_S[CQ]. */
        uint64_t gbl_cq_ena            : 1;  /**< [ 84: 84] See NIX_SQ_CTX_S[CQ_ENA]. */
        uint64_t qint_idx              : 7;  /**< [ 91: 85] See NIX_SQ_CTX_S[QINT_IDX]. */
        uint64_t sq_int                : 8;  /**< [ 99: 92] See NIX_SQ_CTX_S[SQ_INT]. */
        uint64_t sq_int_ena            : 8;  /**< [107:100] See NIX_SQ_CTX_S[SQ_INT_ENA]. */
        uint64_t send_lso_segnum       : 8;  /**< [115:108] See NIX_SQ_CTX_S[SEND_LSO_SEGNUM] - NA. */
        uint64_t gbl_rsvd              : 12; /**< [127:116] Reserved. */
#endif /* Word 1 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 2 - Big Endian */
        uint64_t dnq_rsvd1             : 38; /**< [191:154] Reserved. */
        uint64_t lmt_dis               : 1;  /**< [153:153] See NIX_SQ_CTX_S[LMT_DIS]. */
        uint64_t tail_offset           : 6;  /**< [152:147] See NIX_SQ_CTX_S[TAIL_OFFSET]. */
        uint64_t next_sqb_valid        : 1;  /**< [146:146] See NIX_SQ_CTX_S[NEXT_SQB_VALID]. */
        uint64_t sqe_stype             : 2;  /**< [145:144] See NIX_SQ_CTX_S[SQE_STYPE]. */
        uint64_t sqb_enqueue_count     : 16; /**< [143:128] Used in combination with [SQB_DEQUEUE_COUNT] to respond back to Software
                                                                 for AQ reads corresponding to NIX_SQ_CTX_S[SQB_COUNT] =
                                                                 [SQB_ENQUEUE_COUNT] - [SQB_DEQUEUE_COUNT]. */
#else /* Word 2 - Little Endian */
        uint64_t sqb_enqueue_count     : 16; /**< [143:128] Used in combination with [SQB_DEQUEUE_COUNT] to respond back to Software
                                                                 for AQ reads corresponding to NIX_SQ_CTX_S[SQB_COUNT] =
                                                                 [SQB_ENQUEUE_COUNT] - [SQB_DEQUEUE_COUNT]. */
        uint64_t sqe_stype             : 2;  /**< [145:144] See NIX_SQ_CTX_S[SQE_STYPE]. */
        uint64_t next_sqb_valid        : 1;  /**< [146:146] See NIX_SQ_CTX_S[NEXT_SQB_VALID]. */
        uint64_t tail_offset           : 6;  /**< [152:147] See NIX_SQ_CTX_S[TAIL_OFFSET]. */
        uint64_t lmt_dis               : 1;  /**< [153:153] See NIX_SQ_CTX_S[LMT_DIS]. */
        uint64_t dnq_rsvd1             : 38; /**< [191:154] Reserved. */
#endif /* Word 2 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 3 - Big Endian */
        uint64_t tail_sqb              : 64; /**< [255:192] See NIX_SQ_CTX_S[TAIL_SQB]. */
#else /* Word 3 - Little Endian */
        uint64_t tail_sqb              : 64; /**< [255:192] See NIX_SQ_CTX_S[TAIL_SQB]. */
#endif /* Word 3 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 4 - Big Endian */
        uint64_t next_sqb              : 64; /**< [319:256] See NIX_SQ_CTX_S[NEXT_SQB]. */
#else /* Word 4 - Little Endian */
        uint64_t next_sqb              : 64; /**< [319:256] See NIX_SQ_CTX_S[NEXT_SQB]. */
#endif /* Word 4 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 5 - Big Endian */
        uint64_t scm1_rsvd2            : 31; /**< [383:353] Reserved. */
        uint64_t smq_next_sq_vld       : 1;  /**< [352:352] See NIX_SQ_CTX[SMQ_NEXT_SQ] is valid. */
        uint64_t smq_next_sq           : 20; /**< [351:332] See NIX_SQ_CTX[SMQ_NEXT_SQ]. */
        uint64_t smq_pend              : 1;  /**< [331:331] See NIX_SQ_CTX_S[SMQ_PEND]. */
        uint64_t smq                   : 10; /**< [330:321] See NIX_SQ_CTX_S[SMQ]. */
        uint64_t mnq_dis               : 1;  /**< [320:320] See NIX_SQ_CTX_S[MNQ_DIS]. */
#else /* Word 5 - Little Endian */
        uint64_t mnq_dis               : 1;  /**< [320:320] See NIX_SQ_CTX_S[MNQ_DIS]. */
        uint64_t smq                   : 10; /**< [330:321] See NIX_SQ_CTX_S[SMQ]. */
        uint64_t smq_pend              : 1;  /**< [331:331] See NIX_SQ_CTX_S[SMQ_PEND]. */
        uint64_t smq_next_sq           : 20; /**< [351:332] See NIX_SQ_CTX[SMQ_NEXT_SQ]. */
        uint64_t smq_next_sq_vld       : 1;  /**< [352:352] See NIX_SQ_CTX[SMQ_NEXT_SQ] is valid. */
        uint64_t scm1_rsvd2            : 31; /**< [383:353] Reserved. */
#endif /* Word 5 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 6 - Big Endian */
        uint64_t smenq_sqb             : 64; /**< [447:384] See NIX_SQ_CTX_S[SMENQ_SQB]. */
#else /* Word 6 - Little Endian */
        uint64_t smenq_sqb             : 64; /**< [447:384] See NIX_SQ_CTX_S[SMENQ_SQB]. */
#endif /* Word 6 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 7 - Big Endian */
        uint64_t smq_rr_count          : 25; /**< [511:487] See NIX_SQ_CTX[SMQ_RR_COUNT]. */
        uint64_t smq_rr_quantum        : 24; /**< [486:463] See NIX_SQ_CTX[SMQ_RR_QUANTUM]. */
        uint64_t xoff                  : 1;  /**< [462:462] See NIX_SQ_CTX[XOFF]. */
        uint64_t cq_limit              : 8;  /**< [461:454] See NIX_SQ_CTX[CQ_LIMIT]. */
        uint64_t smenq_offset          : 6;  /**< [453:448] See NIX_SQ_CTX_S[SMENQ_OFFSET]. */
#else /* Word 7 - Little Endian */
        uint64_t smenq_offset          : 6;  /**< [453:448] See NIX_SQ_CTX_S[SMENQ_OFFSET]. */
        uint64_t cq_limit              : 8;  /**< [461:454] See NIX_SQ_CTX[CQ_LIMIT]. */
        uint64_t xoff                  : 1;  /**< [462:462] See NIX_SQ_CTX[XOFF]. */
        uint64_t smq_rr_quantum        : 24; /**< [486:463] See NIX_SQ_CTX[SMQ_RR_QUANTUM]. */
        uint64_t smq_rr_count          : 25; /**< [511:487] See NIX_SQ_CTX[SMQ_RR_COUNT]. */
#endif /* Word 7 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 8 - Big Endian */
        uint64_t vfi_lso_shp_chg       : 9;  /**< [575:567] Used for VF-Isolation. See NIX_SEND_EXT_S[SHP_CHG]. */
        uint64_t vfi_lso_shp_ra        : 2;  /**< [566:565] Used for VF-Isolation. See NIX_SEND_EXT_S[SHP_RA]. */
        uint64_t vfi_lso_sizem1        : 7;  /**< [564:558] Used for VF-Isolation. See NIX_SEND_HDR_S[SIZEM1]. */
        uint64_t vfi_lso_total         : 18; /**< [557:540] Used for VF-Isolation. See NIX_SEND_HDR_S[TOTAL]. */
        uint64_t smq_lso_segnum        : 8;  /**< [539:532] See NIX_SQ_CTX[SMQ_LSO_SEGNUM]. */
        uint64_t scm2_rsvd2            : 20; /**< [531:512] See NIX_SQ_CTX[SMQ_NEXT_SQ]. */
#else /* Word 8 - Little Endian */
        uint64_t scm2_rsvd2            : 20; /**< [531:512] See NIX_SQ_CTX[SMQ_NEXT_SQ]. */
        uint64_t smq_lso_segnum        : 8;  /**< [539:532] See NIX_SQ_CTX[SMQ_LSO_SEGNUM]. */
        uint64_t vfi_lso_total         : 18; /**< [557:540] Used for VF-Isolation. See NIX_SEND_HDR_S[TOTAL]. */
        uint64_t vfi_lso_sizem1        : 7;  /**< [564:558] Used for VF-Isolation. See NIX_SEND_HDR_S[SIZEM1]. */
        uint64_t vfi_lso_shp_ra        : 2;  /**< [566:565] Used for VF-Isolation. See NIX_SEND_EXT_S[SHP_RA]. */
        uint64_t vfi_lso_shp_chg       : 9;  /**< [575:567] Used for VF-Isolation. See NIX_SEND_EXT_S[SHP_CHG]. */
#endif /* Word 8 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 9 - Big Endian */
        uint64_t scm2_rsvd1            : 38; /**< [639:602] Reserved. */
        uint64_t vfi_lso_vld           : 1;  /**< [601:601] Used for VF-Isolation. See NIX_SEND_EXT_S[LSO]. */
        uint64_t vfi_lso_vlan1_ins_ena : 1;  /**< [600:600] Used for VF-Isolation. See NIX_SEND_EXT_S[VLAN1_INS_ENA]. */
        uint64_t vfi_lso_vlan0_ins_ena : 1;  /**< [599:599] Used for VF-Isolation. See NIX_SEND_EXT_S[VLAN0_INS_ENA]. */
        uint64_t vfi_lso_mps           : 14; /**< [598:585] Used for VF-Isolation. See NIX_SEND_EXT_S[LSO_MPS]. */
        uint64_t vfi_lso_sb            : 8;  /**< [584:577] Used for VF-Isolation. See NIX_SEND_EXT_S[LSO_SB]. */
        uint64_t vfi_lso_shp_dis       : 1;  /**< [576:576] Used for VF-Isolation. See NIX_SEND_EXT_S[SHP_DIS]. */
#else /* Word 9 - Little Endian */
        uint64_t vfi_lso_shp_dis       : 1;  /**< [576:576] Used for VF-Isolation. See NIX_SEND_EXT_S[SHP_DIS]. */
        uint64_t vfi_lso_sb            : 8;  /**< [584:577] Used for VF-Isolation. See NIX_SEND_EXT_S[LSO_SB]. */
        uint64_t vfi_lso_mps           : 14; /**< [598:585] Used for VF-Isolation. See NIX_SEND_EXT_S[LSO_MPS]. */
        uint64_t vfi_lso_vlan0_ins_ena : 1;  /**< [599:599] Used for VF-Isolation. See NIX_SEND_EXT_S[VLAN0_INS_ENA]. */
        uint64_t vfi_lso_vlan1_ins_ena : 1;  /**< [600:600] Used for VF-Isolation. See NIX_SEND_EXT_S[VLAN1_INS_ENA]. */
        uint64_t vfi_lso_vld           : 1;  /**< [601:601] Used for VF-Isolation. See NIX_SEND_EXT_S[LSO]. */
        uint64_t scm2_rsvd1            : 38; /**< [639:602] Reserved. */
#endif /* Word 9 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 10 - Big Endian */
        uint64_t head_sqb              : 64; /**< [703:640] See NIX_SQ_CTX_S[HEAD_SQB]. */
#else /* Word 10 - Little Endian */
        uint64_t head_sqb              : 64; /**< [703:640] See NIX_SQ_CTX_S[HEAD_SQB]. */
#endif /* Word 10 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 11 - Big Endian */
        uint64_t dse_rsvd1             : 28; /**< [767:740] Reserved. */
        uint64_t sso_ena               : 1;  /**< [739:739] See NIX_SQ_CTX[SSO_ENA]. */
        uint64_t sdp_mcast             : 1;  /**< [738:738] See NIX_SQ_CTX[SDP_MCAST]. */
        uint64_t default_chan          : 12; /**< [737:726] See NIX_SQ_CTX[DEFAULT_CHAN]. */
        uint64_t sqb_dequeue_count     : 16; /**< [725:710] See [SQB_ENQUEUE_COUNT]. */
        uint64_t head_offset           : 6;  /**< [709:704] See NIX_SQ_CTX_S[HEAD_OFFSET]. */
#else /* Word 11 - Little Endian */
        uint64_t head_offset           : 6;  /**< [709:704] See NIX_SQ_CTX_S[HEAD_OFFSET]. */
        uint64_t sqb_dequeue_count     : 16; /**< [725:710] See [SQB_ENQUEUE_COUNT]. */
        uint64_t default_chan          : 12; /**< [737:726] See NIX_SQ_CTX[DEFAULT_CHAN]. */
        uint64_t sdp_mcast             : 1;  /**< [738:738] See NIX_SQ_CTX[SDP_MCAST]. */
        uint64_t sso_ena               : 1;  /**< [739:739] See NIX_SQ_CTX[SSO_ENA]. */
        uint64_t dse_rsvd1             : 28; /**< [767:740] Reserved. */
#endif /* Word 11 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 12 - Big Endian */
        uint64_t seb_rsvd1             : 32; /**< [831:800] Reserved. */
        uint64_t lso_crc_iv            : 32; /**< [799:768] See NIX_SQ_CTX[LSO_CRC_IV]. */
#else /* Word 12 - Little Endian */
        uint64_t lso_crc_iv            : 32; /**< [799:768] See NIX_SQ_CTX[LSO_CRC_IV]. */
        uint64_t seb_rsvd1             : 32; /**< [831:800] Reserved. */
#endif /* Word 12 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 13 - Big Endian */
        uint64_t dropped_pkts          : 48; /**< [895:848] See NIX_SQ_CTX[DROPPED_PKTS]. */
        uint64_t dropped_octs_msw      : 16; /**< [847:832] See NIX_SQ_CTX[DROPPED_OCTS]. */
#else /* Word 13 - Little Endian */
        uint64_t dropped_octs_msw      : 16; /**< [847:832] See NIX_SQ_CTX[DROPPED_OCTS]. */
        uint64_t dropped_pkts          : 48; /**< [895:848] See NIX_SQ_CTX[DROPPED_PKTS]. */
#endif /* Word 13 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 14 - Big Endian */
        uint64_t dropped_octs_lsw      : 32; /**< [959:928] See NIX_SQ_CTX[DROPPED_OCTS]. */
        uint64_t octs_msw              : 32; /**< [927:896] See NIX_SQ_CTX[OCTS]. */
#else /* Word 14 - Little Endian */
        uint64_t octs_msw              : 32; /**< [927:896] See NIX_SQ_CTX[OCTS]. */
        uint64_t dropped_octs_lsw      : 32; /**< [959:928] See NIX_SQ_CTX[DROPPED_OCTS]. */
#endif /* Word 14 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 15 - Big Endian */
        uint64_t octs_lsw              : 16; /**< [1023:1008] See NIX_SQ_CTX[OCTS]. */
        uint64_t pkts                  : 48; /**< [1007:960] See NIX_SQ_CTX[PKTS]. */
#else /* Word 15 - Little Endian */
        uint64_t pkts                  : 48; /**< [1007:960] See NIX_SQ_CTX[PKTS]. */
        uint64_t octs_lsw              : 16; /**< [1023:1008] See NIX_SQ_CTX[OCTS]. */
#endif /* Word 15 - End */
    } s;
    /* struct bdk_nix_sq_ctx_hw_s_s cn; */
};

/**
 * Structure nix_sq_ctx_s
 *
 * NIX Send Queue Context Structure
 * This structure specifies the format used by software with the NIX admin queue
 * to read and write a send queue's NIX_SQ_CTX_HW_S structure maintained by
 * hardware in LLC/DRAM.
 */
union bdk_nix_sq_ctx_s
{
    uint64_t u[16];
    struct bdk_nix_sq_ctx_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t cq_limit              : 8;  /**< [ 63: 56] Threshold level for suppressing packet send, in units of 1/256th of CQ level.
                                                                 0xff represents an empty CQ ring, 0x0 represents a full ring. Packets will not
                                                                 be sent from the SQ if the available space in the associated CQ
                                                                 (NIX_CQ_CTX_S[AVG_LEVEL]) is less than the [CQ_LIMIT] value. */
        uint64_t cq                    : 20; /**< [ 55: 36] Completion queue for this SQ. Valid when [CQ_ENA] is set. */
        uint64_t state                 : 8;  /**< [ 35: 28] Send queue state (TBD). */
        uint64_t reserved_27           : 1;
        uint64_t mnq_dis               : 1;  /**< [ 26: 26] Meta-descriptor enqueue disable. Hardware sets this bit along with
                                                                 [SQ_INT\<NIX_SQINT_E::MNQ_ERR\>] when an error is detected while enqueuing a
                                                                 meta-descriptor to [SMQ] from this SQ. When set, hardware stops enqueuing to
                                                                 [SMQ] from this SQ. */
        uint64_t lmt_dis               : 1;  /**< [ 25: 25] LMT store disable. Hardware sets this bit along with
                                                                 [SQ_INT\<NIX_SQINT_E::LMT_ERR\>] when an LMT store to NIX_LF_OP_SEND()
                                                                 for this SQ has an error. See also NIX_LF_LMT_ERR_DBG. When set,
                                                                 hardware drops LMT stores targeting this SQ. */
        uint64_t sdp_mcast             : 1;  /**< [ 24: 24] SDP multicast. Valid if the SQ sends packets to SDP (corresponding
                                                                 NIX_AF_TL4()_SDP_LINK_CFG[ENA] is set):
                                                                 0 = SQ sends SDP unicast packets.
                                                                 1 = SQ sends SDP multicast packets. */
        uint64_t substream             : 20; /**< [ 23:  4] Substream ID of:
                                                                 _ IOVAs specified by NIX_SEND_SG_S/NIX_IOVA_S, NIX_SEND_JUMP_S and
                                                                 NIX_SEND_MEM_S.
                                                                 _ SQBs allocated from [SQB_AURA]. */
        uint64_t max_sqe_size          : 2;  /**< [  3:  2] Selects maximum SQE size for this SQ. Enumerated by NIX_MAXSQESZ_E.
                                                                 Internal:
                                                                 Hardware allocates this size for each SQE stored in an SQB. */
        uint64_t cq_ena                : 1;  /**< [  1:  1] Completion queue enable.
                                                                 0 = NIX_SEND_HDR_S[PNC] is ignored and a packet from this SQ will never generate
                                                                 a CQE.
                                                                 1 = A packet with NIX_SEND_HDR_S[PNC] will add a send completion CQE to [CQ]. */
        uint64_t ena                   : 1;  /**< [  0:  0] SQ enable. */
#else /* Word 0 - Little Endian */
        uint64_t ena                   : 1;  /**< [  0:  0] SQ enable. */
        uint64_t cq_ena                : 1;  /**< [  1:  1] Completion queue enable.
                                                                 0 = NIX_SEND_HDR_S[PNC] is ignored and a packet from this SQ will never generate
                                                                 a CQE.
                                                                 1 = A packet with NIX_SEND_HDR_S[PNC] will add a send completion CQE to [CQ]. */
        uint64_t max_sqe_size          : 2;  /**< [  3:  2] Selects maximum SQE size for this SQ. Enumerated by NIX_MAXSQESZ_E.
                                                                 Internal:
                                                                 Hardware allocates this size for each SQE stored in an SQB. */
        uint64_t substream             : 20; /**< [ 23:  4] Substream ID of:
                                                                 _ IOVAs specified by NIX_SEND_SG_S/NIX_IOVA_S, NIX_SEND_JUMP_S and
                                                                 NIX_SEND_MEM_S.
                                                                 _ SQBs allocated from [SQB_AURA]. */
        uint64_t sdp_mcast             : 1;  /**< [ 24: 24] SDP multicast. Valid if the SQ sends packets to SDP (corresponding
                                                                 NIX_AF_TL4()_SDP_LINK_CFG[ENA] is set):
                                                                 0 = SQ sends SDP unicast packets.
                                                                 1 = SQ sends SDP multicast packets. */
        uint64_t lmt_dis               : 1;  /**< [ 25: 25] LMT store disable. Hardware sets this bit along with
                                                                 [SQ_INT\<NIX_SQINT_E::LMT_ERR\>] when an LMT store to NIX_LF_OP_SEND()
                                                                 for this SQ has an error. See also NIX_LF_LMT_ERR_DBG. When set,
                                                                 hardware drops LMT stores targeting this SQ. */
        uint64_t mnq_dis               : 1;  /**< [ 26: 26] Meta-descriptor enqueue disable. Hardware sets this bit along with
                                                                 [SQ_INT\<NIX_SQINT_E::MNQ_ERR\>] when an error is detected while enqueuing a
                                                                 meta-descriptor to [SMQ] from this SQ. When set, hardware stops enqueuing to
                                                                 [SMQ] from this SQ. */
        uint64_t reserved_27           : 1;
        uint64_t state                 : 8;  /**< [ 35: 28] Send queue state (TBD). */
        uint64_t cq                    : 20; /**< [ 55: 36] Completion queue for this SQ. Valid when [CQ_ENA] is set. */
        uint64_t cq_limit              : 8;  /**< [ 63: 56] Threshold level for suppressing packet send, in units of 1/256th of CQ level.
                                                                 0xff represents an empty CQ ring, 0x0 represents a full ring. Packets will not
                                                                 be sent from the SQ if the available space in the associated CQ
                                                                 (NIX_CQ_CTX_S[AVG_LEVEL]) is less than the [CQ_LIMIT] value. */
#endif /* Word 0 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 1 - Big Endian */
        uint64_t sqb_count             : 16; /**< [127:112] Number of SQBs currently in use. Includes the SQBs at [HEAD_SQB] and
                                                                 [TAIL_SQB], and any linked SQBs in between. Excludes the SQB at [NEXT_SQB]. */
        uint64_t default_chan          : 12; /**< [111:100] Default channel enumerated by NIX_CHAN_E.

                                                                 If the SQ transmits to CGX and/or LBK (corresponding
                                                                 NIX_AF_TL4()_SDP_LINK_CFG[ENA] is clear), this is the channel to which a
                                                                 packet is transmitted when NIX_TX_ACTION_S[OP] =
                                                                 NIX_TX_ACTIONOP_E::UCAST_DEFAULT in the NPC result.

                                                                 If the SQ transmits to SDP (corresponding NIX_AF_TL4()_SDP_LINK_CFG[ENA] is
                                                                 set), this is the SDP channel to which packets are transmitted when
                                                                 [SDP_MCAST] is clear, and the SDP multicast index when [SDP_MCAST] is set. */
        uint64_t smq_rr_quantum        : 24; /**< [ 99: 76] Round-robin (DWRR) quantum for packets pushed from this SQ to the associated SMQ (24-bit
                                                                 unsigned integer). Specifies the amount of packet data to push to SMQ in a round as a
                                                                 multiple of four bytes.

                                                                 The minimum value is MTU/4 (rounded up); this is also the typical value for equal-weight
                                                                 arbitration. */
        uint64_t sso_ena               : 1;  /**< [ 75: 75] SSO add work enable.
                                                                 0 = The SQ never adds work to SSO, and NIX_SEND_WORK_S is ignored when present
                                                                 in a send descriptor.
                                                                 1 = A packets with NIX_SEND_WORK_S will add work to SSO. */
        uint64_t xoff                  : 1;  /**< [ 74: 74] Transmit off. When set, the SQ will not push meta descriptors to the
                                                                 associated SMQ. Software can read, set and clear this bit with
                                                                 NIX_LF_SQ_OP_INT[XOFF]. */
        uint64_t smq                   : 10; /**< [ 73: 64] Send meta-descriptor queue for this SQ. Must be less than 512. */
#else /* Word 1 - Little Endian */
        uint64_t smq                   : 10; /**< [ 73: 64] Send meta-descriptor queue for this SQ. Must be less than 512. */
        uint64_t xoff                  : 1;  /**< [ 74: 74] Transmit off. When set, the SQ will not push meta descriptors to the
                                                                 associated SMQ. Software can read, set and clear this bit with
                                                                 NIX_LF_SQ_OP_INT[XOFF]. */
        uint64_t sso_ena               : 1;  /**< [ 75: 75] SSO add work enable.
                                                                 0 = The SQ never adds work to SSO, and NIX_SEND_WORK_S is ignored when present
                                                                 in a send descriptor.
                                                                 1 = A packets with NIX_SEND_WORK_S will add work to SSO. */
        uint64_t smq_rr_quantum        : 24; /**< [ 99: 76] Round-robin (DWRR) quantum for packets pushed from this SQ to the associated SMQ (24-bit
                                                                 unsigned integer). Specifies the amount of packet data to push to SMQ in a round as a
                                                                 multiple of four bytes.

                                                                 The minimum value is MTU/4 (rounded up); this is also the typical value for equal-weight
                                                                 arbitration. */
        uint64_t default_chan          : 12; /**< [111:100] Default channel enumerated by NIX_CHAN_E.

                                                                 If the SQ transmits to CGX and/or LBK (corresponding
                                                                 NIX_AF_TL4()_SDP_LINK_CFG[ENA] is clear), this is the channel to which a
                                                                 packet is transmitted when NIX_TX_ACTION_S[OP] =
                                                                 NIX_TX_ACTIONOP_E::UCAST_DEFAULT in the NPC result.

                                                                 If the SQ transmits to SDP (corresponding NIX_AF_TL4()_SDP_LINK_CFG[ENA] is
                                                                 set), this is the SDP channel to which packets are transmitted when
                                                                 [SDP_MCAST] is clear, and the SDP multicast index when [SDP_MCAST] is set. */
        uint64_t sqb_count             : 16; /**< [127:112] Number of SQBs currently in use. Includes the SQBs at [HEAD_SQB] and
                                                                 [TAIL_SQB], and any linked SQBs in between. Excludes the SQB at [NEXT_SQB]. */
#endif /* Word 1 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 2 - Big Endian */
        uint64_t reserved_191          : 1;
        uint64_t smq_pend              : 1;  /**< [190:190] When set, indicates that this SQ has pending SQEs to be parsed and pushed to the associated SMQ. */
        uint64_t sqe_stype             : 2;  /**< [189:188] Selects the style of write and read for accessing SQBs in LLC/DRAM.
                                                                 Enumerated by NIX_STYPE_E.
                                                                 Must be NIX_STYPE_E::STP when NIX_SQ_CTX_S[MAX_SQE_SIZE] =
                                                                 NIX_MAXSQESZ_E::W8. */
        uint64_t sq_int_ena            : 8;  /**< [187:180] SQ interrupt enables. Bits enumerated by NIX_SQINT_E. Software can read,
                                                                 set or clear these bits with NIX_LF_SQ_OP_INT. */
        uint64_t sq_int                : 8;  /**< [179:172] SQ interrupts. Bits enumerated by NIX_SQINT_E, which also defines when
                                                                 hardware sets each bit. Software can read, set or clear these bits with
                                                                 NIX_LF_SQ_OP_INT. */
        uint64_t reserved_144_171      : 28;
        uint64_t sqe_way_mask          : 16; /**< [143:128] Way partitioning mask for allocating SQB lines in NDC (1 means do not
                                                                 use). All ones disables allocation in NDC. */
#else /* Word 2 - Little Endian */
        uint64_t sqe_way_mask          : 16; /**< [143:128] Way partitioning mask for allocating SQB lines in NDC (1 means do not
                                                                 use). All ones disables allocation in NDC. */
        uint64_t reserved_144_171      : 28;
        uint64_t sq_int                : 8;  /**< [179:172] SQ interrupts. Bits enumerated by NIX_SQINT_E, which also defines when
                                                                 hardware sets each bit. Software can read, set or clear these bits with
                                                                 NIX_LF_SQ_OP_INT. */
        uint64_t sq_int_ena            : 8;  /**< [187:180] SQ interrupt enables. Bits enumerated by NIX_SQINT_E. Software can read,
                                                                 set or clear these bits with NIX_LF_SQ_OP_INT. */
        uint64_t sqe_stype             : 2;  /**< [189:188] Selects the style of write and read for accessing SQBs in LLC/DRAM.
                                                                 Enumerated by NIX_STYPE_E.
                                                                 Must be NIX_STYPE_E::STP when NIX_SQ_CTX_S[MAX_SQE_SIZE] =
                                                                 NIX_MAXSQESZ_E::W8. */
        uint64_t smq_pend              : 1;  /**< [190:190] When set, indicates that this SQ has pending SQEs to be parsed and pushed to the associated SMQ. */
        uint64_t reserved_191          : 1;
#endif /* Word 2 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 3 - Big Endian */
        uint64_t reserved_247_255      : 9;
        uint64_t qint_idx              : 7;  /**< [246:240] Queue interrupt index. Select the QINT within LF (index {a} of
                                                                 NIX_LF_QINT()*) which receives [SQ_INT] events. */
        uint64_t smq_lso_segnum        : 8;  /**< [239:232] Next LSO segment number to enqueue to PSE. */
        uint64_t send_lso_segnum       : 8;  /**< [231:224] Next LSO segment number to send. */
        uint64_t reserved_192_223      : 32;
#else /* Word 3 - Little Endian */
        uint64_t reserved_192_223      : 32;
        uint64_t send_lso_segnum       : 8;  /**< [231:224] Next LSO segment number to send. */
        uint64_t smq_lso_segnum        : 8;  /**< [239:232] Next LSO segment number to enqueue to PSE. */
        uint64_t qint_idx              : 7;  /**< [246:240] Queue interrupt index. Select the QINT within LF (index {a} of
                                                                 NIX_LF_QINT()*) which receives [SQ_INT] events. */
        uint64_t reserved_247_255      : 9;
#endif /* Word 3 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 4 - Big Endian */
        uint64_t next_sqb              : 64; /**< [319:256] IOVA of next SQB. Valid when [NEXT_SQB_VALID] is set. A NULL value when
                                                                 valid indicates allocation of next SQB from [SQB_AURA] failed. */
#else /* Word 4 - Little Endian */
        uint64_t next_sqb              : 64; /**< [319:256] IOVA of next SQB. Valid when [NEXT_SQB_VALID] is set. A NULL value when
                                                                 valid indicates allocation of next SQB from [SQB_AURA] failed. */
#endif /* Word 4 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 5 - Big Endian */
        uint64_t tail_sqb              : 64; /**< [383:320] IOVA of tail SQB. Valid when [SQB_COUNT] is nonzero. */
#else /* Word 5 - Little Endian */
        uint64_t tail_sqb              : 64; /**< [383:320] IOVA of tail SQB. Valid when [SQB_COUNT] is nonzero. */
#endif /* Word 5 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 6 - Big Endian */
        uint64_t smenq_sqb             : 64; /**< [447:384] IOVA of SMQ enqueue SQB. Valid when [SQB_COUNT] is nonzero. */
#else /* Word 6 - Little Endian */
        uint64_t smenq_sqb             : 64; /**< [447:384] IOVA of SMQ enqueue SQB. Valid when [SQB_COUNT] is nonzero. */
#endif /* Word 6 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 7 - Big Endian */
        uint64_t head_sqb              : 64; /**< [511:448] IOVA of head SQB. Valid when [SQB_COUNT] is nonzero. */
#else /* Word 7 - Little Endian */
        uint64_t head_sqb              : 64; /**< [511:448] IOVA of head SQB. Valid when [SQB_COUNT] is nonzero. */
#endif /* Word 7 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 8 - Big Endian */
        uint64_t reserved_560_575      : 16;
        uint64_t sqb_aura              : 20; /**< [559:540] SQB aura number. NPA aura to use for SQE buffer allocations and frees for
                                                                 this SQ. The selected aura must correspond to a pool where the buffers
                                                                 (after any NPA_POOL_S[BUF_OFFSET]) are at least of size
                                                                 NIX_AF_SQ_CONST[SQB_SIZE] (4KB). */
        uint64_t reserved_531_539      : 9;
        uint64_t next_sqb_valid        : 1;  /**< [530:530] Set when [NEXT_SQB] is valid. */
        uint64_t head_offset           : 6;  /**< [529:524] Offset of head SQE in [HEAD_SQB]. */
        uint64_t smenq_offset          : 6;  /**< [523:518] Offset of next SQE to bve pushed to SMQ in [SMENQ_SQB]. */
        uint64_t tail_offset           : 6;  /**< [517:512] Offset of next SQE to be enqueued in [TAIL_SQB]. */
#else /* Word 8 - Little Endian */
        uint64_t tail_offset           : 6;  /**< [517:512] Offset of next SQE to be enqueued in [TAIL_SQB]. */
        uint64_t smenq_offset          : 6;  /**< [523:518] Offset of next SQE to bve pushed to SMQ in [SMENQ_SQB]. */
        uint64_t head_offset           : 6;  /**< [529:524] Offset of head SQE in [HEAD_SQB]. */
        uint64_t next_sqb_valid        : 1;  /**< [530:530] Set when [NEXT_SQB] is valid. */
        uint64_t reserved_531_539      : 9;
        uint64_t sqb_aura              : 20; /**< [559:540] SQB aura number. NPA aura to use for SQE buffer allocations and frees for
                                                                 this SQ. The selected aura must correspond to a pool where the buffers
                                                                 (after any NPA_POOL_S[BUF_OFFSET]) are at least of size
                                                                 NIX_AF_SQ_CONST[SQB_SIZE] (4KB). */
        uint64_t reserved_560_575      : 16;
#endif /* Word 8 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 9 - Big Endian */
        uint64_t reserved_622_639      : 18;
        uint64_t smq_next_sq           : 20; /**< [621:602] Next SQ within the LF to process in SMQ parse link list. Valid when
                                                                 [SMQ_PEND] is set and the SQ is not at the tail of the SMQ's link list. */
        uint64_t reserved_601          : 1;
        uint64_t smq_rr_count          : 25; /**< [600:576] Round-robin (DWRR) deficit counter for packets pushed from this SQ to the associated SMQ.
                                                                 A 25-bit signed integer count. */
#else /* Word 9 - Little Endian */
        uint64_t smq_rr_count          : 25; /**< [600:576] Round-robin (DWRR) deficit counter for packets pushed from this SQ to the associated SMQ.
                                                                 A 25-bit signed integer count. */
        uint64_t reserved_601          : 1;
        uint64_t smq_next_sq           : 20; /**< [621:602] Next SQ within the LF to process in SMQ parse link list. Valid when
                                                                 [SMQ_PEND] is set and the SQ is not at the tail of the SMQ's link list. */
        uint64_t reserved_622_639      : 18;
#endif /* Word 9 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 10 - Big Endian */
        uint64_t reserved_688_703      : 16;
        uint64_t octs                  : 48; /**< [687:640] Number of octets transmitted. Includes frame minimum size pad bytes due to
                                                                 NIX_AF_SMQ()_CFG[MINLEN], and excludes FCS bytes. Also includes any VLAN
                                                                 bytes inserted by NIX_SEND_EXT_S[VLAN*] and/or Vtag bytes inserted by
                                                                 NIX_TX_VTAG_ACTION_S. */
#else /* Word 10 - Little Endian */
        uint64_t octs                  : 48; /**< [687:640] Number of octets transmitted. Includes frame minimum size pad bytes due to
                                                                 NIX_AF_SMQ()_CFG[MINLEN], and excludes FCS bytes. Also includes any VLAN
                                                                 bytes inserted by NIX_SEND_EXT_S[VLAN*] and/or Vtag bytes inserted by
                                                                 NIX_TX_VTAG_ACTION_S. */
        uint64_t reserved_688_703      : 16;
#endif /* Word 10 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 11 - Big Endian */
        uint64_t reserved_752_767      : 16;
        uint64_t pkts                  : 48; /**< [751:704] Number of packets successfully transmitted. */
#else /* Word 11 - Little Endian */
        uint64_t pkts                  : 48; /**< [751:704] Number of packets successfully transmitted. */
        uint64_t reserved_752_767      : 16;
#endif /* Word 11 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 12 - Big Endian */
        uint64_t reserved_800_831      : 32;
        uint64_t lso_crc_iv            : 32; /**< [799:768] NIX_SEND_CRC_S intermediate value between LSO segments.
                                                                 Internal:
                                                                 This word is for SEB use and should not be written by SQM. */
#else /* Word 12 - Little Endian */
        uint64_t lso_crc_iv            : 32; /**< [799:768] NIX_SEND_CRC_S intermediate value between LSO segments.
                                                                 Internal:
                                                                 This word is for SEB use and should not be written by SQM. */
        uint64_t reserved_800_831      : 32;
#endif /* Word 12 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 13 - Big Endian */
        uint64_t reserved_832_895      : 64;
#else /* Word 13 - Little Endian */
        uint64_t reserved_832_895      : 64;
#endif /* Word 13 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 14 - Big Endian */
        uint64_t reserved_944_959      : 16;
        uint64_t dropped_octs          : 48; /**< [943:896] Number of dropped octets. */
#else /* Word 14 - Little Endian */
        uint64_t dropped_octs          : 48; /**< [943:896] Number of dropped octets. */
        uint64_t reserved_944_959      : 16;
#endif /* Word 14 - End */
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 15 - Big Endian */
        uint64_t reserved_1008_1023    : 16;
        uint64_t dropped_pkts          : 48; /**< [1007:960] Number of dropped packets. */
#else /* Word 15 - Little Endian */
        uint64_t dropped_pkts          : 48; /**< [1007:960] Number of dropped packets. */
        uint64_t reserved_1008_1023    : 16;
#endif /* Word 15 - End */
    } s;
    /* struct bdk_nix_sq_ctx_s_s cn; */
};

/**
 * Structure nix_tx_action_s
 *
 * NIX Transmit Action Structure
 * This structure defines the format of NPC_RESULT_S[ACTION] for a transmit packet.
 */
union bdk_nix_tx_action_s
{
    uint64_t u;
    struct bdk_nix_tx_action_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t match_id              : 16; /**< [ 47: 32] Software defined match identifier. */
        uint64_t index                 : 20; /**< [ 31: 12] Transmit channel or table index in NIX. The index type is selected as
                                                                 follows:
                                                                 _ if [OP] = NIX_TX_ACTIONOP_E::UCAST_CHAN, transmit channel.
                                                                 _ if [OP] = NIX_TX_ACTIONOP_E::MCAST, pointer to start of multicast
                                                                 replication list in the NIX TX multicast table.
                                                                 _ otherwise, not used. */
        uint64_t reserved_4_11         : 8;
        uint64_t op                    : 4;  /**< [  3:  0] Action op code enumerated by NIX_TX_ACTIONOP_E. */
#else /* Word 0 - Little Endian */
        uint64_t op                    : 4;  /**< [  3:  0] Action op code enumerated by NIX_TX_ACTIONOP_E. */
        uint64_t reserved_4_11         : 8;
        uint64_t index                 : 20; /**< [ 31: 12] Transmit channel or table index in NIX. The index type is selected as
                                                                 follows:
                                                                 _ if [OP] = NIX_TX_ACTIONOP_E::UCAST_CHAN, transmit channel.
                                                                 _ if [OP] = NIX_TX_ACTIONOP_E::MCAST, pointer to start of multicast
                                                                 replication list in the NIX TX multicast table.
                                                                 _ otherwise, not used. */
        uint64_t match_id              : 16; /**< [ 47: 32] Software defined match identifier. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_tx_action_s_s cn; */
};

/**
 * Structure nix_tx_vtag_action_s
 *
 * NIX Transmit Vtag Action Structure
 * This structure defines the format of NPC_RESULT_S[VTAG_ACTION] for a transmit
 * packet. It specifies optional insert/replace of up to two Vtags (e.g.
 * C-VLAN/S-VLAN tags, 802.1BR E-TAG). If two Vtags are inserted, the Vtag 0 byte
 * offset from packet start (see NIX_RX_VTAG_ACTION_S[VTAG0_RELPTR]) must be less
 * than or equal to the Vtag 1 byte offset. Vtag 1 is always inserted aftrer Vtag
 * 0 in the packet's header, even if their byte offsets are equal.
 *
 * A Vtag must not be inserted within an outer or inner L3/L4 header, but may be
 * inserted within an outer L4 payload.
 */
union bdk_nix_tx_vtag_action_s
{
    uint64_t u;
    struct bdk_nix_tx_vtag_action_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_58_63        : 6;
        uint64_t vtag1_def             : 10; /**< [ 57: 48] Vtag 1 definition. Index to NIX_AF_TX_VTAG_DEF()_CTL/DATA entry that
                                                                 defines the tag size and data to insert or replace. */
        uint64_t reserved_46_47        : 2;
        uint64_t vtag1_op              : 2;  /**< [ 45: 44] Vtag 1 operation enumerated by NIX_TX_VTAGOP_E. */
        uint64_t reserved_43           : 1;
        uint64_t vtag1_lid             : 3;  /**< [ 42: 40] Vtag 1 layer ID enumerated by NPC_LID_E. */
        uint64_t vtag1_relptr          : 8;  /**< [ 39: 32] Vtag 1 relative pointer. See NIX_RX_VTAG_ACTION_S[VTAG1_RELPTR]. Must be even. */
        uint64_t reserved_26_31        : 6;
        uint64_t vtag0_def             : 10; /**< [ 25: 16] Vtag 0 definition. Index to NIX_AF_TX_VTAG_DEF()_CTL/DATA entry that
                                                                 defines the tag size and data to insert or replace. */
        uint64_t reserved_14_15        : 2;
        uint64_t vtag0_op              : 2;  /**< [ 13: 12] Vtag 0 operation enumerated by NIX_TX_VTAGOP_E. */
        uint64_t reserved_11           : 1;
        uint64_t vtag0_lid             : 3;  /**< [ 10:  8] Vtag 0 layer ID enumerated by NPC_LID_E. */
        uint64_t vtag0_relptr          : 8;  /**< [  7:  0] Vtag 0 relative pointer. See NIX_RX_VTAG_ACTION_S[VTAG0_RELPTR]. Must be even. */
#else /* Word 0 - Little Endian */
        uint64_t vtag0_relptr          : 8;  /**< [  7:  0] Vtag 0 relative pointer. See NIX_RX_VTAG_ACTION_S[VTAG0_RELPTR]. Must be even. */
        uint64_t vtag0_lid             : 3;  /**< [ 10:  8] Vtag 0 layer ID enumerated by NPC_LID_E. */
        uint64_t reserved_11           : 1;
        uint64_t vtag0_op              : 2;  /**< [ 13: 12] Vtag 0 operation enumerated by NIX_TX_VTAGOP_E. */
        uint64_t reserved_14_15        : 2;
        uint64_t vtag0_def             : 10; /**< [ 25: 16] Vtag 0 definition. Index to NIX_AF_TX_VTAG_DEF()_CTL/DATA entry that
                                                                 defines the tag size and data to insert or replace. */
        uint64_t reserved_26_31        : 6;
        uint64_t vtag1_relptr          : 8;  /**< [ 39: 32] Vtag 1 relative pointer. See NIX_RX_VTAG_ACTION_S[VTAG1_RELPTR]. Must be even. */
        uint64_t vtag1_lid             : 3;  /**< [ 42: 40] Vtag 1 layer ID enumerated by NPC_LID_E. */
        uint64_t reserved_43           : 1;
        uint64_t vtag1_op              : 2;  /**< [ 45: 44] Vtag 1 operation enumerated by NIX_TX_VTAGOP_E. */
        uint64_t reserved_46_47        : 2;
        uint64_t vtag1_def             : 10; /**< [ 57: 48] Vtag 1 definition. Index to NIX_AF_TX_VTAG_DEF()_CTL/DATA entry that
                                                                 defines the tag size and data to insert or replace. */
        uint64_t reserved_58_63        : 6;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_tx_vtag_action_s_s cn; */
};

/**
 * Structure nix_wqe_hdr_s
 *
 * NIX Work Queue Entry Header Structure
 * This 64-bit structure defines the first word of every receive WQE generated by
 * NIX. It is immediately followed by NIX_RX_PARSE_S.
 * Stored in memory as little-endian unless NIX_AF_LF()_CFG[BE] is set.
 */
union bdk_nix_wqe_hdr_s
{
    uint64_t u;
    struct bdk_nix_wqe_hdr_s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t wqe_type              : 4;  /**< [ 63: 60] WQE type enumerated by NIX_XQE_TYPE_E. */
        uint64_t q                     : 14; /**< [ 59: 46] Lower 14 bits of RQ or SQ within VF/PF. */
        uint64_t node                  : 2;  /**< [ 45: 44] Node number on which the packet was received or transmitted.
                                                                 Internal:
                                                                 This is needed by software; do not remove on single-node parts. */
        uint64_t grp                   : 10; /**< [ 43: 34] The SSO guest-group number used for the packet's ADD_WORK from
                                                                 NIX_RQ_CTX_S[SSO_GRP]. [GRP]\<9:8\> is always zero. */
        uint64_t tt                    : 2;  /**< [ 33: 32] The initial tag type for the packet's SSO ADD_WORK from
                                                                 NIX_RQ_CTX_S[SSO_TT]. Enumerated by SSO_TT_E. */
        uint64_t tag                   : 32; /**< [ 31:  0] The initial tag for the work-queue entry.
                                                                 See pseudocode in NIX_RQ_CTX_S[LTAG]. */
#else /* Word 0 - Little Endian */
        uint64_t tag                   : 32; /**< [ 31:  0] The initial tag for the work-queue entry.
                                                                 See pseudocode in NIX_RQ_CTX_S[LTAG]. */
        uint64_t tt                    : 2;  /**< [ 33: 32] The initial tag type for the packet's SSO ADD_WORK from
                                                                 NIX_RQ_CTX_S[SSO_TT]. Enumerated by SSO_TT_E. */
        uint64_t grp                   : 10; /**< [ 43: 34] The SSO guest-group number used for the packet's ADD_WORK from
                                                                 NIX_RQ_CTX_S[SSO_GRP]. [GRP]\<9:8\> is always zero. */
        uint64_t node                  : 2;  /**< [ 45: 44] Node number on which the packet was received or transmitted.
                                                                 Internal:
                                                                 This is needed by software; do not remove on single-node parts. */
        uint64_t q                     : 14; /**< [ 59: 46] Lower 14 bits of RQ or SQ within VF/PF. */
        uint64_t wqe_type              : 4;  /**< [ 63: 60] WQE type enumerated by NIX_XQE_TYPE_E. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nix_wqe_hdr_s_s cn; */
};

/**
 * Register (RVU_PF_BAR0) nix#_af_active_cycles_pc
 *
 * NIX AF Active Cycles Register
 */
union bdk_nixx_af_active_cycles_pc
{
    uint64_t u;
    struct bdk_nixx_af_active_cycles_pc_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t act_cyc               : 64; /**< [ 63:  0](R/W/H) Counts every coprocessor-clock cycle that the conditional clocks are active. */
#else /* Word 0 - Little Endian */
        uint64_t act_cyc               : 64; /**< [ 63:  0](R/W/H) Counts every coprocessor-clock cycle that the conditional clocks are active. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_active_cycles_pc_s cn; */
};
typedef union bdk_nixx_af_active_cycles_pc bdk_nixx_af_active_cycles_pc_t;

static inline uint64_t BDK_NIXX_AF_ACTIVE_CYCLES_PC(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_ACTIVE_CYCLES_PC(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400000a0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_ACTIVE_CYCLES_PC", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_ACTIVE_CYCLES_PC(a) bdk_nixx_af_active_cycles_pc_t
#define bustype_BDK_NIXX_AF_ACTIVE_CYCLES_PC(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_ACTIVE_CYCLES_PC(a) "NIXX_AF_ACTIVE_CYCLES_PC"
#define device_bar_BDK_NIXX_AF_ACTIVE_CYCLES_PC(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_ACTIVE_CYCLES_PC(a) (a)
#define arguments_BDK_NIXX_AF_ACTIVE_CYCLES_PC(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_base
 *
 * NIX AF Admin Queue Base Address Register
 */
union bdk_nixx_af_aq_base
{
    uint64_t u;
    struct bdk_nixx_af_aq_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t base_addr             : 46; /**< [ 52:  7](R/W) Base RVU PF(0) IOVA\<52:7\> of AQ ring in LLC/DRAM. IOVA bits \<6:0\> are always zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t base_addr             : 46; /**< [ 52:  7](R/W) Base RVU PF(0) IOVA\<52:7\> of AQ ring in LLC/DRAM. IOVA bits \<6:0\> are always zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_aq_base_s cn; */
};
typedef union bdk_nixx_af_aq_base bdk_nixx_af_aq_base_t;

static inline uint64_t BDK_NIXX_AF_AQ_BASE(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_BASE(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000410ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_BASE", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_BASE(a) bdk_nixx_af_aq_base_t
#define bustype_BDK_NIXX_AF_AQ_BASE(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_BASE(a) "NIXX_AF_AQ_BASE"
#define device_bar_BDK_NIXX_AF_AQ_BASE(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_BASE(a) (a)
#define arguments_BDK_NIXX_AF_AQ_BASE(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_cfg
 *
 * NIX AF Admin Queue Configuration Register
 */
union bdk_nixx_af_aq_cfg
{
    uint64_t u;
    struct bdk_nixx_af_aq_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_4_63         : 60;
        uint64_t qsize                 : 4;  /**< [  3:  0](R/W) Specifies AQ ring size in entries of 16 bytes:
                                                                 0x0 = 16 entries.
                                                                 0x1 = 64 entries.
                                                                 0x2 = 256 entries.
                                                                 0x3 = 1K entries.
                                                                 0x4 = 4K entries.
                                                                 0x5 = 16K entries.
                                                                 0x6 = 64K entries.
                                                                 0x7 = 256K entries.
                                                                 0x8 = 1M entries.
                                                                 0x9-0xF = Reserved.

                                                                 Note that the usable size of the ring is the specified size minus 1 (HEAD==TAIL always
                                                                 means empty). */
#else /* Word 0 - Little Endian */
        uint64_t qsize                 : 4;  /**< [  3:  0](R/W) Specifies AQ ring size in entries of 16 bytes:
                                                                 0x0 = 16 entries.
                                                                 0x1 = 64 entries.
                                                                 0x2 = 256 entries.
                                                                 0x3 = 1K entries.
                                                                 0x4 = 4K entries.
                                                                 0x5 = 16K entries.
                                                                 0x6 = 64K entries.
                                                                 0x7 = 256K entries.
                                                                 0x8 = 1M entries.
                                                                 0x9-0xF = Reserved.

                                                                 Note that the usable size of the ring is the specified size minus 1 (HEAD==TAIL always
                                                                 means empty). */
        uint64_t reserved_4_63         : 60;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_aq_cfg_s cn; */
};
typedef union bdk_nixx_af_aq_cfg bdk_nixx_af_aq_cfg_t;

static inline uint64_t BDK_NIXX_AF_AQ_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000400ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_CFG(a) bdk_nixx_af_aq_cfg_t
#define bustype_BDK_NIXX_AF_AQ_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_CFG(a) "NIXX_AF_AQ_CFG"
#define device_bar_BDK_NIXX_AF_AQ_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_CFG(a) (a)
#define arguments_BDK_NIXX_AF_AQ_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_done
 *
 * NIX AF Admin Queue Done Count Register
 */
union bdk_nixx_af_aq_done
{
    uint64_t u;
    struct bdk_nixx_af_aq_done_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_20_63        : 44;
        uint64_t done                  : 20; /**< [ 19:  0](R/W/H) Done count. When NIX_AQ_INST_S[DONEINT] set and that instruction completes,
                                                                 NIX_AF_AQ_DONE[DONE] is incremented. Write to this field are for diagnostic use only;
                                                                 instead software writes NIX_AF_AQ_DONE_ACK with the number of decrements for this field.

                                                                 Interrupts are sent as follows:

                                                                 * When NIX_AF_AQ_DONE[DONE] = 0, then no results are pending, the interrupt
                                                                 coalescing timer is held to zero, and an interrupt is not sent.

                                                                 * When NIX_AF_AQ_DONE[DONE] != 0, then the interrupt coalescing timer
                                                                 counts every microsecond. If the counter is \>= NIX_AF_AQ_DONE_WAIT[TIME_WAIT],
                                                                 or NIX_AF_AQ_DONE[DONE] \>= NIX_AF_AQ_DONE_WAIT[NUM_WAIT], i.e. enough time
                                                                 has passed or enough results have arrived, then the interrupt is sent.
                                                                 Otherwise, it is not sent due to coalescing.

                                                                 * When NIX_AF_AQ_DONE_ACK is written (or NIX_AF_AQ_DONE is written but this is
                                                                 not typical), the interrupt coalescing timer restarts. Note after decrementing
                                                                 this interrupt equation is recomputed, for example if NIX_AF_AQ_DONE[DONE] \>=
                                                                 NIX_AF_AQ_DONE_WAIT[NUM_WAIT] and because the timer is zero, the interrupt will
                                                                 be resent immediately. (This covers the race case between software
                                                                 acknowledging an interrupt and a result returning.).

                                                                 * When NIX_AF_AQ_DONE_ENA_W1S[DONE] = 0, interrupts are not sent, but the
                                                                 counting described above still occurs.

                                                                 AQ instructions complete in order.

                                                                 Software is responsible for making sure [DONE] does not overflow; for example by
                                                                 insuring there are not more than 2^20-1 instructions in flight that may request
                                                                 interrupts. */
#else /* Word 0 - Little Endian */
        uint64_t done                  : 20; /**< [ 19:  0](R/W/H) Done count. When NIX_AQ_INST_S[DONEINT] set and that instruction completes,
                                                                 NIX_AF_AQ_DONE[DONE] is incremented. Write to this field are for diagnostic use only;
                                                                 instead software writes NIX_AF_AQ_DONE_ACK with the number of decrements for this field.

                                                                 Interrupts are sent as follows:

                                                                 * When NIX_AF_AQ_DONE[DONE] = 0, then no results are pending, the interrupt
                                                                 coalescing timer is held to zero, and an interrupt is not sent.

                                                                 * When NIX_AF_AQ_DONE[DONE] != 0, then the interrupt coalescing timer
                                                                 counts every microsecond. If the counter is \>= NIX_AF_AQ_DONE_WAIT[TIME_WAIT],
                                                                 or NIX_AF_AQ_DONE[DONE] \>= NIX_AF_AQ_DONE_WAIT[NUM_WAIT], i.e. enough time
                                                                 has passed or enough results have arrived, then the interrupt is sent.
                                                                 Otherwise, it is not sent due to coalescing.

                                                                 * When NIX_AF_AQ_DONE_ACK is written (or NIX_AF_AQ_DONE is written but this is
                                                                 not typical), the interrupt coalescing timer restarts. Note after decrementing
                                                                 this interrupt equation is recomputed, for example if NIX_AF_AQ_DONE[DONE] \>=
                                                                 NIX_AF_AQ_DONE_WAIT[NUM_WAIT] and because the timer is zero, the interrupt will
                                                                 be resent immediately. (This covers the race case between software
                                                                 acknowledging an interrupt and a result returning.).

                                                                 * When NIX_AF_AQ_DONE_ENA_W1S[DONE] = 0, interrupts are not sent, but the
                                                                 counting described above still occurs.

                                                                 AQ instructions complete in order.

                                                                 Software is responsible for making sure [DONE] does not overflow; for example by
                                                                 insuring there are not more than 2^20-1 instructions in flight that may request
                                                                 interrupts. */
        uint64_t reserved_20_63        : 44;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_aq_done_s cn; */
};
typedef union bdk_nixx_af_aq_done bdk_nixx_af_aq_done_t;

static inline uint64_t BDK_NIXX_AF_AQ_DONE(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_DONE(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000450ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_DONE", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_DONE(a) bdk_nixx_af_aq_done_t
#define bustype_BDK_NIXX_AF_AQ_DONE(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_DONE(a) "NIXX_AF_AQ_DONE"
#define device_bar_BDK_NIXX_AF_AQ_DONE(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_DONE(a) (a)
#define arguments_BDK_NIXX_AF_AQ_DONE(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_done_ack
 *
 * NIX AF Admin Queue Done Count Ack Register
 * This register is written by software to acknowledge interrupts.
 */
union bdk_nixx_af_aq_done_ack
{
    uint64_t u;
    struct bdk_nixx_af_aq_done_ack_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_20_63        : 44;
        uint64_t done_ack              : 20; /**< [ 19:  0](R/W/H) Number of decrements to NIX_AF_AQ_DONE[DONE]. Reads NIX_AF_AQ_DONE[DONE].

                                                                 Written by software to acknowledge interrupts. If NIX_AF_AQ_DONE[DONE] is still
                                                                 nonzero the interrupt will be resent if the conditions described in
                                                                 NIX_AF_AQ_DONE[DONE] are satisfied.

                                                                 Internal:
                                                                 If [DONE_ACK] write value is greater than NIX_AF_AQ_DONE[DONE], hardware
                                                                 resets NIX_AF_AQ_DONE[DONE] to zero. */
#else /* Word 0 - Little Endian */
        uint64_t done_ack              : 20; /**< [ 19:  0](R/W/H) Number of decrements to NIX_AF_AQ_DONE[DONE]. Reads NIX_AF_AQ_DONE[DONE].

                                                                 Written by software to acknowledge interrupts. If NIX_AF_AQ_DONE[DONE] is still
                                                                 nonzero the interrupt will be resent if the conditions described in
                                                                 NIX_AF_AQ_DONE[DONE] are satisfied.

                                                                 Internal:
                                                                 If [DONE_ACK] write value is greater than NIX_AF_AQ_DONE[DONE], hardware
                                                                 resets NIX_AF_AQ_DONE[DONE] to zero. */
        uint64_t reserved_20_63        : 44;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_aq_done_ack_s cn; */
};
typedef union bdk_nixx_af_aq_done_ack bdk_nixx_af_aq_done_ack_t;

static inline uint64_t BDK_NIXX_AF_AQ_DONE_ACK(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_DONE_ACK(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000460ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_DONE_ACK", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_DONE_ACK(a) bdk_nixx_af_aq_done_ack_t
#define bustype_BDK_NIXX_AF_AQ_DONE_ACK(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_DONE_ACK(a) "NIXX_AF_AQ_DONE_ACK"
#define device_bar_BDK_NIXX_AF_AQ_DONE_ACK(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_DONE_ACK(a) (a)
#define arguments_BDK_NIXX_AF_AQ_DONE_ACK(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_done_ena_w1c
 *
 * NIX AF Admin Queue Done Interrupt Enable Clear Register
 */
union bdk_nixx_af_aq_done_ena_w1c
{
    uint64_t u;
    struct bdk_nixx_af_aq_done_ena_w1c_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t done                  : 1;  /**< [  0:  0](R/W1C) Read or clears NIX_AF_AQ_DONE_ENA_W1S[DONE]. */
#else /* Word 0 - Little Endian */
        uint64_t done                  : 1;  /**< [  0:  0](R/W1C) Read or clears NIX_AF_AQ_DONE_ENA_W1S[DONE]. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_aq_done_ena_w1c_s cn; */
};
typedef union bdk_nixx_af_aq_done_ena_w1c bdk_nixx_af_aq_done_ena_w1c_t;

static inline uint64_t BDK_NIXX_AF_AQ_DONE_ENA_W1C(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_DONE_ENA_W1C(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000498ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_DONE_ENA_W1C", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_DONE_ENA_W1C(a) bdk_nixx_af_aq_done_ena_w1c_t
#define bustype_BDK_NIXX_AF_AQ_DONE_ENA_W1C(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_DONE_ENA_W1C(a) "NIXX_AF_AQ_DONE_ENA_W1C"
#define device_bar_BDK_NIXX_AF_AQ_DONE_ENA_W1C(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_DONE_ENA_W1C(a) (a)
#define arguments_BDK_NIXX_AF_AQ_DONE_ENA_W1C(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_done_ena_w1s
 *
 * NIX AF Admin Queue Done Interrupt Enable Set Register
 * Write 1 to these registers will enable the DONEINT interrupt for the queue.
 */
union bdk_nixx_af_aq_done_ena_w1s
{
    uint64_t u;
    struct bdk_nixx_af_aq_done_ena_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t done                  : 1;  /**< [  0:  0](R/W1S) Write 1 will enable DONEINT for this queue. Write zero has no effect.
                                                                 Read will return the enable bit. */
#else /* Word 0 - Little Endian */
        uint64_t done                  : 1;  /**< [  0:  0](R/W1S) Write 1 will enable DONEINT for this queue. Write zero has no effect.
                                                                 Read will return the enable bit. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_aq_done_ena_w1s_s cn; */
};
typedef union bdk_nixx_af_aq_done_ena_w1s bdk_nixx_af_aq_done_ena_w1s_t;

static inline uint64_t BDK_NIXX_AF_AQ_DONE_ENA_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_DONE_ENA_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000490ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_DONE_ENA_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_DONE_ENA_W1S(a) bdk_nixx_af_aq_done_ena_w1s_t
#define bustype_BDK_NIXX_AF_AQ_DONE_ENA_W1S(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_DONE_ENA_W1S(a) "NIXX_AF_AQ_DONE_ENA_W1S"
#define device_bar_BDK_NIXX_AF_AQ_DONE_ENA_W1S(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_DONE_ENA_W1S(a) (a)
#define arguments_BDK_NIXX_AF_AQ_DONE_ENA_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_done_int
 *
 * NIX AF Admin Queue Done Interrupt Register
 */
union bdk_nixx_af_aq_done_int
{
    uint64_t u;
    struct bdk_nixx_af_aq_done_int_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t done                  : 1;  /**< [  0:  0](RO/H) Done interrupt. See NIX_AF_AQ_DONE[DONE]. Note this bit is read-only, to acknowledge
                                                                 interrupts use NIX_AF_AQ_DONE_ACK. To test interrupts, write nonzero to
                                                                 NIX_AF_AQ_DONE[DONE]. */
#else /* Word 0 - Little Endian */
        uint64_t done                  : 1;  /**< [  0:  0](RO/H) Done interrupt. See NIX_AF_AQ_DONE[DONE]. Note this bit is read-only, to acknowledge
                                                                 interrupts use NIX_AF_AQ_DONE_ACK. To test interrupts, write nonzero to
                                                                 NIX_AF_AQ_DONE[DONE]. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_aq_done_int_s cn; */
};
typedef union bdk_nixx_af_aq_done_int bdk_nixx_af_aq_done_int_t;

static inline uint64_t BDK_NIXX_AF_AQ_DONE_INT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_DONE_INT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000480ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_DONE_INT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_DONE_INT(a) bdk_nixx_af_aq_done_int_t
#define bustype_BDK_NIXX_AF_AQ_DONE_INT(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_DONE_INT(a) "NIXX_AF_AQ_DONE_INT"
#define device_bar_BDK_NIXX_AF_AQ_DONE_INT(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_DONE_INT(a) (a)
#define arguments_BDK_NIXX_AF_AQ_DONE_INT(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_done_int_w1s
 *
 * INTERNAL: NIX AF Admin Queue Done Interrupt Set Register
 */
union bdk_nixx_af_aq_done_int_w1s
{
    uint64_t u;
    struct bdk_nixx_af_aq_done_int_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t done                  : 1;  /**< [  0:  0](RO/H) Done interrupt. See NIX_AF_AQ_DONE[DONE]. Note this bit is read-only, to acknowledge
                                                                 interrupts use NIX_AF_AQ_DONE_ACK. To test interrupts, write nonzero to
                                                                 NIX_AF_AQ_DONE[DONE]. */
#else /* Word 0 - Little Endian */
        uint64_t done                  : 1;  /**< [  0:  0](RO/H) Done interrupt. See NIX_AF_AQ_DONE[DONE]. Note this bit is read-only, to acknowledge
                                                                 interrupts use NIX_AF_AQ_DONE_ACK. To test interrupts, write nonzero to
                                                                 NIX_AF_AQ_DONE[DONE]. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_aq_done_int_w1s_s cn; */
};
typedef union bdk_nixx_af_aq_done_int_w1s bdk_nixx_af_aq_done_int_w1s_t;

static inline uint64_t BDK_NIXX_AF_AQ_DONE_INT_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_DONE_INT_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000488ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_DONE_INT_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_DONE_INT_W1S(a) bdk_nixx_af_aq_done_int_w1s_t
#define bustype_BDK_NIXX_AF_AQ_DONE_INT_W1S(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_DONE_INT_W1S(a) "NIXX_AF_AQ_DONE_INT_W1S"
#define device_bar_BDK_NIXX_AF_AQ_DONE_INT_W1S(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_DONE_INT_W1S(a) (a)
#define arguments_BDK_NIXX_AF_AQ_DONE_INT_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_done_wait
 *
 * NIX AF Admin Queue Done Interrupt Coalescing Wait Register
 * Specifies the queue interrupt coalescing settings.
 */
union bdk_nixx_af_aq_done_wait
{
    uint64_t u;
    struct bdk_nixx_af_aq_done_wait_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t time_wait             : 16; /**< [ 47: 32](R/W) Time hold-off in microseconds. When NIX_AF_AQ_DONE[DONE] = 0, or
                                                                 NIX_AF_AQ_DONE_ACK is written a timer is cleared. The timer increments
                                                                 every microsecond, and interrupt coalescing ends when timer reaches
                                                                 [TIME_WAIT]; see NIX_AF_AQ_DONE[DONE]. If 0x0, time coalescing is disabled. */
        uint64_t reserved_20_31        : 12;
        uint64_t num_wait              : 20; /**< [ 19:  0](R/W) Number of messages hold-off. When NIX_AF_AQ_DONE[DONE] \>= [NUM_WAIT] then
                                                                 interrupt coalescing ends; see NIX_AF_AQ_DONE[DONE]. If 0x0, same behavior as
                                                                 0x1. */
#else /* Word 0 - Little Endian */
        uint64_t num_wait              : 20; /**< [ 19:  0](R/W) Number of messages hold-off. When NIX_AF_AQ_DONE[DONE] \>= [NUM_WAIT] then
                                                                 interrupt coalescing ends; see NIX_AF_AQ_DONE[DONE]. If 0x0, same behavior as
                                                                 0x1. */
        uint64_t reserved_20_31        : 12;
        uint64_t time_wait             : 16; /**< [ 47: 32](R/W) Time hold-off in microseconds. When NIX_AF_AQ_DONE[DONE] = 0, or
                                                                 NIX_AF_AQ_DONE_ACK is written a timer is cleared. The timer increments
                                                                 every microsecond, and interrupt coalescing ends when timer reaches
                                                                 [TIME_WAIT]; see NIX_AF_AQ_DONE[DONE]. If 0x0, time coalescing is disabled. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_aq_done_wait_s cn; */
};
typedef union bdk_nixx_af_aq_done_wait bdk_nixx_af_aq_done_wait_t;

static inline uint64_t BDK_NIXX_AF_AQ_DONE_WAIT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_DONE_WAIT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000440ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_DONE_WAIT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_DONE_WAIT(a) bdk_nixx_af_aq_done_wait_t
#define bustype_BDK_NIXX_AF_AQ_DONE_WAIT(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_DONE_WAIT(a) "NIXX_AF_AQ_DONE_WAIT"
#define device_bar_BDK_NIXX_AF_AQ_DONE_WAIT(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_DONE_WAIT(a) (a)
#define arguments_BDK_NIXX_AF_AQ_DONE_WAIT(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_door
 *
 * NIX AF Admin Queue Doorbell Register
 * Software writes to this register to enqueue entries to AQ.
 */
union bdk_nixx_af_aq_door
{
    uint64_t u;
    struct bdk_nixx_af_aq_door_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_16_63        : 48;
        uint64_t count                 : 16; /**< [ 15:  0](WO) Number of enqueued 16-byte entries. Hardware advances
                                                                 NIX_AF_AQ_STATUS[TAIL_PTR] by this value.

                                                                 A doorbell write that would overflow the AQ ring is suppressed and sets
                                                                 NIX_AF_AQ_STATUS[AQ_ERR] and NIX_AF_ERR_INT[AQ_DOOR_ERR]. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 16; /**< [ 15:  0](WO) Number of enqueued 16-byte entries. Hardware advances
                                                                 NIX_AF_AQ_STATUS[TAIL_PTR] by this value.

                                                                 A doorbell write that would overflow the AQ ring is suppressed and sets
                                                                 NIX_AF_AQ_STATUS[AQ_ERR] and NIX_AF_ERR_INT[AQ_DOOR_ERR]. */
        uint64_t reserved_16_63        : 48;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_aq_door_s cn; */
};
typedef union bdk_nixx_af_aq_door bdk_nixx_af_aq_door_t;

static inline uint64_t BDK_NIXX_AF_AQ_DOOR(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_DOOR(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000430ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_DOOR", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_DOOR(a) bdk_nixx_af_aq_door_t
#define bustype_BDK_NIXX_AF_AQ_DOOR(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_DOOR(a) "NIXX_AF_AQ_DOOR"
#define device_bar_BDK_NIXX_AF_AQ_DOOR(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_DOOR(a) (a)
#define arguments_BDK_NIXX_AF_AQ_DOOR(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_aq_status
 *
 * NIX AF Admin Queue Status Register
 */
union bdk_nixx_af_aq_status
{
    uint64_t u;
    struct bdk_nixx_af_aq_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t aq_err                : 1;  /**< [ 63: 63](R/W1C/H) AQ error. See NIX_AF_ERR_INT[AQ_INST_FAULT,AQ_RES_FAULT,AQ_DOOR_ERR] and
                                                                 NIX_AF_RAS[AQ_INST_POISON,AQ_RES_POISON].
                                                                 When set, hardware stops reading instructions from the AQ ring. Software
                                                                 clears the error by writing a one back. */
        uint64_t reserved_56_62        : 7;
        uint64_t tail_ptr              : 20; /**< [ 55: 36](R/W/H) Tail pointer \<24:4\> of AQ ring relative to NIX_AF_AQ_BASE. Address bits \<3:0\>
                                                                 are always 0x0. Hardware advances the tail pointer when software writes to
                                                                 NIX_AF_AQ_DOOR. */
        uint64_t reserved_24_35        : 12;
        uint64_t head_ptr              : 20; /**< [ 23:  4](R/W/H) Head pointer \<24:4\> of AQ ring relative to NIX_AF_AQ_BASE. Address bits \<3:0\>
                                                                 are always 0x0. Hardware advances the head pointer when it pops an entry
                                                                 from the AQ. */
        uint64_t reserved_0_3          : 4;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_3          : 4;
        uint64_t head_ptr              : 20; /**< [ 23:  4](R/W/H) Head pointer \<24:4\> of AQ ring relative to NIX_AF_AQ_BASE. Address bits \<3:0\>
                                                                 are always 0x0. Hardware advances the head pointer when it pops an entry
                                                                 from the AQ. */
        uint64_t reserved_24_35        : 12;
        uint64_t tail_ptr              : 20; /**< [ 55: 36](R/W/H) Tail pointer \<24:4\> of AQ ring relative to NIX_AF_AQ_BASE. Address bits \<3:0\>
                                                                 are always 0x0. Hardware advances the tail pointer when software writes to
                                                                 NIX_AF_AQ_DOOR. */
        uint64_t reserved_56_62        : 7;
        uint64_t aq_err                : 1;  /**< [ 63: 63](R/W1C/H) AQ error. See NIX_AF_ERR_INT[AQ_INST_FAULT,AQ_RES_FAULT,AQ_DOOR_ERR] and
                                                                 NIX_AF_RAS[AQ_INST_POISON,AQ_RES_POISON].
                                                                 When set, hardware stops reading instructions from the AQ ring. Software
                                                                 clears the error by writing a one back. */
#endif /* Word 0 - End */
    } s;
    struct bdk_nixx_af_aq_status_cn
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t aq_err                : 1;  /**< [ 63: 63](R/W1C/H) AQ error. See NIX_AF_ERR_INT[AQ_INST_FAULT,AQ_RES_FAULT,AQ_DOOR_ERR] and
                                                                 NIX_AF_RAS[AQ_INST_POISON,AQ_RES_POISON].
                                                                 When set, hardware stops reading instructions from the AQ ring. Software
                                                                 clears the error by writing a one back. */
        uint64_t reserved_56_62        : 7;
        uint64_t tail_ptr              : 20; /**< [ 55: 36](R/W/H) Tail pointer \<24:4\> of AQ ring relative to NIX_AF_AQ_BASE. Address bits \<3:0\>
                                                                 are always 0x0. Hardware advances the tail pointer when software writes to
                                                                 NIX_AF_AQ_DOOR. */
        uint64_t reserved_32_35        : 4;
        uint64_t reserved_24_31        : 8;
        uint64_t head_ptr              : 20; /**< [ 23:  4](R/W/H) Head pointer \<24:4\> of AQ ring relative to NIX_AF_AQ_BASE. Address bits \<3:0\>
                                                                 are always 0x0. Hardware advances the head pointer when it pops an entry
                                                                 from the AQ. */
        uint64_t reserved_0_3          : 4;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_3          : 4;
        uint64_t head_ptr              : 20; /**< [ 23:  4](R/W/H) Head pointer \<24:4\> of AQ ring relative to NIX_AF_AQ_BASE. Address bits \<3:0\>
                                                                 are always 0x0. Hardware advances the head pointer when it pops an entry
                                                                 from the AQ. */
        uint64_t reserved_24_31        : 8;
        uint64_t reserved_32_35        : 4;
        uint64_t tail_ptr              : 20; /**< [ 55: 36](R/W/H) Tail pointer \<24:4\> of AQ ring relative to NIX_AF_AQ_BASE. Address bits \<3:0\>
                                                                 are always 0x0. Hardware advances the tail pointer when software writes to
                                                                 NIX_AF_AQ_DOOR. */
        uint64_t reserved_56_62        : 7;
        uint64_t aq_err                : 1;  /**< [ 63: 63](R/W1C/H) AQ error. See NIX_AF_ERR_INT[AQ_INST_FAULT,AQ_RES_FAULT,AQ_DOOR_ERR] and
                                                                 NIX_AF_RAS[AQ_INST_POISON,AQ_RES_POISON].
                                                                 When set, hardware stops reading instructions from the AQ ring. Software
                                                                 clears the error by writing a one back. */
#endif /* Word 0 - End */
    } cn;
};
typedef union bdk_nixx_af_aq_status bdk_nixx_af_aq_status_t;

static inline uint64_t BDK_NIXX_AF_AQ_STATUS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AQ_STATUS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000420ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AQ_STATUS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AQ_STATUS(a) bdk_nixx_af_aq_status_t
#define bustype_BDK_NIXX_AF_AQ_STATUS(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AQ_STATUS(a) "NIXX_AF_AQ_STATUS"
#define device_bar_BDK_NIXX_AF_AQ_STATUS(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AQ_STATUS(a) (a)
#define arguments_BDK_NIXX_AF_AQ_STATUS(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_avg_delay
 *
 * NIX AF Queue Average Delay Register
 */
union bdk_nixx_af_avg_delay
{
    uint64_t u;
    struct bdk_nixx_af_avg_delay_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_40_63        : 24;
        uint64_t avg_timer             : 16; /**< [ 39: 24](R/W/H) Running counter that is incremented every [AVG_DLY]+1 microseconds. */
        uint64_t reserved_19_23        : 5;
        uint64_t avg_dly               : 19; /**< [ 18:  0](R/W) Average-queue-size delay. [AVG_DLY]+1 is the number of microseconds per
                                                                 timer tick for calculating the moving average for each CQ level. Note the
                                                                 minimum of one microsecond implies that at 100 M packets/sec, approximately
                                                                 100 packets may arrive between average calculations.

                                                                 Larger [AVG_DLY] causes the moving averages of all CQ levels to track
                                                                 changes in the actual free space more slowly. Larger NIX_CQ_CTX_S[AVG_CON])
                                                                 values causes a specific CQ to track more slowly, but only affects an
                                                                 individual level, rather than all. */
#else /* Word 0 - Little Endian */
        uint64_t avg_dly               : 19; /**< [ 18:  0](R/W) Average-queue-size delay. [AVG_DLY]+1 is the number of microseconds per
                                                                 timer tick for calculating the moving average for each CQ level. Note the
                                                                 minimum of one microsecond implies that at 100 M packets/sec, approximately
                                                                 100 packets may arrive between average calculations.

                                                                 Larger [AVG_DLY] causes the moving averages of all CQ levels to track
                                                                 changes in the actual free space more slowly. Larger NIX_CQ_CTX_S[AVG_CON])
                                                                 values causes a specific CQ to track more slowly, but only affects an
                                                                 individual level, rather than all. */
        uint64_t reserved_19_23        : 5;
        uint64_t avg_timer             : 16; /**< [ 39: 24](R/W/H) Running counter that is incremented every [AVG_DLY]+1 microseconds. */
        uint64_t reserved_40_63        : 24;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_avg_delay_s cn; */
};
typedef union bdk_nixx_af_avg_delay bdk_nixx_af_avg_delay_t;

static inline uint64_t BDK_NIXX_AF_AVG_DELAY(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_AVG_DELAY(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400000e0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_AVG_DELAY", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_AVG_DELAY(a) bdk_nixx_af_avg_delay_t
#define bustype_BDK_NIXX_AF_AVG_DELAY(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_AVG_DELAY(a) "NIXX_AF_AVG_DELAY"
#define device_bar_BDK_NIXX_AF_AVG_DELAY(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_AVG_DELAY(a) (a)
#define arguments_BDK_NIXX_AF_AVG_DELAY(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_blk_rst
 *
 * NIX AF Block Reset Register
 */
union bdk_nixx_af_blk_rst
{
    uint64_t u;
    struct bdk_nixx_af_blk_rst_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t busy                  : 1;  /**< [ 63: 63](RO/H) When one, the block is busy completing reset. No access except the reading of
                                                                 this bit should occur to the block until this is clear. */
        uint64_t reserved_1_62         : 62;
        uint64_t rst                   : 1;  /**< [  0:  0](WO/H) Write one to reset the block, except for privileged AF registers in PF BAR0
                                                                 (TIM_PRIV_*). Software must ensure that all block activity is quiesced before
                                                                 writing 1. */
#else /* Word 0 - Little Endian */
        uint64_t rst                   : 1;  /**< [  0:  0](WO/H) Write one to reset the block, except for privileged AF registers in PF BAR0
                                                                 (TIM_PRIV_*). Software must ensure that all block activity is quiesced before
                                                                 writing 1. */
        uint64_t reserved_1_62         : 62;
        uint64_t busy                  : 1;  /**< [ 63: 63](RO/H) When one, the block is busy completing reset. No access except the reading of
                                                                 this bit should occur to the block until this is clear. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_blk_rst_s cn; */
};
typedef union bdk_nixx_af_blk_rst bdk_nixx_af_blk_rst_t;

static inline uint64_t BDK_NIXX_AF_BLK_RST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_BLK_RST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400000b0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_BLK_RST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_BLK_RST(a) bdk_nixx_af_blk_rst_t
#define bustype_BDK_NIXX_AF_BLK_RST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_BLK_RST(a) "NIXX_AF_BLK_RST"
#define device_bar_BDK_NIXX_AF_BLK_RST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_BLK_RST(a) (a)
#define arguments_BDK_NIXX_AF_BLK_RST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_cfg
 *
 * NIX AF General Configuration Register
 */
union bdk_nixx_af_cfg
{
    uint64_t u;
    struct bdk_nixx_af_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_5_63         : 59;
        uint64_t force_intf_clk_en     : 1;  /**< [  4:  4](R/W) For debug only. Must be zero during normal operation.
                                                                 Internal:
                                                                 Force the clk_en signal active on busses between PnR blocks. */
        uint64_t force_cond_clk_en     : 1;  /**< [  3:  3](R/W) For debug only. Must be zero during normal operation.
                                                                 Internal:
                                                                 Force the conditional clock active within each PnR block. */
        uint64_t calibrate_x2p         : 1;  /**< [  2:  2](R/W) Calibrate X2P bus. Writing this bit from zero to one starts a calibration cycle.
                                                                 Software may then monitor the NIX_AF_STATUS[CALIBRATE_DONE] bit for completion,
                                                                 and clear this bit. */
        uint64_t af_be                 : 1;  /**< [  1:  1](R/W) Admin function big-endian select. Specifies endianness of all admin queue
                                                                 instructions, results and associated structures stored in LLC/DRAM:

                                                                 0 = Little-endian. All AF data structures are in byte invariant little-endian
                                                                 format (LE8) with the following ordering within each 64-bit word: \<7:0\> at byte
                                                                 address 0, \<15:8\> at address 1, ..., \<63:56\> at address 0x7.

                                                                 1 = Big-endian. All AF data structures are in byte invariant big-endian format
                                                                 (BE8) with the following ordering within each 64-bit word: \<63:56\> at byte
                                                                 address 0, \<55:48\> at address 1, ..., \<7:0\> at address 0x7.

                                                                 The affected data structures are:
                                                                 * NIX_AQ_INST_S.
                                                                 * NIX_AQ_RES_S.
                                                                 * Context read/write data following NIX_AQ_RES_S. */
        uint64_t ena                   : 1;  /**< [  0:  0](R/W) Enable NIX block.
                                                                 Internal:
                                                                 Used to enable conditional SCLK. */
#else /* Word 0 - Little Endian */
        uint64_t ena                   : 1;  /**< [  0:  0](R/W) Enable NIX block.
                                                                 Internal:
                                                                 Used to enable conditional SCLK. */
        uint64_t af_be                 : 1;  /**< [  1:  1](R/W) Admin function big-endian select. Specifies endianness of all admin queue
                                                                 instructions, results and associated structures stored in LLC/DRAM:

                                                                 0 = Little-endian. All AF data structures are in byte invariant little-endian
                                                                 format (LE8) with the following ordering within each 64-bit word: \<7:0\> at byte
                                                                 address 0, \<15:8\> at address 1, ..., \<63:56\> at address 0x7.

                                                                 1 = Big-endian. All AF data structures are in byte invariant big-endian format
                                                                 (BE8) with the following ordering within each 64-bit word: \<63:56\> at byte
                                                                 address 0, \<55:48\> at address 1, ..., \<7:0\> at address 0x7.

                                                                 The affected data structures are:
                                                                 * NIX_AQ_INST_S.
                                                                 * NIX_AQ_RES_S.
                                                                 * Context read/write data following NIX_AQ_RES_S. */
        uint64_t calibrate_x2p         : 1;  /**< [  2:  2](R/W) Calibrate X2P bus. Writing this bit from zero to one starts a calibration cycle.
                                                                 Software may then monitor the NIX_AF_STATUS[CALIBRATE_DONE] bit for completion,
                                                                 and clear this bit. */
        uint64_t force_cond_clk_en     : 1;  /**< [  3:  3](R/W) For debug only. Must be zero during normal operation.
                                                                 Internal:
                                                                 Force the conditional clock active within each PnR block. */
        uint64_t force_intf_clk_en     : 1;  /**< [  4:  4](R/W) For debug only. Must be zero during normal operation.
                                                                 Internal:
                                                                 Force the clk_en signal active on busses between PnR blocks. */
        uint64_t reserved_5_63         : 59;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_cfg_s cn; */
};
typedef union bdk_nixx_af_cfg bdk_nixx_af_cfg_t;

static inline uint64_t BDK_NIXX_AF_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000000ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_CFG(a) bdk_nixx_af_cfg_t
#define bustype_BDK_NIXX_AF_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_CFG(a) "NIXX_AF_CFG"
#define device_bar_BDK_NIXX_AF_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_CFG(a) (a)
#define arguments_BDK_NIXX_AF_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_cint_delay
 *
 * NIX AF Completion Interrupt Delay Register
 */
union bdk_nixx_af_cint_delay
{
    uint64_t u;
    struct bdk_nixx_af_cint_delay_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_32_63        : 32;
        uint64_t cint_timer            : 16; /**< [ 31: 16](R/W/H) Completion interrupt timer clock. Running counter that is
                                                                 incremented every ([CINT_DLY]+1)*100 nanoseconds. */
        uint64_t reserved_10_15        : 6;
        uint64_t cint_dly              : 10; /**< [  9:  0](R/W) Completion interrupt timer delay. ([CINT_DLY]+1)*100 is the number
                                                                 of nanoseconds per timer tick for completion interrupts. */
#else /* Word 0 - Little Endian */
        uint64_t cint_dly              : 10; /**< [  9:  0](R/W) Completion interrupt timer delay. ([CINT_DLY]+1)*100 is the number
                                                                 of nanoseconds per timer tick for completion interrupts. */
        uint64_t reserved_10_15        : 6;
        uint64_t cint_timer            : 16; /**< [ 31: 16](R/W/H) Completion interrupt timer clock. Running counter that is
                                                                 incremented every ([CINT_DLY]+1)*100 nanoseconds. */
        uint64_t reserved_32_63        : 32;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_cint_delay_s cn; */
};
typedef union bdk_nixx_af_cint_delay bdk_nixx_af_cint_delay_t;

static inline uint64_t BDK_NIXX_AF_CINT_DELAY(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_CINT_DELAY(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400000f0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_CINT_DELAY", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_CINT_DELAY(a) bdk_nixx_af_cint_delay_t
#define bustype_BDK_NIXX_AF_CINT_DELAY(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_CINT_DELAY(a) "NIXX_AF_CINT_DELAY"
#define device_bar_BDK_NIXX_AF_CINT_DELAY(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_CINT_DELAY(a) (a)
#define arguments_BDK_NIXX_AF_CINT_DELAY(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_cint_timer#
 *
 * NIX AF Completion Interrupt Timer Registers
 */
union bdk_nixx_af_cint_timerx
{
    uint64_t u;
    struct bdk_nixx_af_cint_timerx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_33_63        : 31;
        uint64_t active                : 1;  /**< [ 32: 32](RO/H) This CINT timer is active and the remaining fields in this register
                                                                 are valid when this bit is set. See also [EXPIR_TIME].

                                                                 A CINT timer is activated when NIX_LF_CINT()_CNT[QCOUNT]
                                                                 increments from 0. The timer is reactivated when software writes a
                                                                 one to clear NIX_LF_CINT()_INT[INTR] and
                                                                 NIX_LF_CINT()_CNT[ECOUNT] is nonzero.

                                                                 An active CINT timer is removed/deactivated when one of the
                                                                 following occurs:
                                                                 * NIX_LF_CINT()_INT[INTR] is set.
                                                                 * NIX_LF_CINT()_CNT[ECOUNT] = 0.

                                                                 Note that when all CINT timers are active and a new timer needs to be
                                                                 activated, the least recently activated or reactivated timer is removed and
                                                                 the associated NIX_LF_CINT()_INT[INTR] is set. */
        uint64_t lf                    : 8;  /**< [ 31: 24](RO/H) Local function associated with this timer. */
        uint64_t reserved_23           : 1;
        uint64_t cint                  : 7;  /**< [ 22: 16](RO/H) Completion interrupt within [LF] associated with this timer. */
        uint64_t expir_time            : 16; /**< [ 15:  0](RO/H) CINT expiration time. Updated as follows when the CINT timer is
                                                                 activated or reactivated (see [ACTIVE]):
                                                                 _ [EXPIR_TIME] = NIX_AF_CINT_DELAY[CINT_TIMER] + NIX_LF_CINT()_WAIT[TIME_WAIT]

                                                                 When [ACTIVE] is set and NIX_AF_CINT_DELAY[CINT_TIMER] crosses
                                                                 [EXPIR_TIME], hardware sets the associated
                                                                 NIX_LF_CINT()_INT[INTR] bit and deactivates this CINT timer. */
#else /* Word 0 - Little Endian */
        uint64_t expir_time            : 16; /**< [ 15:  0](RO/H) CINT expiration time. Updated as follows when the CINT timer is
                                                                 activated or reactivated (see [ACTIVE]):
                                                                 _ [EXPIR_TIME] = NIX_AF_CINT_DELAY[CINT_TIMER] + NIX_LF_CINT()_WAIT[TIME_WAIT]

                                                                 When [ACTIVE] is set and NIX_AF_CINT_DELAY[CINT_TIMER] crosses
                                                                 [EXPIR_TIME], hardware sets the associated
                                                                 NIX_LF_CINT()_INT[INTR] bit and deactivates this CINT timer. */
        uint64_t cint                  : 7;  /**< [ 22: 16](RO/H) Completion interrupt within [LF] associated with this timer. */
        uint64_t reserved_23           : 1;
        uint64_t lf                    : 8;  /**< [ 31: 24](RO/H) Local function associated with this timer. */
        uint64_t active                : 1;  /**< [ 32: 32](RO/H) This CINT timer is active and the remaining fields in this register
                                                                 are valid when this bit is set. See also [EXPIR_TIME].

                                                                 A CINT timer is activated when NIX_LF_CINT()_CNT[QCOUNT]
                                                                 increments from 0. The timer is reactivated when software writes a
                                                                 one to clear NIX_LF_CINT()_INT[INTR] and
                                                                 NIX_LF_CINT()_CNT[ECOUNT] is nonzero.

                                                                 An active CINT timer is removed/deactivated when one of the
                                                                 following occurs:
                                                                 * NIX_LF_CINT()_INT[INTR] is set.
                                                                 * NIX_LF_CINT()_CNT[ECOUNT] = 0.

                                                                 Note that when all CINT timers are active and a new timer needs to be
                                                                 activated, the least recently activated or reactivated timer is removed and
                                                                 the associated NIX_LF_CINT()_INT[INTR] is set. */
        uint64_t reserved_33_63        : 31;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_cint_timerx_s cn; */
};
typedef union bdk_nixx_af_cint_timerx bdk_nixx_af_cint_timerx_t;

static inline uint64_t BDK_NIXX_AF_CINT_TIMERX(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_CINT_TIMERX(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001a40ll + 0x10000000ll * ((a) & 0x0) + 0x40000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_CINT_TIMERX", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_CINT_TIMERX(a,b) bdk_nixx_af_cint_timerx_t
#define bustype_BDK_NIXX_AF_CINT_TIMERX(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_CINT_TIMERX(a,b) "NIXX_AF_CINT_TIMERX"
#define device_bar_BDK_NIXX_AF_CINT_TIMERX(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_CINT_TIMERX(a,b) (a)
#define arguments_BDK_NIXX_AF_CINT_TIMERX(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_const
 *
 * NIX AF Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_const
{
    uint64_t u;
    struct bdk_nixx_af_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_56_63        : 8;
        uint64_t links                 : 8;  /**< [ 55: 48](RO) Number of links enumerated by NIX_LINK_E. */
        uint64_t lbk_from_nix1         : 6;  /**< [ 47: 42](RO/H) Reserved. */
        uint64_t lbk_from_nix0         : 6;  /**< [ 41: 36](RO/H) Link number that receives loopback packets transmitted by NIX(0).
                                                                 Value is NIX_LINK_E::LBK(0) for NIX(0) to NIX(0) loopback. */
        uint64_t lbk_to_nix1           : 6;  /**< [ 35: 30](RO/H) Reserved. */
        uint64_t lbk_to_nix0           : 6;  /**< [ 29: 24](RO/H) Link number for transmitting loopback packets to NIX(0).
                                                                 Value is NIX_LINK_E::LBK(0) for NIX(0) to NIX(0) loopback. */
        uint64_t lbk_channels          : 8;  /**< [ 23: 16](RO) Number of channels per LBK interface/link. */
        uint64_t num_cgx               : 4;  /**< [ 15: 12](RO) Number of CGX interfaces enumerated in NIX_LINK_E. */
        uint64_t cgx_lmacs             : 4;  /**< [ 11:  8](RO) Number of LMACs (links) per CGX. */
        uint64_t cgx_lmac_channels     : 8;  /**< [  7:  0](RO) Number of channels per CGX link/LMAC. */
#else /* Word 0 - Little Endian */
        uint64_t cgx_lmac_channels     : 8;  /**< [  7:  0](RO) Number of channels per CGX link/LMAC. */
        uint64_t cgx_lmacs             : 4;  /**< [ 11:  8](RO) Number of LMACs (links) per CGX. */
        uint64_t num_cgx               : 4;  /**< [ 15: 12](RO) Number of CGX interfaces enumerated in NIX_LINK_E. */
        uint64_t lbk_channels          : 8;  /**< [ 23: 16](RO) Number of channels per LBK interface/link. */
        uint64_t lbk_to_nix0           : 6;  /**< [ 29: 24](RO/H) Link number for transmitting loopback packets to NIX(0).
                                                                 Value is NIX_LINK_E::LBK(0) for NIX(0) to NIX(0) loopback. */
        uint64_t lbk_to_nix1           : 6;  /**< [ 35: 30](RO/H) Reserved. */
        uint64_t lbk_from_nix0         : 6;  /**< [ 41: 36](RO/H) Link number that receives loopback packets transmitted by NIX(0).
                                                                 Value is NIX_LINK_E::LBK(0) for NIX(0) to NIX(0) loopback. */
        uint64_t lbk_from_nix1         : 6;  /**< [ 47: 42](RO/H) Reserved. */
        uint64_t links                 : 8;  /**< [ 55: 48](RO) Number of links enumerated by NIX_LINK_E. */
        uint64_t reserved_56_63        : 8;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_const_s cn; */
};
typedef union bdk_nixx_af_const bdk_nixx_af_const_t;

static inline uint64_t BDK_NIXX_AF_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000020ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_CONST(a) bdk_nixx_af_const_t
#define bustype_BDK_NIXX_AF_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_CONST(a) "NIXX_AF_CONST"
#define device_bar_BDK_NIXX_AF_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_CONST(a) (a)
#define arguments_BDK_NIXX_AF_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_const1
 *
 * NIX AF Constants 1 Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_const1
{
    uint64_t u;
    struct bdk_nixx_af_const1_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_40_63        : 24;
        uint64_t lf_rx_stats           : 8;  /**< [ 39: 32](RO) Number of per-LF receive statistics counters enumerated by NIX_STAT_LF_RX_E. */
        uint64_t lf_tx_stats           : 8;  /**< [ 31: 24](RO) Number of per-LF transmit statistics counters enumerated by NIX_STAT_LF_TX_E. */
        uint64_t rx_bpids              : 12; /**< [ 23: 12](RO) Number of receive backpressure IDs. */
        uint64_t sdp_channels          : 12; /**< [ 11:  0](RO) Number of channels per SDP interface/link. */
#else /* Word 0 - Little Endian */
        uint64_t sdp_channels          : 12; /**< [ 11:  0](RO) Number of channels per SDP interface/link. */
        uint64_t rx_bpids              : 12; /**< [ 23: 12](RO) Number of receive backpressure IDs. */
        uint64_t lf_tx_stats           : 8;  /**< [ 31: 24](RO) Number of per-LF transmit statistics counters enumerated by NIX_STAT_LF_TX_E. */
        uint64_t lf_rx_stats           : 8;  /**< [ 39: 32](RO) Number of per-LF receive statistics counters enumerated by NIX_STAT_LF_RX_E. */
        uint64_t reserved_40_63        : 24;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_const1_s cn; */
};
typedef union bdk_nixx_af_const1 bdk_nixx_af_const1_t;

static inline uint64_t BDK_NIXX_AF_CONST1(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_CONST1(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000028ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_CONST1", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_CONST1(a) bdk_nixx_af_const1_t
#define bustype_BDK_NIXX_AF_CONST1(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_CONST1(a) "NIXX_AF_CONST1"
#define device_bar_BDK_NIXX_AF_CONST1(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_CONST1(a) (a)
#define arguments_BDK_NIXX_AF_CONST1(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_const2
 *
 * NIX AF Constants 2 Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_const2
{
    uint64_t u;
    struct bdk_nixx_af_const2_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_36_63        : 28;
        uint64_t cints                 : 12; /**< [ 35: 24](RO) Number of completion interrupts per LF. */
        uint64_t qints                 : 12; /**< [ 23: 12](RO) Number of queue interrupts per LF. */
        uint64_t lfs                   : 12; /**< [ 11:  0](RO) Number of Local Functions. */
#else /* Word 0 - Little Endian */
        uint64_t lfs                   : 12; /**< [ 11:  0](RO) Number of Local Functions. */
        uint64_t qints                 : 12; /**< [ 23: 12](RO) Number of queue interrupts per LF. */
        uint64_t cints                 : 12; /**< [ 35: 24](RO) Number of completion interrupts per LF. */
        uint64_t reserved_36_63        : 28;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_const2_s cn; */
};
typedef union bdk_nixx_af_const2 bdk_nixx_af_const2_t;

static inline uint64_t BDK_NIXX_AF_CONST2(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_CONST2(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000030ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_CONST2", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_CONST2(a) bdk_nixx_af_const2_t
#define bustype_BDK_NIXX_AF_CONST2(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_CONST2(a) "NIXX_AF_CONST2"
#define device_bar_BDK_NIXX_AF_CONST2(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_CONST2(a) (a)
#define arguments_BDK_NIXX_AF_CONST2(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_const3
 *
 * NIX AF Constants 2 Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_const3
{
    uint64_t u;
    struct bdk_nixx_af_const3_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_36_63        : 28;
        uint64_t dyno_array_log2counters : 4;/**< [ 35: 32](RO) IPSEC dynamic ordering counter array size as log2(counters). Size of
                                                                 counter array accessed by the NIX admin queue when NIX_AQ_INST_S[CTYPE] =
                                                                 NIX_AQ_CTYPE_E::DYNO. The size of the array in bytes is
                                                                 1 \<\< ([DYNO_ARRAY_LOG2COUNTERS] + [DYNO_LOG2BYTES]). */
        uint64_t dyno_log2bytes        : 4;  /**< [ 31: 28](RO) IPSEC dynamic ordering counter size as log2(bytes). Size of each entry in a
                                                                 local function's dynamic ordering (DYNO) counter table in NDC/LLC/DRAM. See
                                                                 NIX_AF_LF()_RX_IPSEC_DYNO_BASE and NIX_AF_LF()_RX_IPSEC_DYNO_CFG. */
        uint64_t cint_log2bytes        : 4;  /**< [ 27: 24](RO) Completion interrupt context size as log2(bytes). Size of each
                                                                 NIX_CINT_HW_S structure in a local function's completion interrupt context
                                                                 table NDC/LLC/DRAM. See NIX_AF_LF()_CINTS_BASE and NIX_AF_LF()_CINTS_CFG. */
        uint64_t qint_log2bytes        : 4;  /**< [ 23: 20](RO) Queue interrupt context size as log2(bytes). Size of each NIX_QINT_HW_S
                                                                 structure in a local function's queue interrupt context table NDC/LLC/DRAM.
                                                                 See NIX_AF_LF()_QINTS_BASE and NIX_AF_LF()_QINTS_CFG. */
        uint64_t mce_log2bytes         : 4;  /**< [ 19: 16](RO) Receive multicast/mirror entry size as log2(bytes). Size of each
                                                                 NIX_RX_MCE_S structure in the AF multicast/mirror table NDC/LLC/DRAM. See
                                                                 NIX_AF_LF()_RX_MCAST_BASE and NIX_AF_LF()_RX_MCAST_CFG. */
        uint64_t rsse_log2bytes        : 4;  /**< [ 15: 12](RO) RSS entry size as log2(bytes). Size of each NIX_RSSE_S structure in a
                                                                 local function's RSS table NDC/LLC/DRAM. See NIX_AF_LF()_RSS_BASE
                                                                 and NIX_AF_LF()_RSS_CFG. */
        uint64_t cq_ctx_log2bytes      : 4;  /**< [ 11:  8](RO) CQ context size as log2(bytes). Size of each NIX_CQ_CTX_S structure in a
                                                                 local function's CQ context table NDC/LLC/DRAM. See NIX_AF_LF()_CQS_BASE
                                                                 and NIX_AF_LF()_CQS_CFG. */
        uint64_t rq_ctx_log2bytes      : 4;  /**< [  7:  4](RO) RQ context size as log2(bytes). Size of each NIX_RQ_CTX_S structure in a
                                                                 local function's RQ context table NDC/LLC/DRAM. See NIX_AF_LF()_RQS_BASE
                                                                 and NIX_AF_LF()_RQS_CFG. */
        uint64_t sq_ctx_log2bytes      : 4;  /**< [  3:  0](RO) SQ context size as log2(bytes). Size of each NIX_SQ_CTX_HW_S structure in a
                                                                 local function's SQ context table NDC/LLC/DRAM. See NIX_AF_LF()_SQS_BASE
                                                                 and NIX_AF_LF()_SQS_CFG. */
#else /* Word 0 - Little Endian */
        uint64_t sq_ctx_log2bytes      : 4;  /**< [  3:  0](RO) SQ context size as log2(bytes). Size of each NIX_SQ_CTX_HW_S structure in a
                                                                 local function's SQ context table NDC/LLC/DRAM. See NIX_AF_LF()_SQS_BASE
                                                                 and NIX_AF_LF()_SQS_CFG. */
        uint64_t rq_ctx_log2bytes      : 4;  /**< [  7:  4](RO) RQ context size as log2(bytes). Size of each NIX_RQ_CTX_S structure in a
                                                                 local function's RQ context table NDC/LLC/DRAM. See NIX_AF_LF()_RQS_BASE
                                                                 and NIX_AF_LF()_RQS_CFG. */
        uint64_t cq_ctx_log2bytes      : 4;  /**< [ 11:  8](RO) CQ context size as log2(bytes). Size of each NIX_CQ_CTX_S structure in a
                                                                 local function's CQ context table NDC/LLC/DRAM. See NIX_AF_LF()_CQS_BASE
                                                                 and NIX_AF_LF()_CQS_CFG. */
        uint64_t rsse_log2bytes        : 4;  /**< [ 15: 12](RO) RSS entry size as log2(bytes). Size of each NIX_RSSE_S structure in a
                                                                 local function's RSS table NDC/LLC/DRAM. See NIX_AF_LF()_RSS_BASE
                                                                 and NIX_AF_LF()_RSS_CFG. */
        uint64_t mce_log2bytes         : 4;  /**< [ 19: 16](RO) Receive multicast/mirror entry size as log2(bytes). Size of each
                                                                 NIX_RX_MCE_S structure in the AF multicast/mirror table NDC/LLC/DRAM. See
                                                                 NIX_AF_LF()_RX_MCAST_BASE and NIX_AF_LF()_RX_MCAST_CFG. */
        uint64_t qint_log2bytes        : 4;  /**< [ 23: 20](RO) Queue interrupt context size as log2(bytes). Size of each NIX_QINT_HW_S
                                                                 structure in a local function's queue interrupt context table NDC/LLC/DRAM.
                                                                 See NIX_AF_LF()_QINTS_BASE and NIX_AF_LF()_QINTS_CFG. */
        uint64_t cint_log2bytes        : 4;  /**< [ 27: 24](RO) Completion interrupt context size as log2(bytes). Size of each
                                                                 NIX_CINT_HW_S structure in a local function's completion interrupt context
                                                                 table NDC/LLC/DRAM. See NIX_AF_LF()_CINTS_BASE and NIX_AF_LF()_CINTS_CFG. */
        uint64_t dyno_log2bytes        : 4;  /**< [ 31: 28](RO) IPSEC dynamic ordering counter size as log2(bytes). Size of each entry in a
                                                                 local function's dynamic ordering (DYNO) counter table in NDC/LLC/DRAM. See
                                                                 NIX_AF_LF()_RX_IPSEC_DYNO_BASE and NIX_AF_LF()_RX_IPSEC_DYNO_CFG. */
        uint64_t dyno_array_log2counters : 4;/**< [ 35: 32](RO) IPSEC dynamic ordering counter array size as log2(counters). Size of
                                                                 counter array accessed by the NIX admin queue when NIX_AQ_INST_S[CTYPE] =
                                                                 NIX_AQ_CTYPE_E::DYNO. The size of the array in bytes is
                                                                 1 \<\< ([DYNO_ARRAY_LOG2COUNTERS] + [DYNO_LOG2BYTES]). */
        uint64_t reserved_36_63        : 28;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_const3_s cn; */
};
typedef union bdk_nixx_af_const3 bdk_nixx_af_const3_t;

static inline uint64_t BDK_NIXX_AF_CONST3(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_CONST3(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000038ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_CONST3", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_CONST3(a) bdk_nixx_af_const3_t
#define bustype_BDK_NIXX_AF_CONST3(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_CONST3(a) "NIXX_AF_CONST3"
#define device_bar_BDK_NIXX_AF_CONST3(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_CONST3(a) (a)
#define arguments_BDK_NIXX_AF_CONST3(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_cq_const
 *
 * NIX AF CQ Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_cq_const
{
    uint64_t u;
    struct bdk_nixx_af_cq_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_24_63        : 40;
        uint64_t queues_per_lf         : 24; /**< [ 23:  0](RO) Maximum number of completion queues per LF. */
#else /* Word 0 - Little Endian */
        uint64_t queues_per_lf         : 24; /**< [ 23:  0](RO) Maximum number of completion queues per LF. */
        uint64_t reserved_24_63        : 40;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_cq_const_s cn; */
};
typedef union bdk_nixx_af_cq_const bdk_nixx_af_cq_const_t;

static inline uint64_t BDK_NIXX_AF_CQ_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_CQ_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000048ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_CQ_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_CQ_CONST(a) bdk_nixx_af_cq_const_t
#define bustype_BDK_NIXX_AF_CQ_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_CQ_CONST(a) "NIXX_AF_CQ_CONST"
#define device_bar_BDK_NIXX_AF_CQ_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_CQ_CONST(a) (a)
#define arguments_BDK_NIXX_AF_CQ_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_debug_npc_resp_data#
 *
 * NIX AF NPC Response Data Registers
 * This register contains the captured NPC response.
 */
union bdk_nixx_af_debug_npc_resp_datax
{
    uint64_t u;
    struct bdk_nixx_af_debug_npc_resp_datax_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t data                  : 64; /**< [ 63:  0](RO/H) NPC Response words 0-7. */
#else /* Word 0 - Little Endian */
        uint64_t data                  : 64; /**< [ 63:  0](RO/H) NPC Response words 0-7. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_debug_npc_resp_datax_s cn; */
};
typedef union bdk_nixx_af_debug_npc_resp_datax bdk_nixx_af_debug_npc_resp_datax_t;

static inline uint64_t BDK_NIXX_AF_DEBUG_NPC_RESP_DATAX(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_DEBUG_NPC_RESP_DATAX(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=7)))
        return 0x850040000680ll + 0x10000000ll * ((a) & 0x0) + 8ll * ((b) & 0x7);
    __bdk_csr_fatal("NIXX_AF_DEBUG_NPC_RESP_DATAX", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_DEBUG_NPC_RESP_DATAX(a,b) bdk_nixx_af_debug_npc_resp_datax_t
#define bustype_BDK_NIXX_AF_DEBUG_NPC_RESP_DATAX(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_DEBUG_NPC_RESP_DATAX(a,b) "NIXX_AF_DEBUG_NPC_RESP_DATAX"
#define device_bar_BDK_NIXX_AF_DEBUG_NPC_RESP_DATAX(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_DEBUG_NPC_RESP_DATAX(a,b) (a)
#define arguments_BDK_NIXX_AF_DEBUG_NPC_RESP_DATAX(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_err_int
 *
 * NIX Admin Function Error Interrupt Register
 */
union bdk_nixx_af_err_int
{
    uint64_t u;
    struct bdk_nixx_af_err_int_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_15_63        : 49;
        uint64_t aq_inst_fault         : 1;  /**< [ 14: 14](R/W1C/H) Memory fault on NIX_AQ_INST_S read. Hardware also sets
                                                                 NIX_AF_AQ_STATUS[AQ_ERR]. */
        uint64_t aq_res_fault          : 1;  /**< [ 13: 13](R/W1C/H) Memory fault on NIX_AQ_RES_S write, or on read/write data following
                                                                 NIX_AQ_RES_S. Hardware also sets NIX_AF_AQ_STATUS[AQ_ERR]. */
        uint64_t aq_door_err           : 1;  /**< [ 12: 12](R/W1C/H) AQ doorbell error. See NIX_AF_AQ_DOOR[COUNT]. Hardware also sets
                                                                 NIX_AF_AQ_STATUS[AQ_ERR]. */
        uint64_t reserved_6_11         : 6;
        uint64_t rx_mce_list_err       : 1;  /**< [  5:  5](R/W1C/H) Receive multicast/mirror replication list error. One of the following (may
                                                                 not be exhaustive):
                                                                 * The length of a multicast or mirror replication list exceeds
                                                                 NIX_AF_RX_MCAST_CFG[MAX_LIST_LENM1]+1.
                                                                 * Invalid opcode in NIX_RX_MCE_S[OP].
                                                                 * NIX_RX_MCE_S[NEXT] pointer outside of table size specified by
                                                                 NIX_AF_RX_MCAST_CFG[SIZE]. */
        uint64_t rx_mce_fault          : 1;  /**< [  4:  4](R/W1C/H) Memory fault on NIX_RX_MCE_S read. */
        uint64_t rx_mirror_wqe_fault   : 1;  /**< [  3:  3](R/W1C/H) Memory fault on WQE read from a multicast buffer. */
        uint64_t rx_mcast_wqe_fault    : 1;  /**< [  2:  2](R/W1C/H) Memory fault on WQE read from a mirror buffer. */
        uint64_t rx_mirror_data_fault  : 1;  /**< [  1:  1](R/W1C/H) Memory fault on packet data or WQE write to a mirror buffer. */
        uint64_t rx_mcast_data_fault   : 1;  /**< [  0:  0](R/W1C/H) Memory fault on packet data or WQE write to a multicast buffer. */
#else /* Word 0 - Little Endian */
        uint64_t rx_mcast_data_fault   : 1;  /**< [  0:  0](R/W1C/H) Memory fault on packet data or WQE write to a multicast buffer. */
        uint64_t rx_mirror_data_fault  : 1;  /**< [  1:  1](R/W1C/H) Memory fault on packet data or WQE write to a mirror buffer. */
        uint64_t rx_mcast_wqe_fault    : 1;  /**< [  2:  2](R/W1C/H) Memory fault on WQE read from a mirror buffer. */
        uint64_t rx_mirror_wqe_fault   : 1;  /**< [  3:  3](R/W1C/H) Memory fault on WQE read from a multicast buffer. */
        uint64_t rx_mce_fault          : 1;  /**< [  4:  4](R/W1C/H) Memory fault on NIX_RX_MCE_S read. */
        uint64_t rx_mce_list_err       : 1;  /**< [  5:  5](R/W1C/H) Receive multicast/mirror replication list error. One of the following (may
                                                                 not be exhaustive):
                                                                 * The length of a multicast or mirror replication list exceeds
                                                                 NIX_AF_RX_MCAST_CFG[MAX_LIST_LENM1]+1.
                                                                 * Invalid opcode in NIX_RX_MCE_S[OP].
                                                                 * NIX_RX_MCE_S[NEXT] pointer outside of table size specified by
                                                                 NIX_AF_RX_MCAST_CFG[SIZE]. */
        uint64_t reserved_6_11         : 6;
        uint64_t aq_door_err           : 1;  /**< [ 12: 12](R/W1C/H) AQ doorbell error. See NIX_AF_AQ_DOOR[COUNT]. Hardware also sets
                                                                 NIX_AF_AQ_STATUS[AQ_ERR]. */
        uint64_t aq_res_fault          : 1;  /**< [ 13: 13](R/W1C/H) Memory fault on NIX_AQ_RES_S write, or on read/write data following
                                                                 NIX_AQ_RES_S. Hardware also sets NIX_AF_AQ_STATUS[AQ_ERR]. */
        uint64_t aq_inst_fault         : 1;  /**< [ 14: 14](R/W1C/H) Memory fault on NIX_AQ_INST_S read. Hardware also sets
                                                                 NIX_AF_AQ_STATUS[AQ_ERR]. */
        uint64_t reserved_15_63        : 49;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_err_int_s cn; */
};
typedef union bdk_nixx_af_err_int bdk_nixx_af_err_int_t;

static inline uint64_t BDK_NIXX_AF_ERR_INT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_ERR_INT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000180ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_ERR_INT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_ERR_INT(a) bdk_nixx_af_err_int_t
#define bustype_BDK_NIXX_AF_ERR_INT(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_ERR_INT(a) "NIXX_AF_ERR_INT"
#define device_bar_BDK_NIXX_AF_ERR_INT(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_ERR_INT(a) (a)
#define arguments_BDK_NIXX_AF_ERR_INT(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_err_int_ena_w1c
 *
 * NIX Admin Function Error Interrupt Enable Clear Register
 * This register clears interrupt enable bits.
 */
union bdk_nixx_af_err_int_ena_w1c
{
    uint64_t u;
    struct bdk_nixx_af_err_int_ena_w1c_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_15_63        : 49;
        uint64_t aq_inst_fault         : 1;  /**< [ 14: 14](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[AQ_INST_FAULT]. */
        uint64_t aq_res_fault          : 1;  /**< [ 13: 13](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[AQ_RES_FAULT]. */
        uint64_t aq_door_err           : 1;  /**< [ 12: 12](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[AQ_DOOR_ERR]. */
        uint64_t reserved_6_11         : 6;
        uint64_t rx_mce_list_err       : 1;  /**< [  5:  5](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MCE_LIST_ERR]. */
        uint64_t rx_mce_fault          : 1;  /**< [  4:  4](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MCE_FAULT]. */
        uint64_t rx_mirror_wqe_fault   : 1;  /**< [  3:  3](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MIRROR_WQE_FAULT]. */
        uint64_t rx_mcast_wqe_fault    : 1;  /**< [  2:  2](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MCAST_WQE_FAULT]. */
        uint64_t rx_mirror_data_fault  : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MIRROR_DATA_FAULT]. */
        uint64_t rx_mcast_data_fault   : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MCAST_DATA_FAULT]. */
#else /* Word 0 - Little Endian */
        uint64_t rx_mcast_data_fault   : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MCAST_DATA_FAULT]. */
        uint64_t rx_mirror_data_fault  : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MIRROR_DATA_FAULT]. */
        uint64_t rx_mcast_wqe_fault    : 1;  /**< [  2:  2](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MCAST_WQE_FAULT]. */
        uint64_t rx_mirror_wqe_fault   : 1;  /**< [  3:  3](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MIRROR_WQE_FAULT]. */
        uint64_t rx_mce_fault          : 1;  /**< [  4:  4](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MCE_FAULT]. */
        uint64_t rx_mce_list_err       : 1;  /**< [  5:  5](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[RX_MCE_LIST_ERR]. */
        uint64_t reserved_6_11         : 6;
        uint64_t aq_door_err           : 1;  /**< [ 12: 12](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[AQ_DOOR_ERR]. */
        uint64_t aq_res_fault          : 1;  /**< [ 13: 13](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[AQ_RES_FAULT]. */
        uint64_t aq_inst_fault         : 1;  /**< [ 14: 14](R/W1C/H) Reads or clears enable for NIX_AF_ERR_INT[AQ_INST_FAULT]. */
        uint64_t reserved_15_63        : 49;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_err_int_ena_w1c_s cn; */
};
typedef union bdk_nixx_af_err_int_ena_w1c bdk_nixx_af_err_int_ena_w1c_t;

static inline uint64_t BDK_NIXX_AF_ERR_INT_ENA_W1C(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_ERR_INT_ENA_W1C(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000198ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_ERR_INT_ENA_W1C", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_ERR_INT_ENA_W1C(a) bdk_nixx_af_err_int_ena_w1c_t
#define bustype_BDK_NIXX_AF_ERR_INT_ENA_W1C(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_ERR_INT_ENA_W1C(a) "NIXX_AF_ERR_INT_ENA_W1C"
#define device_bar_BDK_NIXX_AF_ERR_INT_ENA_W1C(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_ERR_INT_ENA_W1C(a) (a)
#define arguments_BDK_NIXX_AF_ERR_INT_ENA_W1C(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_err_int_ena_w1s
 *
 * NIX Admin Function Error Interrupt Enable Set Register
 * This register sets interrupt enable bits.
 */
union bdk_nixx_af_err_int_ena_w1s
{
    uint64_t u;
    struct bdk_nixx_af_err_int_ena_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_15_63        : 49;
        uint64_t aq_inst_fault         : 1;  /**< [ 14: 14](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[AQ_INST_FAULT]. */
        uint64_t aq_res_fault          : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[AQ_RES_FAULT]. */
        uint64_t aq_door_err           : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[AQ_DOOR_ERR]. */
        uint64_t reserved_6_11         : 6;
        uint64_t rx_mce_list_err       : 1;  /**< [  5:  5](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MCE_LIST_ERR]. */
        uint64_t rx_mce_fault          : 1;  /**< [  4:  4](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MCE_FAULT]. */
        uint64_t rx_mirror_wqe_fault   : 1;  /**< [  3:  3](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MIRROR_WQE_FAULT]. */
        uint64_t rx_mcast_wqe_fault    : 1;  /**< [  2:  2](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MCAST_WQE_FAULT]. */
        uint64_t rx_mirror_data_fault  : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MIRROR_DATA_FAULT]. */
        uint64_t rx_mcast_data_fault   : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MCAST_DATA_FAULT]. */
#else /* Word 0 - Little Endian */
        uint64_t rx_mcast_data_fault   : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MCAST_DATA_FAULT]. */
        uint64_t rx_mirror_data_fault  : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MIRROR_DATA_FAULT]. */
        uint64_t rx_mcast_wqe_fault    : 1;  /**< [  2:  2](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MCAST_WQE_FAULT]. */
        uint64_t rx_mirror_wqe_fault   : 1;  /**< [  3:  3](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MIRROR_WQE_FAULT]. */
        uint64_t rx_mce_fault          : 1;  /**< [  4:  4](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MCE_FAULT]. */
        uint64_t rx_mce_list_err       : 1;  /**< [  5:  5](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[RX_MCE_LIST_ERR]. */
        uint64_t reserved_6_11         : 6;
        uint64_t aq_door_err           : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[AQ_DOOR_ERR]. */
        uint64_t aq_res_fault          : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[AQ_RES_FAULT]. */
        uint64_t aq_inst_fault         : 1;  /**< [ 14: 14](R/W1S/H) Reads or sets enable for NIX_AF_ERR_INT[AQ_INST_FAULT]. */
        uint64_t reserved_15_63        : 49;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_err_int_ena_w1s_s cn; */
};
typedef union bdk_nixx_af_err_int_ena_w1s bdk_nixx_af_err_int_ena_w1s_t;

static inline uint64_t BDK_NIXX_AF_ERR_INT_ENA_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_ERR_INT_ENA_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000190ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_ERR_INT_ENA_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_ERR_INT_ENA_W1S(a) bdk_nixx_af_err_int_ena_w1s_t
#define bustype_BDK_NIXX_AF_ERR_INT_ENA_W1S(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_ERR_INT_ENA_W1S(a) "NIXX_AF_ERR_INT_ENA_W1S"
#define device_bar_BDK_NIXX_AF_ERR_INT_ENA_W1S(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_ERR_INT_ENA_W1S(a) (a)
#define arguments_BDK_NIXX_AF_ERR_INT_ENA_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_err_int_w1s
 *
 * NIX Admin Function Error Interrupt Set Register
 * This register sets interrupt bits.
 */
union bdk_nixx_af_err_int_w1s
{
    uint64_t u;
    struct bdk_nixx_af_err_int_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_15_63        : 49;
        uint64_t aq_inst_fault         : 1;  /**< [ 14: 14](R/W1S/H) Reads or sets NIX_AF_ERR_INT[AQ_INST_FAULT]. */
        uint64_t aq_res_fault          : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets NIX_AF_ERR_INT[AQ_RES_FAULT]. */
        uint64_t aq_door_err           : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets NIX_AF_ERR_INT[AQ_DOOR_ERR]. */
        uint64_t reserved_6_11         : 6;
        uint64_t rx_mce_list_err       : 1;  /**< [  5:  5](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MCE_LIST_ERR]. */
        uint64_t rx_mce_fault          : 1;  /**< [  4:  4](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MCE_FAULT]. */
        uint64_t rx_mirror_wqe_fault   : 1;  /**< [  3:  3](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MIRROR_WQE_FAULT]. */
        uint64_t rx_mcast_wqe_fault    : 1;  /**< [  2:  2](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MCAST_WQE_FAULT]. */
        uint64_t rx_mirror_data_fault  : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MIRROR_DATA_FAULT]. */
        uint64_t rx_mcast_data_fault   : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MCAST_DATA_FAULT]. */
#else /* Word 0 - Little Endian */
        uint64_t rx_mcast_data_fault   : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MCAST_DATA_FAULT]. */
        uint64_t rx_mirror_data_fault  : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MIRROR_DATA_FAULT]. */
        uint64_t rx_mcast_wqe_fault    : 1;  /**< [  2:  2](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MCAST_WQE_FAULT]. */
        uint64_t rx_mirror_wqe_fault   : 1;  /**< [  3:  3](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MIRROR_WQE_FAULT]. */
        uint64_t rx_mce_fault          : 1;  /**< [  4:  4](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MCE_FAULT]. */
        uint64_t rx_mce_list_err       : 1;  /**< [  5:  5](R/W1S/H) Reads or sets NIX_AF_ERR_INT[RX_MCE_LIST_ERR]. */
        uint64_t reserved_6_11         : 6;
        uint64_t aq_door_err           : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets NIX_AF_ERR_INT[AQ_DOOR_ERR]. */
        uint64_t aq_res_fault          : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets NIX_AF_ERR_INT[AQ_RES_FAULT]. */
        uint64_t aq_inst_fault         : 1;  /**< [ 14: 14](R/W1S/H) Reads or sets NIX_AF_ERR_INT[AQ_INST_FAULT]. */
        uint64_t reserved_15_63        : 49;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_err_int_w1s_s cn; */
};
typedef union bdk_nixx_af_err_int_w1s bdk_nixx_af_err_int_w1s_t;

static inline uint64_t BDK_NIXX_AF_ERR_INT_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_ERR_INT_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000188ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_ERR_INT_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_ERR_INT_W1S(a) bdk_nixx_af_err_int_w1s_t
#define bustype_BDK_NIXX_AF_ERR_INT_W1S(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_ERR_INT_W1S(a) "NIXX_AF_ERR_INT_W1S"
#define device_bar_BDK_NIXX_AF_ERR_INT_W1S(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_ERR_INT_W1S(a) (a)
#define arguments_BDK_NIXX_AF_ERR_INT_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_expr_tx_fifo_status
 *
 * NIX AF Express Transmit FIFO Status Register
 * Status of FIFO which transmits express packets to CGX and LBK.
 */
union bdk_nixx_af_expr_tx_fifo_status
{
    uint64_t u;
    struct bdk_nixx_af_expr_tx_fifo_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_12_63        : 52;
        uint64_t count                 : 12; /**< [ 11:  0](RO/H) Number of 128-bit entries in the TX FIFO. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 12; /**< [ 11:  0](RO/H) Number of 128-bit entries in the TX FIFO. */
        uint64_t reserved_12_63        : 52;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_expr_tx_fifo_status_s cn; */
};
typedef union bdk_nixx_af_expr_tx_fifo_status bdk_nixx_af_expr_tx_fifo_status_t;

static inline uint64_t BDK_NIXX_AF_EXPR_TX_FIFO_STATUS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_EXPR_TX_FIFO_STATUS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000630ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_EXPR_TX_FIFO_STATUS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_EXPR_TX_FIFO_STATUS(a) bdk_nixx_af_expr_tx_fifo_status_t
#define bustype_BDK_NIXX_AF_EXPR_TX_FIFO_STATUS(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_EXPR_TX_FIFO_STATUS(a) "NIXX_AF_EXPR_TX_FIFO_STATUS"
#define device_bar_BDK_NIXX_AF_EXPR_TX_FIFO_STATUS(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_EXPR_TX_FIFO_STATUS(a) (a)
#define arguments_BDK_NIXX_AF_EXPR_TX_FIFO_STATUS(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_gen_int
 *
 * NIX AF General Interrupt Register
 */
union bdk_nixx_af_gen_int
{
    uint64_t u;
    struct bdk_nixx_af_gen_int_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_5_63         : 59;
        uint64_t smq_flush_done        : 1;  /**< [  4:  4](R/W1C/H) SMQ flush done. Set when an SMQ flush operation initiated by
                                                                 NIX_AF_SMQ()_CFG[FLUSH] is complete. */
        uint64_t tl1_drain             : 1;  /**< [  3:  3](R/W1C/H) Set when a NIX_AF_TL*()_SW_XOFF[DRAIN,DRAIN_IRQ] command reaches the TL1
                                                                 level. */
        uint64_t reserved_2            : 1;
        uint64_t rx_mirror_drop        : 1;  /**< [  1:  1](R/W1C/H) Receive mirror packet dropped due to insufficient space in the RX mirror
                                                                 buffer specified by NIX_AF_RX_MIRROR_BUF_BASE and NIX_AF_RX_MIRROR_BUF_CFG. */
        uint64_t rx_mcast_drop         : 1;  /**< [  0:  0](R/W1C/H) Receive multicast packet dropped due to insufficient space in the RX
                                                                 multicast buffer specified by NIX_AF_RX_MCAST_BUF_BASE and
                                                                 NIX_AF_RX_MCAST_BUF_CFG. */
#else /* Word 0 - Little Endian */
        uint64_t rx_mcast_drop         : 1;  /**< [  0:  0](R/W1C/H) Receive multicast packet dropped due to insufficient space in the RX
                                                                 multicast buffer specified by NIX_AF_RX_MCAST_BUF_BASE and
                                                                 NIX_AF_RX_MCAST_BUF_CFG. */
        uint64_t rx_mirror_drop        : 1;  /**< [  1:  1](R/W1C/H) Receive mirror packet dropped due to insufficient space in the RX mirror
                                                                 buffer specified by NIX_AF_RX_MIRROR_BUF_BASE and NIX_AF_RX_MIRROR_BUF_CFG. */
        uint64_t reserved_2            : 1;
        uint64_t tl1_drain             : 1;  /**< [  3:  3](R/W1C/H) Set when a NIX_AF_TL*()_SW_XOFF[DRAIN,DRAIN_IRQ] command reaches the TL1
                                                                 level. */
        uint64_t smq_flush_done        : 1;  /**< [  4:  4](R/W1C/H) SMQ flush done. Set when an SMQ flush operation initiated by
                                                                 NIX_AF_SMQ()_CFG[FLUSH] is complete. */
        uint64_t reserved_5_63         : 59;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_gen_int_s cn; */
};
typedef union bdk_nixx_af_gen_int bdk_nixx_af_gen_int_t;

static inline uint64_t BDK_NIXX_AF_GEN_INT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_GEN_INT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000160ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_GEN_INT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_GEN_INT(a) bdk_nixx_af_gen_int_t
#define bustype_BDK_NIXX_AF_GEN_INT(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_GEN_INT(a) "NIXX_AF_GEN_INT"
#define device_bar_BDK_NIXX_AF_GEN_INT(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_GEN_INT(a) (a)
#define arguments_BDK_NIXX_AF_GEN_INT(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_gen_int_ena_w1c
 *
 * NIX AF General Interrupt Enable Clear Register
 * This register clears interrupt enable bits.
 */
union bdk_nixx_af_gen_int_ena_w1c
{
    uint64_t u;
    struct bdk_nixx_af_gen_int_ena_w1c_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_5_63         : 59;
        uint64_t smq_flush_done        : 1;  /**< [  4:  4](R/W1C/H) Reads or clears enable for NIX_AF_GEN_INT[SMQ_FLUSH_DONE]. */
        uint64_t tl1_drain             : 1;  /**< [  3:  3](R/W1C/H) Reads or clears enable for NIX_AF_GEN_INT[TL1_DRAIN]. */
        uint64_t reserved_2            : 1;
        uint64_t rx_mirror_drop        : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_AF_GEN_INT[RX_MIRROR_DROP]. */
        uint64_t rx_mcast_drop         : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_AF_GEN_INT[RX_MCAST_DROP]. */
#else /* Word 0 - Little Endian */
        uint64_t rx_mcast_drop         : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_AF_GEN_INT[RX_MCAST_DROP]. */
        uint64_t rx_mirror_drop        : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_AF_GEN_INT[RX_MIRROR_DROP]. */
        uint64_t reserved_2            : 1;
        uint64_t tl1_drain             : 1;  /**< [  3:  3](R/W1C/H) Reads or clears enable for NIX_AF_GEN_INT[TL1_DRAIN]. */
        uint64_t smq_flush_done        : 1;  /**< [  4:  4](R/W1C/H) Reads or clears enable for NIX_AF_GEN_INT[SMQ_FLUSH_DONE]. */
        uint64_t reserved_5_63         : 59;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_gen_int_ena_w1c_s cn; */
};
typedef union bdk_nixx_af_gen_int_ena_w1c bdk_nixx_af_gen_int_ena_w1c_t;

static inline uint64_t BDK_NIXX_AF_GEN_INT_ENA_W1C(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_GEN_INT_ENA_W1C(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000178ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_GEN_INT_ENA_W1C", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_GEN_INT_ENA_W1C(a) bdk_nixx_af_gen_int_ena_w1c_t
#define bustype_BDK_NIXX_AF_GEN_INT_ENA_W1C(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_GEN_INT_ENA_W1C(a) "NIXX_AF_GEN_INT_ENA_W1C"
#define device_bar_BDK_NIXX_AF_GEN_INT_ENA_W1C(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_GEN_INT_ENA_W1C(a) (a)
#define arguments_BDK_NIXX_AF_GEN_INT_ENA_W1C(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_gen_int_ena_w1s
 *
 * NIX AF General Interrupt Enable Set Register
 * This register sets interrupt enable bits.
 */
union bdk_nixx_af_gen_int_ena_w1s
{
    uint64_t u;
    struct bdk_nixx_af_gen_int_ena_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_5_63         : 59;
        uint64_t smq_flush_done        : 1;  /**< [  4:  4](R/W1S/H) Reads or sets enable for NIX_AF_GEN_INT[SMQ_FLUSH_DONE]. */
        uint64_t tl1_drain             : 1;  /**< [  3:  3](R/W1S/H) Reads or sets enable for NIX_AF_GEN_INT[TL1_DRAIN]. */
        uint64_t reserved_2            : 1;
        uint64_t rx_mirror_drop        : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_AF_GEN_INT[RX_MIRROR_DROP]. */
        uint64_t rx_mcast_drop         : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_AF_GEN_INT[RX_MCAST_DROP]. */
#else /* Word 0 - Little Endian */
        uint64_t rx_mcast_drop         : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_AF_GEN_INT[RX_MCAST_DROP]. */
        uint64_t rx_mirror_drop        : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_AF_GEN_INT[RX_MIRROR_DROP]. */
        uint64_t reserved_2            : 1;
        uint64_t tl1_drain             : 1;  /**< [  3:  3](R/W1S/H) Reads or sets enable for NIX_AF_GEN_INT[TL1_DRAIN]. */
        uint64_t smq_flush_done        : 1;  /**< [  4:  4](R/W1S/H) Reads or sets enable for NIX_AF_GEN_INT[SMQ_FLUSH_DONE]. */
        uint64_t reserved_5_63         : 59;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_gen_int_ena_w1s_s cn; */
};
typedef union bdk_nixx_af_gen_int_ena_w1s bdk_nixx_af_gen_int_ena_w1s_t;

static inline uint64_t BDK_NIXX_AF_GEN_INT_ENA_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_GEN_INT_ENA_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000170ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_GEN_INT_ENA_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_GEN_INT_ENA_W1S(a) bdk_nixx_af_gen_int_ena_w1s_t
#define bustype_BDK_NIXX_AF_GEN_INT_ENA_W1S(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_GEN_INT_ENA_W1S(a) "NIXX_AF_GEN_INT_ENA_W1S"
#define device_bar_BDK_NIXX_AF_GEN_INT_ENA_W1S(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_GEN_INT_ENA_W1S(a) (a)
#define arguments_BDK_NIXX_AF_GEN_INT_ENA_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_gen_int_w1s
 *
 * NIX AF General Interrupt Set Register
 * This register sets interrupt bits.
 */
union bdk_nixx_af_gen_int_w1s
{
    uint64_t u;
    struct bdk_nixx_af_gen_int_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_5_63         : 59;
        uint64_t smq_flush_done        : 1;  /**< [  4:  4](R/W1S/H) Reads or sets NIX_AF_GEN_INT[SMQ_FLUSH_DONE]. */
        uint64_t tl1_drain             : 1;  /**< [  3:  3](R/W1S/H) Reads or sets NIX_AF_GEN_INT[TL1_DRAIN]. */
        uint64_t reserved_2            : 1;
        uint64_t rx_mirror_drop        : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_AF_GEN_INT[RX_MIRROR_DROP]. */
        uint64_t rx_mcast_drop         : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_AF_GEN_INT[RX_MCAST_DROP]. */
#else /* Word 0 - Little Endian */
        uint64_t rx_mcast_drop         : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_AF_GEN_INT[RX_MCAST_DROP]. */
        uint64_t rx_mirror_drop        : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_AF_GEN_INT[RX_MIRROR_DROP]. */
        uint64_t reserved_2            : 1;
        uint64_t tl1_drain             : 1;  /**< [  3:  3](R/W1S/H) Reads or sets NIX_AF_GEN_INT[TL1_DRAIN]. */
        uint64_t smq_flush_done        : 1;  /**< [  4:  4](R/W1S/H) Reads or sets NIX_AF_GEN_INT[SMQ_FLUSH_DONE]. */
        uint64_t reserved_5_63         : 59;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_gen_int_w1s_s cn; */
};
typedef union bdk_nixx_af_gen_int_w1s bdk_nixx_af_gen_int_w1s_t;

static inline uint64_t BDK_NIXX_AF_GEN_INT_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_GEN_INT_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000168ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_GEN_INT_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_GEN_INT_W1S(a) bdk_nixx_af_gen_int_w1s_t
#define bustype_BDK_NIXX_AF_GEN_INT_W1S(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_GEN_INT_W1S(a) "NIXX_AF_GEN_INT_W1S"
#define device_bar_BDK_NIXX_AF_GEN_INT_W1S(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_GEN_INT_W1S(a) (a)
#define arguments_BDK_NIXX_AF_GEN_INT_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_cfg
 *
 * NIX AF Local Function Configuration Registers
 */
union bdk_nixx_af_lfx_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_35_63        : 29;
        uint64_t xqe_size              : 2;  /**< [ 34: 33](R/W) Selects the WQE/CQE size for the LF. Enumerated by NIX_XQESZ_E. */
        uint64_t be                    : 1;  /**< [ 32: 32](R/W) LF big-endian select:
                                                                   0 = Little-endian. All data structures are in byte invariant little-endian
                                                                 format (LE8) with the following ordering within each 64-bit word: \<7:0\> at byte
                                                                 address 0, \<15:8\> at address 1, ..., \<63:56\> at address 0x7.

                                                                   1 = Big-endian. All data structures are in byte invariant big-endian format
                                                                 (BE8) with the following ordering within each 64-bit word: \<63:56\> at byte
                                                                 address 0, \<55:48\> at address 1, ..., \<7:0\> at address 0x7.

                                                                 The affected data structures are:
                                                                 * All send subdescriptors (NIX_SEND_*_S) enqueued with
                                                                 NIX_LF_OP_SEND() and pointed to by NIX_SEND_JUMP_S.
                                                                 * All WQEs/CQEs.

                                                                 Regardless of this setting:
                                                                 * CSRs (excluding NIX_LF_OP_SEND()) are always little endian.
                                                                 * Packet data either pointed-to or in NIX structures (e.g. NIX_SEND_SG_S, NIX_SEND_IMM_S,
                                                                 NIX_RX_SG_S) are byte-invariant and endian agnostic. */
        uint64_t sso_pf_func           : 16; /**< [ 31: 16](R/W) SSO PF and function to which ADD_WORK submissions are sent. Format
                                                                 specified by RVU_PF_FUNC_S. */
        uint64_t npa_pf_func           : 16; /**< [ 15:  0](R/W) NPA PF and function whose auras are used to allocate and free buffers.
                                                                 Format specified by RVU_PF_FUNC_S. */
#else /* Word 0 - Little Endian */
        uint64_t npa_pf_func           : 16; /**< [ 15:  0](R/W) NPA PF and function whose auras are used to allocate and free buffers.
                                                                 Format specified by RVU_PF_FUNC_S. */
        uint64_t sso_pf_func           : 16; /**< [ 31: 16](R/W) SSO PF and function to which ADD_WORK submissions are sent. Format
                                                                 specified by RVU_PF_FUNC_S. */
        uint64_t be                    : 1;  /**< [ 32: 32](R/W) LF big-endian select:
                                                                   0 = Little-endian. All data structures are in byte invariant little-endian
                                                                 format (LE8) with the following ordering within each 64-bit word: \<7:0\> at byte
                                                                 address 0, \<15:8\> at address 1, ..., \<63:56\> at address 0x7.

                                                                   1 = Big-endian. All data structures are in byte invariant big-endian format
                                                                 (BE8) with the following ordering within each 64-bit word: \<63:56\> at byte
                                                                 address 0, \<55:48\> at address 1, ..., \<7:0\> at address 0x7.

                                                                 The affected data structures are:
                                                                 * All send subdescriptors (NIX_SEND_*_S) enqueued with
                                                                 NIX_LF_OP_SEND() and pointed to by NIX_SEND_JUMP_S.
                                                                 * All WQEs/CQEs.

                                                                 Regardless of this setting:
                                                                 * CSRs (excluding NIX_LF_OP_SEND()) are always little endian.
                                                                 * Packet data either pointed-to or in NIX structures (e.g. NIX_SEND_SG_S, NIX_SEND_IMM_S,
                                                                 NIX_RX_SG_S) are byte-invariant and endian agnostic. */
        uint64_t xqe_size              : 2;  /**< [ 34: 33](R/W) Selects the WQE/CQE size for the LF. Enumerated by NIX_XQESZ_E. */
        uint64_t reserved_35_63        : 29;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_cfg bdk_nixx_af_lfx_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004000ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_CFG(a,b) bdk_nixx_af_lfx_cfg_t
#define bustype_BDK_NIXX_AF_LFX_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_CFG(a,b) "NIXX_AF_LFX_CFG"
#define device_bar_BDK_NIXX_AF_LFX_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_cints_base
 *
 * NIX AF Local Function Completion Interrupts Base Address Registers
 * This register specifies the base RVU PF(0) IOVA of LF's completion interrupt
 * context table in NDC/LLC/DRAM. The table consists of NIX_AF_CONST2[CINTS]
 * contiguous NIX_CINT_HW_S structures.
 */
union bdk_nixx_af_lfx_cints_base
{
    uint64_t u;
    struct bdk_nixx_af_lfx_cints_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_cints_base_s cn; */
};
typedef union bdk_nixx_af_lfx_cints_base bdk_nixx_af_lfx_cints_base_t;

static inline uint64_t BDK_NIXX_AF_LFX_CINTS_BASE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_CINTS_BASE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004130ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_CINTS_BASE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_CINTS_BASE(a,b) bdk_nixx_af_lfx_cints_base_t
#define bustype_BDK_NIXX_AF_LFX_CINTS_BASE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_CINTS_BASE(a,b) "NIXX_AF_LFX_CINTS_BASE"
#define device_bar_BDK_NIXX_AF_LFX_CINTS_BASE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_CINTS_BASE(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_CINTS_BASE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_cints_cfg
 *
 * NIX AF Local Function Completion Interrupts Configuration Registers
 * This register controls access to the LF's completion interrupt context table in
 * LLC/DRAM. The table consists of NIX_AF_CONST2[CINTS] contiguous NIX_CINT_HW_S
 * structures. The size of each structure is 1 \<\< NIX_AF_CONST3[CINT_LOG2BYTES]
 * bytes.
 */
union bdk_nixx_af_lfx_cints_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_cints_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_37_63        : 27;
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing context structures in
                                                                 LLC/DRAM:
                                                                 0 = Writes and reads of context data will not allocate into the LLC.
                                                                 1 = Writes and reads of context data are allocated into the LLC. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating context structures in NDC (1 means do
                                                                 not use). All ones disables allocation in NDC. */
        uint64_t reserved_0_19         : 20;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_19         : 20;
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating context structures in NDC (1 means do
                                                                 not use). All ones disables allocation in NDC. */
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing context structures in
                                                                 LLC/DRAM:
                                                                 0 = Writes and reads of context data will not allocate into the LLC.
                                                                 1 = Writes and reads of context data are allocated into the LLC. */
        uint64_t reserved_37_63        : 27;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_cints_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_cints_cfg bdk_nixx_af_lfx_cints_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_CINTS_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_CINTS_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004120ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_CINTS_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_CINTS_CFG(a,b) bdk_nixx_af_lfx_cints_cfg_t
#define bustype_BDK_NIXX_AF_LFX_CINTS_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_CINTS_CFG(a,b) "NIXX_AF_LFX_CINTS_CFG"
#define device_bar_BDK_NIXX_AF_LFX_CINTS_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_CINTS_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_CINTS_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_cqs_base
 *
 * NIX AF Local Function Completion Queues Base Address Register
 * This register specifies the base RVU PF(0) IOVA of the LF's CQ context table.
 * The table consists of NIX_AF_LF()_CQS_CFG[MAX_QUEUESM1]+1 contiguous
 * NIX_CQ_CTX_S structures.
 */
union bdk_nixx_af_lfx_cqs_base
{
    uint64_t u;
    struct bdk_nixx_af_lfx_cqs_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_cqs_base_s cn; */
};
typedef union bdk_nixx_af_lfx_cqs_base bdk_nixx_af_lfx_cqs_base_t;

static inline uint64_t BDK_NIXX_AF_LFX_CQS_BASE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_CQS_BASE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004070ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_CQS_BASE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_CQS_BASE(a,b) bdk_nixx_af_lfx_cqs_base_t
#define bustype_BDK_NIXX_AF_LFX_CQS_BASE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_CQS_BASE(a,b) "NIXX_AF_LFX_CQS_BASE"
#define device_bar_BDK_NIXX_AF_LFX_CQS_BASE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_CQS_BASE(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_CQS_BASE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_cqs_cfg
 *
 * NIX AF Local Function Completion Queues Configuration Register
 * This register configures completion queues in the LF.
 */
union bdk_nixx_af_lfx_cqs_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_cqs_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_37_63        : 27;
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing queue context structures
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of context data will not allocate into the LLC.
                                                                 1 = Writes and reads of context data are allocated into the LLC. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating queue context structures in NDC (1
                                                                 means do not use). All ones disables allocation in NDC. */
        uint64_t max_queuesm1          : 20; /**< [ 19:  0](R/W) Maximum number of queues minus one. */
#else /* Word 0 - Little Endian */
        uint64_t max_queuesm1          : 20; /**< [ 19:  0](R/W) Maximum number of queues minus one. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating queue context structures in NDC (1
                                                                 means do not use). All ones disables allocation in NDC. */
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing queue context structures
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of context data will not allocate into the LLC.
                                                                 1 = Writes and reads of context data are allocated into the LLC. */
        uint64_t reserved_37_63        : 27;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_cqs_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_cqs_cfg bdk_nixx_af_lfx_cqs_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_CQS_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_CQS_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004060ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_CQS_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_CQS_CFG(a,b) bdk_nixx_af_lfx_cqs_cfg_t
#define bustype_BDK_NIXX_AF_LFX_CQS_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_CQS_CFG(a,b) "NIXX_AF_LFX_CQS_CFG"
#define device_bar_BDK_NIXX_AF_LFX_CQS_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_CQS_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_CQS_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_lock#
 *
 * NIX AF Local Function Lockdown Registers
 * Internal:
 * The NIX lockdown depth of 32 bytes is shallow compared to 96 bytes for NIC and meant for outer
 * MAC and/or VLAN (optionally preceded by a small number of skip bytes). NPC's MCAM can be used
 * for deeper protocol-aware lockdown.
 */
union bdk_nixx_af_lfx_lockx
{
    uint64_t u;
    struct bdk_nixx_af_lfx_lockx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t bit_ena               : 32; /**< [ 63: 32](R/W) Lockdown bit enable. Each set bit indicates that the transmitted packet's corresponding
                                                                 bit number will be compared against [DATA]. */
        uint64_t data                  : 32; /**< [ 31:  0](R/W) Lockdown data. If corresponding [BIT_ENA] is set and
                                                                 NIX_AF_LF()_TX_CFG[LOCK_ENA] is set, outbound packet data must match the
                                                                 [DATA] bit or the packet will be dropped. Bytes are numbered in little
                                                                 endian form, with byte 0 the first byte onto the wire:
                                                                 _ If LOCK(0)[BIT_ENA]\<7:0\> set, LOCK(0)[DATA]\<7:0\> = packet byte 0.
                                                                 _ If LOCK(0)[BIT_ENA]\<15:8\> set, LOCK(0)[DATA]\<15:8\> = packet byte 1.
                                                                 _ If LOCK(0)[BIT_ENA]\<23:6\> set, LOCK(0)[DATA]\<23:16\> = packet byte 2.
                                                                 _ ...
                                                                 _ If LOCK(1)[BIT_ENA]\<7:0\> set, LOCK(1)[DATA]\<7:0\> = packet byte 4.
                                                                 _ ...
                                                                 _ If LOCK(7)[BIT_ENA]\<31:24\> set, LOCK(7)[DATA]\<31:24\> = packet byte 31.

                                                                 Lockdown data is checked after all packet modifications by hardware other
                                                                 than checksum/CRC updates. These modifications include potential VLAN
                                                                 insertion, packet shaper marking, LSO modifications and Vtag
                                                                 insertion/replacement.

                                                                 In addition, if any checksum/CRC bits updated by NIX_SEND_HDR_S[CKL*]
                                                                 and/or NIX_SEND_CRC_S are locked down, a lockdown violation is detected and
                                                                 the packet is dropped. */
#else /* Word 0 - Little Endian */
        uint64_t data                  : 32; /**< [ 31:  0](R/W) Lockdown data. If corresponding [BIT_ENA] is set and
                                                                 NIX_AF_LF()_TX_CFG[LOCK_ENA] is set, outbound packet data must match the
                                                                 [DATA] bit or the packet will be dropped. Bytes are numbered in little
                                                                 endian form, with byte 0 the first byte onto the wire:
                                                                 _ If LOCK(0)[BIT_ENA]\<7:0\> set, LOCK(0)[DATA]\<7:0\> = packet byte 0.
                                                                 _ If LOCK(0)[BIT_ENA]\<15:8\> set, LOCK(0)[DATA]\<15:8\> = packet byte 1.
                                                                 _ If LOCK(0)[BIT_ENA]\<23:6\> set, LOCK(0)[DATA]\<23:16\> = packet byte 2.
                                                                 _ ...
                                                                 _ If LOCK(1)[BIT_ENA]\<7:0\> set, LOCK(1)[DATA]\<7:0\> = packet byte 4.
                                                                 _ ...
                                                                 _ If LOCK(7)[BIT_ENA]\<31:24\> set, LOCK(7)[DATA]\<31:24\> = packet byte 31.

                                                                 Lockdown data is checked after all packet modifications by hardware other
                                                                 than checksum/CRC updates. These modifications include potential VLAN
                                                                 insertion, packet shaper marking, LSO modifications and Vtag
                                                                 insertion/replacement.

                                                                 In addition, if any checksum/CRC bits updated by NIX_SEND_HDR_S[CKL*]
                                                                 and/or NIX_SEND_CRC_S are locked down, a lockdown violation is detected and
                                                                 the packet is dropped. */
        uint64_t bit_ena               : 32; /**< [ 63: 32](R/W) Lockdown bit enable. Each set bit indicates that the transmitted packet's corresponding
                                                                 bit number will be compared against [DATA]. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_lockx_s cn; */
};
typedef union bdk_nixx_af_lfx_lockx bdk_nixx_af_lfx_lockx_t;

static inline uint64_t BDK_NIXX_AF_LFX_LOCKX(unsigned long a, unsigned long b, unsigned long c) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_LOCKX(unsigned long a, unsigned long b, unsigned long c)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127) && (c<=7)))
        return 0x850040004300ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f) + 8ll * ((c) & 0x7);
    __bdk_csr_fatal("NIXX_AF_LFX_LOCKX", 3, a, b, c, 0);
}

#define typedef_BDK_NIXX_AF_LFX_LOCKX(a,b,c) bdk_nixx_af_lfx_lockx_t
#define bustype_BDK_NIXX_AF_LFX_LOCKX(a,b,c) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_LOCKX(a,b,c) "NIXX_AF_LFX_LOCKX"
#define device_bar_BDK_NIXX_AF_LFX_LOCKX(a,b,c) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_LOCKX(a,b,c) (a)
#define arguments_BDK_NIXX_AF_LFX_LOCKX(a,b,c) (a),(b),(c),-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_qints_base
 *
 * NIX AF Local Function Queue Interrupts Base Address Registers
 * This register specifies the base RVU PF(0) IOVA of LF's queue interrupt context
 * table in NDC/LLC/DRAM. The table consists of NIX_AF_CONST2[QINTS] contiguous
 * NIX_QINT_HW_S structures.
 */
union bdk_nixx_af_lfx_qints_base
{
    uint64_t u;
    struct bdk_nixx_af_lfx_qints_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_qints_base_s cn; */
};
typedef union bdk_nixx_af_lfx_qints_base bdk_nixx_af_lfx_qints_base_t;

static inline uint64_t BDK_NIXX_AF_LFX_QINTS_BASE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_QINTS_BASE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004110ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_QINTS_BASE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_QINTS_BASE(a,b) bdk_nixx_af_lfx_qints_base_t
#define bustype_BDK_NIXX_AF_LFX_QINTS_BASE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_QINTS_BASE(a,b) "NIXX_AF_LFX_QINTS_BASE"
#define device_bar_BDK_NIXX_AF_LFX_QINTS_BASE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_QINTS_BASE(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_QINTS_BASE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_qints_cfg
 *
 * NIX AF Local Function Queue Interrupts Configuration Registers
 * This register controls access to the LF's queue interrupt context table in
 * LLC/DRAM. The table consists of NIX_AF_CONST2[QINTS] contiguous NIX_QINT_HW_S
 * structures. The size of each structure is 1 \<\< NIX_AF_CONST3[QINT_LOG2BYTES]
 * bytes.
 */
union bdk_nixx_af_lfx_qints_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_qints_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_37_63        : 27;
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing context structures in
                                                                 LLC/DRAM:
                                                                 0 = Writes and reads of context data will not allocate into the LLC.
                                                                 1 = Writes and reads of context data are allocated into the LLC. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating context structures in NDC (1 means do
                                                                 not use). All ones disables allocation in NDC. */
        uint64_t reserved_0_19         : 20;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_19         : 20;
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating context structures in NDC (1 means do
                                                                 not use). All ones disables allocation in NDC. */
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing context structures in
                                                                 LLC/DRAM:
                                                                 0 = Writes and reads of context data will not allocate into the LLC.
                                                                 1 = Writes and reads of context data are allocated into the LLC. */
        uint64_t reserved_37_63        : 27;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_qints_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_qints_cfg bdk_nixx_af_lfx_qints_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_QINTS_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_QINTS_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004100ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_QINTS_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_QINTS_CFG(a,b) bdk_nixx_af_lfx_qints_cfg_t
#define bustype_BDK_NIXX_AF_LFX_QINTS_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_QINTS_CFG(a,b) "NIXX_AF_LFX_QINTS_CFG"
#define device_bar_BDK_NIXX_AF_LFX_QINTS_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_QINTS_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_QINTS_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rqs_base
 *
 * NIX AF Local Function Receive Queues Base Address Register
 * This register specifies the base RVU PF(0) IOVA of the LF's RQ context table.
 * The table consists of NIX_AF_LF()_RQS_CFG[MAX_QUEUESM1]+1 contiguous
 * NIX_RQ_CTX_S structures.
 */
union bdk_nixx_af_lfx_rqs_base
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rqs_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rqs_base_s cn; */
};
typedef union bdk_nixx_af_lfx_rqs_base bdk_nixx_af_lfx_rqs_base_t;

static inline uint64_t BDK_NIXX_AF_LFX_RQS_BASE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RQS_BASE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004050ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_RQS_BASE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RQS_BASE(a,b) bdk_nixx_af_lfx_rqs_base_t
#define bustype_BDK_NIXX_AF_LFX_RQS_BASE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RQS_BASE(a,b) "NIXX_AF_LFX_RQS_BASE"
#define device_bar_BDK_NIXX_AF_LFX_RQS_BASE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RQS_BASE(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_RQS_BASE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rqs_cfg
 *
 * NIX AF Local Function Receive Queues Configuration Register
 * This register configures receive queues in the LF.
 */
union bdk_nixx_af_lfx_rqs_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rqs_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_37_63        : 27;
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing queue context structures
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of context data will not allocate into the LLC.
                                                                 1 = Writes and reads of context data are allocated into the LLC. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating queue context structures in NDC (1
                                                                 means do not use). All ones disables allocation in NDC. */
        uint64_t max_queuesm1          : 20; /**< [ 19:  0](R/W) Maximum number of queues minus one. */
#else /* Word 0 - Little Endian */
        uint64_t max_queuesm1          : 20; /**< [ 19:  0](R/W) Maximum number of queues minus one. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating queue context structures in NDC (1
                                                                 means do not use). All ones disables allocation in NDC. */
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing queue context structures
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of context data will not allocate into the LLC.
                                                                 1 = Writes and reads of context data are allocated into the LLC. */
        uint64_t reserved_37_63        : 27;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rqs_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_rqs_cfg bdk_nixx_af_lfx_rqs_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_RQS_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RQS_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004040ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_RQS_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RQS_CFG(a,b) bdk_nixx_af_lfx_rqs_cfg_t
#define bustype_BDK_NIXX_AF_LFX_RQS_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RQS_CFG(a,b) "NIXX_AF_LFX_RQS_CFG"
#define device_bar_BDK_NIXX_AF_LFX_RQS_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RQS_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_RQS_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rss_base
 *
 * NIX AF Local Function Receive Size Scaling Table Base Address Register
 * This register specifies the base RVU PF(0) IOVA of the RSS table per LF. The
 * table is present when NIX_AF_LF()_RSS_CFG[ENA] is set and consists of
 * 2^(NIX_AF_LF()_RSS_CFG[SIZE]+8) contiguous NIX_RSSE_S structures, where the
 * size of each structure is 1 \<\< NIX_AF_CONST3[RSSE_LOG2BYTES] bytes.
 * See NIX_AF_LF()_RSS_GRP().
 */
union bdk_nixx_af_lfx_rss_base
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rss_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rss_base_s cn; */
};
typedef union bdk_nixx_af_lfx_rss_base bdk_nixx_af_lfx_rss_base_t;

static inline uint64_t BDK_NIXX_AF_LFX_RSS_BASE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RSS_BASE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x8500400040d0ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_RSS_BASE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RSS_BASE(a,b) bdk_nixx_af_lfx_rss_base_t
#define bustype_BDK_NIXX_AF_LFX_RSS_BASE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RSS_BASE(a,b) "NIXX_AF_LFX_RSS_BASE"
#define device_bar_BDK_NIXX_AF_LFX_RSS_BASE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RSS_BASE(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_RSS_BASE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rss_cfg
 *
 * NIX AF Local Function Receive Size Scaling Table Configuration Register
 * See NIX_AF_LF()_RSS_BASE and NIX_AF_LF()_RSS_GRP().
 */
union bdk_nixx_af_lfx_rss_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rss_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_37_63        : 27;
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing NIX_RSSE_S structures
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of NIX_RSSE_S data will not allocate into the LLC.
                                                                 1 = Writes and reads of NIX_RSSE_S data are allocated into the LLC. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating NIX_RSSE_S structures in NDC (1 means
                                                                 do not use). All ones disables allocation in NDC. */
        uint64_t reserved_5_19         : 15;
        uint64_t ena                   : 1;  /**< [  4:  4](R/W) RSS is enabled for the LF. */
        uint64_t size                  : 4;  /**< [  3:  0](R/W) Specifies table size in NIX_RSSE_S entries of four bytes when [ENA] is set:
                                                                 0x0 = 256 entries.
                                                                 0x1 = 512 entries.
                                                                 0x2 = 1K entries.
                                                                 0x3 = 2K entries.
                                                                 0x4-0xF = Reserved. */
#else /* Word 0 - Little Endian */
        uint64_t size                  : 4;  /**< [  3:  0](R/W) Specifies table size in NIX_RSSE_S entries of four bytes when [ENA] is set:
                                                                 0x0 = 256 entries.
                                                                 0x1 = 512 entries.
                                                                 0x2 = 1K entries.
                                                                 0x3 = 2K entries.
                                                                 0x4-0xF = Reserved. */
        uint64_t ena                   : 1;  /**< [  4:  4](R/W) RSS is enabled for the LF. */
        uint64_t reserved_5_19         : 15;
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating NIX_RSSE_S structures in NDC (1 means
                                                                 do not use). All ones disables allocation in NDC. */
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing NIX_RSSE_S structures
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of NIX_RSSE_S data will not allocate into the LLC.
                                                                 1 = Writes and reads of NIX_RSSE_S data are allocated into the LLC. */
        uint64_t reserved_37_63        : 27;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rss_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_rss_cfg bdk_nixx_af_lfx_rss_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_RSS_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RSS_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x8500400040c0ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_RSS_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RSS_CFG(a,b) bdk_nixx_af_lfx_rss_cfg_t
#define bustype_BDK_NIXX_AF_LFX_RSS_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RSS_CFG(a,b) "NIXX_AF_LFX_RSS_CFG"
#define device_bar_BDK_NIXX_AF_LFX_RSS_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RSS_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_RSS_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rss_grp#
 *
 * NIX AF Local Function Receive Side Scaling Group Registers
 * A receive packet targets a LF's RSS group when its NIX_RX_ACTION_S[OP] =
 * NIX_RX_ACTIONOP_E::RSS, or its target multicast list has an entry with
 * NIX_RX_MCE_S[OP] = NIX_RX_MCOP_E::RSS. The RSS group index (this register's last
 * index) is NIX_RX_ACTION_S[INDEX] or NIX_RX_MCE_S[INDEX].
 *
 * The RSS computation is as follows:
 * * The packet's FLOW_TAG (see NIX_LF_RX_SECRET()) and RSS group are used to
 * select a NIX_RSSE_S entry in the LF's RSS table (see [SIZEM1]).
 * * NIX_RSSE_S selects the packet's destination RQ.
 */
union bdk_nixx_af_lfx_rss_grpx
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rss_grpx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_19_63        : 45;
        uint64_t sizem1                : 3;  /**< [ 18: 16](R/W) Number of RSS adder bits minus one to add in RSS calculation.
                                                                 0x0 = RSS_ADDER\<0\> included in RSS.
                                                                 0x1 = RSS_ADDER\<1:0\> included in RSS.
                                                                 0x2 = RSS_ADDER\<2:0\> included in RSS.
                                                                 0x3 = RSS_ADDER\<3:0\> included in RSS.
                                                                 0x4 = RSS_ADDER\<4:0\> included in RSS.
                                                                 0x5 = RSS_ADDER\<5:0\> included in RSS.
                                                                 0x6 = RSS_ADDER\<6:0\> included in RSS.
                                                                 0x7 = RSS_ADDER\<7:0\> included in RSS.

                                                                 where:

                                                                 _ RSS_ADDER\<7:0\> = FLOW_TAG\<7:0\> ^ FLOW_TAG\<15:8\> ^ FLOW_TAG\<23:16\> ^ FLOW_TAG\<31:24\>

                                                                 The RVU PF(0) IOVA of the packet's final NIX_RSSE_S structure is computed as follows:
                                                                 \<pre\>
                                                                 rsse_offset = ([OFFSET] + RSS_ADDER[\<[SIZEM1]:0\>]) % (1 \<\< (NIX_AF_LF()_RSS_CFG[SIZE] + 8));
                                                                 rsse_iova = NIX_AF_LF()_RSS_BASE + 4*rsse_offset;
                                                                 \</pre\> */
        uint64_t reserved_11_15        : 5;
        uint64_t offset                : 11; /**< [ 10:  0](R/W) Offset (number of four-byte NIX_RSSE_S structures) into RSS table from
                                                                 NIX_AF_LF()_RSS_BASE. See [SIZEM1]. */
#else /* Word 0 - Little Endian */
        uint64_t offset                : 11; /**< [ 10:  0](R/W) Offset (number of four-byte NIX_RSSE_S structures) into RSS table from
                                                                 NIX_AF_LF()_RSS_BASE. See [SIZEM1]. */
        uint64_t reserved_11_15        : 5;
        uint64_t sizem1                : 3;  /**< [ 18: 16](R/W) Number of RSS adder bits minus one to add in RSS calculation.
                                                                 0x0 = RSS_ADDER\<0\> included in RSS.
                                                                 0x1 = RSS_ADDER\<1:0\> included in RSS.
                                                                 0x2 = RSS_ADDER\<2:0\> included in RSS.
                                                                 0x3 = RSS_ADDER\<3:0\> included in RSS.
                                                                 0x4 = RSS_ADDER\<4:0\> included in RSS.
                                                                 0x5 = RSS_ADDER\<5:0\> included in RSS.
                                                                 0x6 = RSS_ADDER\<6:0\> included in RSS.
                                                                 0x7 = RSS_ADDER\<7:0\> included in RSS.

                                                                 where:

                                                                 _ RSS_ADDER\<7:0\> = FLOW_TAG\<7:0\> ^ FLOW_TAG\<15:8\> ^ FLOW_TAG\<23:16\> ^ FLOW_TAG\<31:24\>

                                                                 The RVU PF(0) IOVA of the packet's final NIX_RSSE_S structure is computed as follows:
                                                                 \<pre\>
                                                                 rsse_offset = ([OFFSET] + RSS_ADDER[\<[SIZEM1]:0\>]) % (1 \<\< (NIX_AF_LF()_RSS_CFG[SIZE] + 8));
                                                                 rsse_iova = NIX_AF_LF()_RSS_BASE + 4*rsse_offset;
                                                                 \</pre\> */
        uint64_t reserved_19_63        : 45;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rss_grpx_s cn; */
};
typedef union bdk_nixx_af_lfx_rss_grpx bdk_nixx_af_lfx_rss_grpx_t;

static inline uint64_t BDK_NIXX_AF_LFX_RSS_GRPX(unsigned long a, unsigned long b, unsigned long c) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RSS_GRPX(unsigned long a, unsigned long b, unsigned long c)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127) && (c<=7)))
        return 0x850040004600ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f) + 8ll * ((c) & 0x7);
    __bdk_csr_fatal("NIXX_AF_LFX_RSS_GRPX", 3, a, b, c, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RSS_GRPX(a,b,c) bdk_nixx_af_lfx_rss_grpx_t
#define bustype_BDK_NIXX_AF_LFX_RSS_GRPX(a,b,c) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RSS_GRPX(a,b,c) "NIXX_AF_LFX_RSS_GRPX"
#define device_bar_BDK_NIXX_AF_LFX_RSS_GRPX(a,b,c) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RSS_GRPX(a,b,c) (a)
#define arguments_BDK_NIXX_AF_LFX_RSS_GRPX(a,b,c) (a),(b),(c),-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rx_cfg
 *
 * NIX AF Local Function Receive Configuration Register
 */
union bdk_nixx_af_lfx_rx_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rx_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_42_63        : 22;
        uint64_t len_ol3               : 1;  /**< [ 41: 41](R/W) Outer L3 length error check enable. See NIX_RX_PERRCODE_E::OL3_LEN. */
        uint64_t len_ol4               : 1;  /**< [ 40: 40](R/W) Outer L4 UDP length error check enable. See NIX_RX_PERRCODE_E::OL4_LEN. */
        uint64_t len_il3               : 1;  /**< [ 39: 39](R/W) Inner L3 length error check enable. See NIX_RX_PERRCODE_E::IL3_LEN. */
        uint64_t len_il4               : 1;  /**< [ 38: 38](R/W) Inner L4 UDP length error check enable. See NIX_RX_PERRCODE_E::IL4_LEN. */
        uint64_t csum_ol4              : 1;  /**< [ 37: 37](R/W) Enable checking of outer L4 TCP/UDP/SCTP checksum. See
                                                                 NIX_RX_PERRCODE_E::OL4_CHK. */
        uint64_t csum_il4              : 1;  /**< [ 36: 36](R/W) Enable checking of inner L4 TCP/UDP/SCTP checksum. See
                                                                 NIX_RX_PERRCODE_E::IL4_CHK. */
        uint64_t dis_apad              : 1;  /**< [ 35: 35](R/W) When set, disables APAD alignment. See NIX_RX_IMM_S[APAD]. */
        uint64_t ip6_udp_opt           : 1;  /**< [ 34: 34](R/W) IPv6/UDP checksum is optional. IPv4 allows an optional UDP checksum by
                                                                 sending the all-0s patterns. IPv6 outlaws this and the spec says to always
                                                                 check UDP checksum.
                                                                 0 = Spec compliant, do not allow all-0s IPv6/UDP checksum.
                                                                 1 = Treat IPv6 as IPv4; the all-0s pattern will cause a UDP checksum pass. */
        uint64_t lenerr_en             : 1;  /**< [ 33: 33](R/W) Outer L2 length error check enable. See NIX_RE_OPCODE_E::OL2_LENMISM. */
        uint64_t reserved_0_32         : 33;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_32         : 33;
        uint64_t lenerr_en             : 1;  /**< [ 33: 33](R/W) Outer L2 length error check enable. See NIX_RE_OPCODE_E::OL2_LENMISM. */
        uint64_t ip6_udp_opt           : 1;  /**< [ 34: 34](R/W) IPv6/UDP checksum is optional. IPv4 allows an optional UDP checksum by
                                                                 sending the all-0s patterns. IPv6 outlaws this and the spec says to always
                                                                 check UDP checksum.
                                                                 0 = Spec compliant, do not allow all-0s IPv6/UDP checksum.
                                                                 1 = Treat IPv6 as IPv4; the all-0s pattern will cause a UDP checksum pass. */
        uint64_t dis_apad              : 1;  /**< [ 35: 35](R/W) When set, disables APAD alignment. See NIX_RX_IMM_S[APAD]. */
        uint64_t csum_il4              : 1;  /**< [ 36: 36](R/W) Enable checking of inner L4 TCP/UDP/SCTP checksum. See
                                                                 NIX_RX_PERRCODE_E::IL4_CHK. */
        uint64_t csum_ol4              : 1;  /**< [ 37: 37](R/W) Enable checking of outer L4 TCP/UDP/SCTP checksum. See
                                                                 NIX_RX_PERRCODE_E::OL4_CHK. */
        uint64_t len_il4               : 1;  /**< [ 38: 38](R/W) Inner L4 UDP length error check enable. See NIX_RX_PERRCODE_E::IL4_LEN. */
        uint64_t len_il3               : 1;  /**< [ 39: 39](R/W) Inner L3 length error check enable. See NIX_RX_PERRCODE_E::IL3_LEN. */
        uint64_t len_ol4               : 1;  /**< [ 40: 40](R/W) Outer L4 UDP length error check enable. See NIX_RX_PERRCODE_E::OL4_LEN. */
        uint64_t len_ol3               : 1;  /**< [ 41: 41](R/W) Outer L3 length error check enable. See NIX_RX_PERRCODE_E::OL3_LEN. */
        uint64_t reserved_42_63        : 22;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rx_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_rx_cfg bdk_nixx_af_lfx_rx_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_RX_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RX_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x8500400040a0ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_RX_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RX_CFG(a,b) bdk_nixx_af_lfx_rx_cfg_t
#define bustype_BDK_NIXX_AF_LFX_RX_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RX_CFG(a,b) "NIXX_AF_LFX_RX_CFG"
#define device_bar_BDK_NIXX_AF_LFX_RX_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RX_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_RX_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rx_ipsec_cfg
 *
 * NIX AF LF Receive IPSEC Configuration Registers
 */
union bdk_nixx_af_lfx_rx_ipsec_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rx_ipsec_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_42_63        : 22;
        uint64_t hshcpt                : 1;  /**< [ 41: 41](RAZ) Hash CPT index. Always zero. */
        uint64_t defcpt                : 1;  /**< [ 40: 40](RAZ) Default CPT index. Always zero. */
        uint64_t tag_const             : 24; /**< [ 39: 16](R/W) Constant value ORed into NIX_WQE_HDR_S[TAG]\<31:8\> for IPSEC fast-path packets
                                                                 (NIX_WQE_HDR_S[WQE_TYPE] = NIX_XQE_TYPE_E::RX_IPSECH/RX_IPSECD/RX_IPSECS). */
        uint64_t tt                    : 2;  /**< [ 15: 14](R/W) SSO tag type to load to NIX_WQE_HDR_S[TT] for IPSEC fast-path packets
                                                                 (NIX_WQE_HDR_S[WQE_TYPE] = NIX_XQE_TYPE_E::RX_IPSECH/RX_IPSECD/RX_IPSECS). */
        uint64_t lenm1_max             : 14; /**< [ 13:  0](R/W) Maximum length in bytes (minus 1) of a packet that may use the IPSEC
                                                                 hardware fast-path. */
#else /* Word 0 - Little Endian */
        uint64_t lenm1_max             : 14; /**< [ 13:  0](R/W) Maximum length in bytes (minus 1) of a packet that may use the IPSEC
                                                                 hardware fast-path. */
        uint64_t tt                    : 2;  /**< [ 15: 14](R/W) SSO tag type to load to NIX_WQE_HDR_S[TT] for IPSEC fast-path packets
                                                                 (NIX_WQE_HDR_S[WQE_TYPE] = NIX_XQE_TYPE_E::RX_IPSECH/RX_IPSECD/RX_IPSECS). */
        uint64_t tag_const             : 24; /**< [ 39: 16](R/W) Constant value ORed into NIX_WQE_HDR_S[TAG]\<31:8\> for IPSEC fast-path packets
                                                                 (NIX_WQE_HDR_S[WQE_TYPE] = NIX_XQE_TYPE_E::RX_IPSECH/RX_IPSECD/RX_IPSECS). */
        uint64_t defcpt                : 1;  /**< [ 40: 40](RAZ) Default CPT index. Always zero. */
        uint64_t hshcpt                : 1;  /**< [ 41: 41](RAZ) Hash CPT index. Always zero. */
        uint64_t reserved_42_63        : 22;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rx_ipsec_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_rx_ipsec_cfg bdk_nixx_af_lfx_rx_ipsec_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_RX_IPSEC_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RX_IPSEC_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004140ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_RX_IPSEC_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RX_IPSEC_CFG(a,b) bdk_nixx_af_lfx_rx_ipsec_cfg_t
#define bustype_BDK_NIXX_AF_LFX_RX_IPSEC_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RX_IPSEC_CFG(a,b) "NIXX_AF_LFX_RX_IPSEC_CFG"
#define device_bar_BDK_NIXX_AF_LFX_RX_IPSEC_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RX_IPSEC_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_RX_IPSEC_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rx_ipsec_dyno_base
 *
 * NIX AF LF Receive IPSEC Dynamic Ordering Base Address Registers
 * This register specifies the base RVU PF(0) IOVA of LF's dynamic ordering
 * (DYNO) counter table in NDC/LLC/DRAM. The table consists of
 * 1 \<\< (NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_IDX_W]) counters. The size of each
 * counter is 1 \<\< NIX_AF_CONST3[DYNO_LOG2BYTES] bytes.
 * See NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_ENA].
 */
union bdk_nixx_af_lfx_rx_ipsec_dyno_base
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rx_ipsec_dyno_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rx_ipsec_dyno_base_s cn; */
};
typedef union bdk_nixx_af_lfx_rx_ipsec_dyno_base bdk_nixx_af_lfx_rx_ipsec_dyno_base_t;

static inline uint64_t BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_BASE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_BASE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004158ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_RX_IPSEC_DYNO_BASE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_BASE(a,b) bdk_nixx_af_lfx_rx_ipsec_dyno_base_t
#define bustype_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_BASE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_BASE(a,b) "NIXX_AF_LFX_RX_IPSEC_DYNO_BASE"
#define device_bar_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_BASE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_BASE(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_BASE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rx_ipsec_dyno_cfg
 *
 * NIX AF LF Receive IPSEC Dynamic Ordering Base Address Registers
 */
union bdk_nixx_af_lfx_rx_ipsec_dyno_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rx_ipsec_dyno_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_37_63        : 27;
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing dynamic ordering counters
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of NIX_RSSE_S data will not allocate into the LLC.
                                                                 1 = Writes and reads of NIX_RSSE_S data are allocated into the LLC. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating NIX_RSSE_S structures in NDC (1 means
                                                                 do not use). All ones disables allocation in NDC. */
        uint64_t reserved_5_19         : 15;
        uint64_t dyno_ena              : 1;  /**< [  4:  4](R/W) Dynamic ordering enable. When set, enables dynamic ordering (DYNO) counters
                                                                 used to enforce ordering between IPSEC hardware fast-path packets
                                                                 (NIX_WQE_HDR_S[WQE_TYPE] = NIX_XQE_TYPE_E::RX_IPSECH) and dynamically
                                                                 prevented packets (NIX_WQE_HDR_S[WQE_TYPE] = NIX_XQE_TYPE_E::RX_IPSECD)
                                                                 within a given flow.

                                                                 When set, hardware maintains 1 \<\< [DYNO_IDX_W] DYNO counters in
                                                                 NDC/LLC/DRAM starting at address NIX_AF_LF()_RX_IPSEC_DYNO_BASE, where the
                                                                 size of each counter is 1 \<\< NIX_AF_CONST3[DYNO_LOG2BYTES] bytes.

                                                                 The DYNO counter index for an IPSEC hardware fast-path or a dynamically
                                                                 prevented packet is obtained from the lower [DYNO_IDX_W] bits of the
                                                                 packet's IPSEC SA index. See NIX_AF_LF()_RX_IPSEC_SA_CFG[SA_IDX_W].

                                                                 Hardware increments the selected DYNO counter by one for each dynamically
                                                                 prevented packet. Software decrements the counter by writing to
                                                                 NIX_LF_OP_IPSEC_DYNO_CNT. Hardware suppresses the IPSEC hardware fast-path
                                                                 when the counter is non-zero. */
        uint64_t dyno_idx_w            : 4;  /**< [  3:  0](R/W) Dynamic ordering counter index width. When [DYNO_ENA]==1, specifies the
                                                                 number of lower bits of an IPSEC packet's SA index used to select the DYNO
                                                                 counter for the packet. Must not be greater than
                                                                 NIX_AF_LF()_RX_IPSEC_SA_CFG[SA_IDX_W]. See [DYNO_ENA]. */
#else /* Word 0 - Little Endian */
        uint64_t dyno_idx_w            : 4;  /**< [  3:  0](R/W) Dynamic ordering counter index width. When [DYNO_ENA]==1, specifies the
                                                                 number of lower bits of an IPSEC packet's SA index used to select the DYNO
                                                                 counter for the packet. Must not be greater than
                                                                 NIX_AF_LF()_RX_IPSEC_SA_CFG[SA_IDX_W]. See [DYNO_ENA]. */
        uint64_t dyno_ena              : 1;  /**< [  4:  4](R/W) Dynamic ordering enable. When set, enables dynamic ordering (DYNO) counters
                                                                 used to enforce ordering between IPSEC hardware fast-path packets
                                                                 (NIX_WQE_HDR_S[WQE_TYPE] = NIX_XQE_TYPE_E::RX_IPSECH) and dynamically
                                                                 prevented packets (NIX_WQE_HDR_S[WQE_TYPE] = NIX_XQE_TYPE_E::RX_IPSECD)
                                                                 within a given flow.

                                                                 When set, hardware maintains 1 \<\< [DYNO_IDX_W] DYNO counters in
                                                                 NDC/LLC/DRAM starting at address NIX_AF_LF()_RX_IPSEC_DYNO_BASE, where the
                                                                 size of each counter is 1 \<\< NIX_AF_CONST3[DYNO_LOG2BYTES] bytes.

                                                                 The DYNO counter index for an IPSEC hardware fast-path or a dynamically
                                                                 prevented packet is obtained from the lower [DYNO_IDX_W] bits of the
                                                                 packet's IPSEC SA index. See NIX_AF_LF()_RX_IPSEC_SA_CFG[SA_IDX_W].

                                                                 Hardware increments the selected DYNO counter by one for each dynamically
                                                                 prevented packet. Software decrements the counter by writing to
                                                                 NIX_LF_OP_IPSEC_DYNO_CNT. Hardware suppresses the IPSEC hardware fast-path
                                                                 when the counter is non-zero. */
        uint64_t reserved_5_19         : 15;
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating NIX_RSSE_S structures in NDC (1 means
                                                                 do not use). All ones disables allocation in NDC. */
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing dynamic ordering counters
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of NIX_RSSE_S data will not allocate into the LLC.
                                                                 1 = Writes and reads of NIX_RSSE_S data are allocated into the LLC. */
        uint64_t reserved_37_63        : 27;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rx_ipsec_dyno_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_rx_ipsec_dyno_cfg bdk_nixx_af_lfx_rx_ipsec_dyno_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004150ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_RX_IPSEC_DYNO_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_CFG(a,b) bdk_nixx_af_lfx_rx_ipsec_dyno_cfg_t
#define bustype_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_CFG(a,b) "NIXX_AF_LFX_RX_IPSEC_DYNO_CFG"
#define device_bar_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_RX_IPSEC_DYNO_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rx_ipsec_sa_base
 *
 * NIX AF LF Receive IPSEC Security Association Base Address Register
 * This register specifies the base IOVA of CPT's IPSEC SA table in LLC/DRAM.
 */
union bdk_nixx_af_lfx_rx_ipsec_sa_base
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rx_ipsec_sa_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rx_ipsec_sa_base_s cn; */
};
typedef union bdk_nixx_af_lfx_rx_ipsec_sa_base bdk_nixx_af_lfx_rx_ipsec_sa_base_t;

static inline uint64_t BDK_NIXX_AF_LFX_RX_IPSEC_SA_BASE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RX_IPSEC_SA_BASE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004170ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_RX_IPSEC_SA_BASE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RX_IPSEC_SA_BASE(a,b) bdk_nixx_af_lfx_rx_ipsec_sa_base_t
#define bustype_BDK_NIXX_AF_LFX_RX_IPSEC_SA_BASE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RX_IPSEC_SA_BASE(a,b) "NIXX_AF_LFX_RX_IPSEC_SA_BASE"
#define device_bar_BDK_NIXX_AF_LFX_RX_IPSEC_SA_BASE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RX_IPSEC_SA_BASE(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_RX_IPSEC_SA_BASE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rx_ipsec_sa_cfg
 *
 * NIX AF LF Receive IPSEC Security Association Configuration Register
 */
union bdk_nixx_af_lfx_rx_ipsec_sa_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rx_ipsec_sa_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t sa_idx_w              : 5;  /**< [ 40: 36](R/W) Security association index width. Number of lower bits from the SPI field
                                                                 of an IPSEC packet that provide the packet's SA index. The SA index is
                                                                 computed as follows:
                                                                 \<pre\>
                                                                 SPI\<31:0\> = packet's 32-bit IPSEC SPI field; // see NIX_AF_RX_DEF_IPSEC()
                                                                 SA_index = SPI & ((1 \<\< [SA_IDX_W]) - 1);
                                                                 \</pre\>

                                                                 If the packet's SA index is greater than [SA_IDX_MAX], the packet uses the
                                                                 IPSEC software fast-path (NIX_WQE_HDR_S[WQE_TYPE]/NIX_CQE_HDR_S[CQE_TYPE] =
                                                                 NIX_XQE_TYPE_E::RX_IPSECS). */
        uint64_t sa_pow2_size          : 4;  /**< [ 35: 32](R/W) Power of 2 size of IPSEC SA structure used by CPT:
                                                                 0x0-0x4 = reserved.
                                                                 0x5 = 32 bytes.
                                                                 0x6 = 64 bytes.
                                                                 0x7 = 128 bytes.
                                                                 0x8 = 256 bytes.
                                                                 0x9 = 512 bytes.
                                                                 0xA-0xF = Reserved. */
        uint64_t sa_idx_max            : 32; /**< [ 31:  0](R/W) Maximum SA index recognized by hardware for the LF. See [SA_IDX_W]. */
#else /* Word 0 - Little Endian */
        uint64_t sa_idx_max            : 32; /**< [ 31:  0](R/W) Maximum SA index recognized by hardware for the LF. See [SA_IDX_W]. */
        uint64_t sa_pow2_size          : 4;  /**< [ 35: 32](R/W) Power of 2 size of IPSEC SA structure used by CPT:
                                                                 0x0-0x4 = reserved.
                                                                 0x5 = 32 bytes.
                                                                 0x6 = 64 bytes.
                                                                 0x7 = 128 bytes.
                                                                 0x8 = 256 bytes.
                                                                 0x9 = 512 bytes.
                                                                 0xA-0xF = Reserved. */
        uint64_t sa_idx_w              : 5;  /**< [ 40: 36](R/W) Security association index width. Number of lower bits from the SPI field
                                                                 of an IPSEC packet that provide the packet's SA index. The SA index is
                                                                 computed as follows:
                                                                 \<pre\>
                                                                 SPI\<31:0\> = packet's 32-bit IPSEC SPI field; // see NIX_AF_RX_DEF_IPSEC()
                                                                 SA_index = SPI & ((1 \<\< [SA_IDX_W]) - 1);
                                                                 \</pre\>

                                                                 If the packet's SA index is greater than [SA_IDX_MAX], the packet uses the
                                                                 IPSEC software fast-path (NIX_WQE_HDR_S[WQE_TYPE]/NIX_CQE_HDR_S[CQE_TYPE] =
                                                                 NIX_XQE_TYPE_E::RX_IPSECS). */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rx_ipsec_sa_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_rx_ipsec_sa_cfg bdk_nixx_af_lfx_rx_ipsec_sa_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_RX_IPSEC_SA_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RX_IPSEC_SA_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004160ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_RX_IPSEC_SA_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RX_IPSEC_SA_CFG(a,b) bdk_nixx_af_lfx_rx_ipsec_sa_cfg_t
#define bustype_BDK_NIXX_AF_LFX_RX_IPSEC_SA_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RX_IPSEC_SA_CFG(a,b) "NIXX_AF_LFX_RX_IPSEC_SA_CFG"
#define device_bar_BDK_NIXX_AF_LFX_RX_IPSEC_SA_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RX_IPSEC_SA_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_RX_IPSEC_SA_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rx_stat#
 *
 * NIX AF Local Function Receive Statistics Registers
 * The last dimension indicates which statistic, and is enumerated by NIX_STAT_LF_RX_E.
 */
union bdk_nixx_af_lfx_rx_statx
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rx_statx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. See also NIX_LF_RX_STAT() for a read-only alias of this
                                                                 field. */
#else /* Word 0 - Little Endian */
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. See also NIX_LF_RX_STAT() for a read-only alias of this
                                                                 field. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rx_statx_s cn; */
};
typedef union bdk_nixx_af_lfx_rx_statx bdk_nixx_af_lfx_rx_statx_t;

static inline uint64_t BDK_NIXX_AF_LFX_RX_STATX(unsigned long a, unsigned long b, unsigned long c) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RX_STATX(unsigned long a, unsigned long b, unsigned long c)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127) && (c<=11)))
        return 0x850040004500ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f) + 8ll * ((c) & 0xf);
    __bdk_csr_fatal("NIXX_AF_LFX_RX_STATX", 3, a, b, c, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RX_STATX(a,b,c) bdk_nixx_af_lfx_rx_statx_t
#define bustype_BDK_NIXX_AF_LFX_RX_STATX(a,b,c) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RX_STATX(a,b,c) "NIXX_AF_LFX_RX_STATX"
#define device_bar_BDK_NIXX_AF_LFX_RX_STATX(a,b,c) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RX_STATX(a,b,c) (a)
#define arguments_BDK_NIXX_AF_LFX_RX_STATX(a,b,c) (a),(b),(c),-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_rx_vtag_type#
 *
 * NIX AF Local Function Receive Vtag Type Registers
 * These registers specify optional Vtag (e.g. VLAN, E-TAG) actions for received
 * packets. Indexed by NIX_RX_VTAG_ACTION_S[VTAG*_TYPE].
 */
union bdk_nixx_af_lfx_rx_vtag_typex
{
    uint64_t u;
    struct bdk_nixx_af_lfx_rx_vtag_typex_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_6_63         : 58;
        uint64_t capture               : 1;  /**< [  5:  5](R/W) When set, Vtag's information is captured in NIX_RX_PARSE_S[VTAG*]. */
        uint64_t strip                 : 1;  /**< [  4:  4](R/W) When set, the Vtag is stripped from the received packet header. Note that the
                                                                 Vtag is silently stripped if [STRIP] is set and [CAPTURE] is clear. */
        uint64_t reserved_3            : 1;
        uint64_t size                  : 3;  /**< [  2:  0](R/W) Vtag size enumerated by NIX_VTAGSIZE_E. */
#else /* Word 0 - Little Endian */
        uint64_t size                  : 3;  /**< [  2:  0](R/W) Vtag size enumerated by NIX_VTAGSIZE_E. */
        uint64_t reserved_3            : 1;
        uint64_t strip                 : 1;  /**< [  4:  4](R/W) When set, the Vtag is stripped from the received packet header. Note that the
                                                                 Vtag is silently stripped if [STRIP] is set and [CAPTURE] is clear. */
        uint64_t capture               : 1;  /**< [  5:  5](R/W) When set, Vtag's information is captured in NIX_RX_PARSE_S[VTAG*]. */
        uint64_t reserved_6_63         : 58;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_rx_vtag_typex_s cn; */
};
typedef union bdk_nixx_af_lfx_rx_vtag_typex bdk_nixx_af_lfx_rx_vtag_typex_t;

static inline uint64_t BDK_NIXX_AF_LFX_RX_VTAG_TYPEX(unsigned long a, unsigned long b, unsigned long c) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_RX_VTAG_TYPEX(unsigned long a, unsigned long b, unsigned long c)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127) && (c<=7)))
        return 0x850040004200ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f) + 8ll * ((c) & 0x7);
    __bdk_csr_fatal("NIXX_AF_LFX_RX_VTAG_TYPEX", 3, a, b, c, 0);
}

#define typedef_BDK_NIXX_AF_LFX_RX_VTAG_TYPEX(a,b,c) bdk_nixx_af_lfx_rx_vtag_typex_t
#define bustype_BDK_NIXX_AF_LFX_RX_VTAG_TYPEX(a,b,c) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_RX_VTAG_TYPEX(a,b,c) "NIXX_AF_LFX_RX_VTAG_TYPEX"
#define device_bar_BDK_NIXX_AF_LFX_RX_VTAG_TYPEX(a,b,c) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_RX_VTAG_TYPEX(a,b,c) (a)
#define arguments_BDK_NIXX_AF_LFX_RX_VTAG_TYPEX(a,b,c) (a),(b),(c),-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_sqs_base
 *
 * NIX AF Local Function Send Queues Base Address Register
 * This register specifies the base RVU PF(0) IOVA of the LF's SQ context table.
 * The table consists of NIX_AF_LF()_SQS_CFG[MAX_QUEUESM1]+1 contiguous
 * NIX_SQ_CTX_HW_S structures.
 */
union bdk_nixx_af_lfx_sqs_base
{
    uint64_t u;
    struct bdk_nixx_af_lfx_sqs_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_sqs_base_s cn; */
};
typedef union bdk_nixx_af_lfx_sqs_base bdk_nixx_af_lfx_sqs_base_t;

static inline uint64_t BDK_NIXX_AF_LFX_SQS_BASE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_SQS_BASE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004030ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_SQS_BASE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_SQS_BASE(a,b) bdk_nixx_af_lfx_sqs_base_t
#define bustype_BDK_NIXX_AF_LFX_SQS_BASE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_SQS_BASE(a,b) "NIXX_AF_LFX_SQS_BASE"
#define device_bar_BDK_NIXX_AF_LFX_SQS_BASE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_SQS_BASE(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_SQS_BASE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_sqs_cfg
 *
 * NIX AF Local Function Send Queues Configuration Register
 * This register configures send queues in the LF.
 */
union bdk_nixx_af_lfx_sqs_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_sqs_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_37_63        : 27;
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing queue context structures
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of context data will not allocate into the LLC.
                                                                 1 = Writes and reads of context data are allocated into the LLC. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating queue context structures in NDC (1
                                                                 means do not use). All ones disables allocation in NDC. */
        uint64_t max_queuesm1          : 20; /**< [ 19:  0](R/W) Maximum number of queues minus one. */
#else /* Word 0 - Little Endian */
        uint64_t max_queuesm1          : 20; /**< [ 19:  0](R/W) Maximum number of queues minus one. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating queue context structures in NDC (1
                                                                 means do not use). All ones disables allocation in NDC. */
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing queue context structures
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of context data will not allocate into the LLC.
                                                                 1 = Writes and reads of context data are allocated into the LLC. */
        uint64_t reserved_37_63        : 27;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_sqs_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_sqs_cfg bdk_nixx_af_lfx_sqs_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_SQS_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_SQS_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004020ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_SQS_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_SQS_CFG(a,b) bdk_nixx_af_lfx_sqs_cfg_t
#define bustype_BDK_NIXX_AF_LFX_SQS_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_SQS_CFG(a,b) "NIXX_AF_LFX_SQS_CFG"
#define device_bar_BDK_NIXX_AF_LFX_SQS_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_SQS_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_SQS_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_tx_cfg
 *
 * NIX AF Local Function Transmit Configuration Register
 */
union bdk_nixx_af_lfx_tx_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_tx_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_36_63        : 28;
        uint64_t lmt_ena               : 1;  /**< [ 35: 35](R/W) LMT store enable.
                                                                 0 = Hardware drops stores to NIX_LF_OP_SEND().
                                                                 1 = Stores to NIX_LF_OP_SEND() are enabled. */
        uint64_t lock_ena              : 1;  /**< [ 34: 34](R/W) Lockdown enable. When set, NIX_AF_LF()_LOCK() can be used to lock
                                                                 down one or more bits in packets transmitted by the LF. */
        uint64_t lock_viol_cqe_ena     : 1;  /**< [ 33: 33](R/W) Enable generation of send WQE/CQE with NIX_SEND_COMP_S[STATUS] =
                                                                 NIX_SEND_STATUS_E::LOCK_VIOL due to the following conditions:
                                                                 * [LOCK_ENA] is set and packet data locked by NIX_AF_LF()_LOCK() does not
                                                                 match.
                                                                 * NIX_TX_ACTION_S[OP] = NIX_TX_ACTIONOP_E::DROP_VIOL. */
        uint64_t send_tstmp_ena        : 1;  /**< [ 32: 32](R/W) Send timestamp enable. When set, the LF is allowed to PTP timestamps in
                                                                 send packets. See NIX_SEND_EXT_S[TSTMP]. */
        uint64_t vlan1_ins_etype       : 16; /**< [ 31: 16](R/W) VLAN 1 Insert Ethertype. Ethertype of VLAN tag that is inserted when
                                                                 NIX_SEND_EXT_S[VLAN1_INS_ENA]=1. */
        uint64_t vlan0_ins_etype       : 16; /**< [ 15:  0](R/W) VLAN 0 Insert Ethertype. Ethertype of VLAN tag that is inserted when
                                                                 NIX_SEND_EXT_S[VLAN0_INS_ENA]=1. */
#else /* Word 0 - Little Endian */
        uint64_t vlan0_ins_etype       : 16; /**< [ 15:  0](R/W) VLAN 0 Insert Ethertype. Ethertype of VLAN tag that is inserted when
                                                                 NIX_SEND_EXT_S[VLAN0_INS_ENA]=1. */
        uint64_t vlan1_ins_etype       : 16; /**< [ 31: 16](R/W) VLAN 1 Insert Ethertype. Ethertype of VLAN tag that is inserted when
                                                                 NIX_SEND_EXT_S[VLAN1_INS_ENA]=1. */
        uint64_t send_tstmp_ena        : 1;  /**< [ 32: 32](R/W) Send timestamp enable. When set, the LF is allowed to PTP timestamps in
                                                                 send packets. See NIX_SEND_EXT_S[TSTMP]. */
        uint64_t lock_viol_cqe_ena     : 1;  /**< [ 33: 33](R/W) Enable generation of send WQE/CQE with NIX_SEND_COMP_S[STATUS] =
                                                                 NIX_SEND_STATUS_E::LOCK_VIOL due to the following conditions:
                                                                 * [LOCK_ENA] is set and packet data locked by NIX_AF_LF()_LOCK() does not
                                                                 match.
                                                                 * NIX_TX_ACTION_S[OP] = NIX_TX_ACTIONOP_E::DROP_VIOL. */
        uint64_t lock_ena              : 1;  /**< [ 34: 34](R/W) Lockdown enable. When set, NIX_AF_LF()_LOCK() can be used to lock
                                                                 down one or more bits in packets transmitted by the LF. */
        uint64_t lmt_ena               : 1;  /**< [ 35: 35](R/W) LMT store enable.
                                                                 0 = Hardware drops stores to NIX_LF_OP_SEND().
                                                                 1 = Stores to NIX_LF_OP_SEND() are enabled. */
        uint64_t reserved_36_63        : 28;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_tx_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_tx_cfg bdk_nixx_af_lfx_tx_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_TX_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_TX_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004080ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_TX_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_TX_CFG(a,b) bdk_nixx_af_lfx_tx_cfg_t
#define bustype_BDK_NIXX_AF_LFX_TX_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_TX_CFG(a,b) "NIXX_AF_LFX_TX_CFG"
#define device_bar_BDK_NIXX_AF_LFX_TX_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_TX_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_TX_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_tx_cfg2
 *
 * NIX AF Local Function Transmit Configuration Register
 */
union bdk_nixx_af_lfx_tx_cfg2
{
    uint64_t u;
    struct bdk_nixx_af_lfx_tx_cfg2_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t lmt_ena               : 1;  /**< [  0:  0](R/W) LMT store enable.
                                                                 0 = Hardware drops stores to NIX_LF_OP_SEND().
                                                                 1 = Stores to NIX_LF_OP_SEND() are enabled. */
#else /* Word 0 - Little Endian */
        uint64_t lmt_ena               : 1;  /**< [  0:  0](R/W) LMT store enable.
                                                                 0 = Hardware drops stores to NIX_LF_OP_SEND().
                                                                 1 = Stores to NIX_LF_OP_SEND() are enabled. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_tx_cfg2_s cn; */
};
typedef union bdk_nixx_af_lfx_tx_cfg2 bdk_nixx_af_lfx_tx_cfg2_t;

static inline uint64_t BDK_NIXX_AF_LFX_TX_CFG2(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_TX_CFG2(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004028ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_TX_CFG2", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_TX_CFG2(a,b) bdk_nixx_af_lfx_tx_cfg2_t
#define bustype_BDK_NIXX_AF_LFX_TX_CFG2(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_TX_CFG2(a,b) "NIXX_AF_LFX_TX_CFG2"
#define device_bar_BDK_NIXX_AF_LFX_TX_CFG2(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_TX_CFG2(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_TX_CFG2(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_tx_parse_cfg
 *
 * NIX AF Local Function Transmit Parse Configuration Register
 */
union bdk_nixx_af_lfx_tx_parse_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lfx_tx_parse_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_6_63         : 58;
        uint64_t pkind                 : 6;  /**< [  5:  0](R/W/H) Port kind supplied to NPC to seed the parsing of packets to be transmitted by this LF. */
#else /* Word 0 - Little Endian */
        uint64_t pkind                 : 6;  /**< [  5:  0](R/W/H) Port kind supplied to NPC to seed the parsing of packets to be transmitted by this LF. */
        uint64_t reserved_6_63         : 58;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_tx_parse_cfg_s cn; */
};
typedef union bdk_nixx_af_lfx_tx_parse_cfg bdk_nixx_af_lfx_tx_parse_cfg_t;

static inline uint64_t BDK_NIXX_AF_LFX_TX_PARSE_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_TX_PARSE_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004090ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_TX_PARSE_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_TX_PARSE_CFG(a,b) bdk_nixx_af_lfx_tx_parse_cfg_t
#define bustype_BDK_NIXX_AF_LFX_TX_PARSE_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_TX_PARSE_CFG(a,b) "NIXX_AF_LFX_TX_PARSE_CFG"
#define device_bar_BDK_NIXX_AF_LFX_TX_PARSE_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_TX_PARSE_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_TX_PARSE_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_tx_stat#
 *
 * NIX AF Local Function Transmit Statistics Registers
 * The last dimension indicates which statistic, and is enumerated by NIX_STAT_LF_TX_E.
 */
union bdk_nixx_af_lfx_tx_statx
{
    uint64_t u;
    struct bdk_nixx_af_lfx_tx_statx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. See also NIX_LF_TX_STAT() for a read-only alias of this
                                                                 field. */
#else /* Word 0 - Little Endian */
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. See also NIX_LF_TX_STAT() for a read-only alias of this
                                                                 field. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_tx_statx_s cn; */
};
typedef union bdk_nixx_af_lfx_tx_statx bdk_nixx_af_lfx_tx_statx_t;

static inline uint64_t BDK_NIXX_AF_LFX_TX_STATX(unsigned long a, unsigned long b, unsigned long c) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_TX_STATX(unsigned long a, unsigned long b, unsigned long c)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127) && (c<=4)))
        return 0x850040004400ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f) + 8ll * ((c) & 0x7);
    __bdk_csr_fatal("NIXX_AF_LFX_TX_STATX", 3, a, b, c, 0);
}

#define typedef_BDK_NIXX_AF_LFX_TX_STATX(a,b,c) bdk_nixx_af_lfx_tx_statx_t
#define bustype_BDK_NIXX_AF_LFX_TX_STATX(a,b,c) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_TX_STATX(a,b,c) "NIXX_AF_LFX_TX_STATX"
#define device_bar_BDK_NIXX_AF_LFX_TX_STATX(a,b,c) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_TX_STATX(a,b,c) (a)
#define arguments_BDK_NIXX_AF_LFX_TX_STATX(a,b,c) (a),(b),(c),-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf#_tx_status
 *
 * NIX AF LF Transmit Status Register
 */
union bdk_nixx_af_lfx_tx_status
{
    uint64_t u;
    struct bdk_nixx_af_lfx_tx_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t sq_ctx_err            : 1;  /**< [  0:  0](R/W1C/H) SQ context error. An error was detected on a NIX_SQ_CTX_S read. When set:
                                                                 * Hardware drops LMT stores to NIX_LF_OP_SEND().
                                                                 * Hardware stops enqueueing MDs to SMQs assigned to the LF
                                                                 (NIX_AF_SMQ()_CFG[LF]).

                                                                 Software can clear this bit by writing a one. */
#else /* Word 0 - Little Endian */
        uint64_t sq_ctx_err            : 1;  /**< [  0:  0](R/W1C/H) SQ context error. An error was detected on a NIX_SQ_CTX_S read. When set:
                                                                 * Hardware drops LMT stores to NIX_LF_OP_SEND().
                                                                 * Hardware stops enqueueing MDs to SMQs assigned to the LF
                                                                 (NIX_AF_SMQ()_CFG[LF]).

                                                                 Software can clear this bit by writing a one. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lfx_tx_status_s cn; */
};
typedef union bdk_nixx_af_lfx_tx_status bdk_nixx_af_lfx_tx_status_t;

static inline uint64_t BDK_NIXX_AF_LFX_TX_STATUS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LFX_TX_STATUS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040004180ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_LFX_TX_STATUS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_LFX_TX_STATUS(a,b) bdk_nixx_af_lfx_tx_status_t
#define bustype_BDK_NIXX_AF_LFX_TX_STATUS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LFX_TX_STATUS(a,b) "NIXX_AF_LFX_TX_STATUS"
#define device_bar_BDK_NIXX_AF_LFX_TX_STATUS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LFX_TX_STATUS(a,b) (a)
#define arguments_BDK_NIXX_AF_LFX_TX_STATUS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lf_rst
 *
 * NIX Admin Function LF Soft Reset Register
 */
union bdk_nixx_af_lf_rst
{
    uint64_t u;
    struct bdk_nixx_af_lf_rst_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_13_63        : 51;
        uint64_t exec                  : 1;  /**< [ 12: 12](R/W1S/H) Execute LF soft reset. When software writes a one to set this bit, hardware
                                                                 resets the local function selected by [LF]. Hardware clears this bit when
                                                                 done. */
        uint64_t reserved_8_11         : 4;
        uint64_t lf                    : 8;  /**< [  7:  0](R/W) Local function that is reset when [EXEC] is set. */
#else /* Word 0 - Little Endian */
        uint64_t lf                    : 8;  /**< [  7:  0](R/W) Local function that is reset when [EXEC] is set. */
        uint64_t reserved_8_11         : 4;
        uint64_t exec                  : 1;  /**< [ 12: 12](R/W1S/H) Execute LF soft reset. When software writes a one to set this bit, hardware
                                                                 resets the local function selected by [LF]. Hardware clears this bit when
                                                                 done. */
        uint64_t reserved_13_63        : 51;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lf_rst_s cn; */
};
typedef union bdk_nixx_af_lf_rst bdk_nixx_af_lf_rst_t;

static inline uint64_t BDK_NIXX_AF_LF_RST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LF_RST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000150ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_LF_RST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_LF_RST(a) bdk_nixx_af_lf_rst_t
#define bustype_BDK_NIXX_AF_LF_RST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LF_RST(a) "NIXX_AF_LF_RST"
#define device_bar_BDK_NIXX_AF_LF_RST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LF_RST(a) (a)
#define arguments_BDK_NIXX_AF_LF_RST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lso_cfg
 *
 * NIX AF Large Send Offload Configuration Register
 */
union bdk_nixx_af_lso_cfg
{
    uint64_t u;
    struct bdk_nixx_af_lso_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t enable                : 1;  /**< [ 63: 63](R/W) Large send offload enable. When clear, NIX ignores NIX_SEND_EXT_S[LSO] and treats
                                                                 all send descriptors as non-LSO. */
        uint64_t crc_enable            : 1;  /**< [ 62: 62](R/W) Enable NIX_SEND_CRC_S with LSO. When clear, NIX ignores NIX_SEND_CRC_S subdescriptors in a
                                                                 send descriptor with NIX_SEND_EXT_S[LSO]=1. */
        uint64_t reserved_48_61        : 14;
        uint64_t tcp_fsf               : 16; /**< [ 47: 32](R/W) TCP first segment flags mask. Used for the first packet/segment of an LSO
                                                                 descriptor when NIX_AF_LSO_FORMAT()_FIELD()[ALG] = NIX_LSOALG_E::TCP_FLAGS.
                                                                 The selected 16-bit field, typically consisting of the TCP data offset and
                                                                 TCP flags, is modified as follows:

                                                                 _ FIELD_new = (FIELD_original) AND [TCP_FSF].

                                                                 The upper four bits are typically 0xF in order to keep the original TCP data
                                                                 offset value. */
        uint64_t tcp_msf               : 16; /**< [ 31: 16](R/W) TCP middle segment flags mask. Same as [TCP_FSF] but used for LSO
                                                                 packets/segments other than the first and last segments of an LSO
                                                                 descriptor. */
        uint64_t tcp_lsf               : 16; /**< [ 15:  0](R/W) TCP last segment flags mask. Same as [TCP_FSF] but used for the last
                                                                 packet/segment of an LSO descriptor. */
#else /* Word 0 - Little Endian */
        uint64_t tcp_lsf               : 16; /**< [ 15:  0](R/W) TCP last segment flags mask. Same as [TCP_FSF] but used for the last
                                                                 packet/segment of an LSO descriptor. */
        uint64_t tcp_msf               : 16; /**< [ 31: 16](R/W) TCP middle segment flags mask. Same as [TCP_FSF] but used for LSO
                                                                 packets/segments other than the first and last segments of an LSO
                                                                 descriptor. */
        uint64_t tcp_fsf               : 16; /**< [ 47: 32](R/W) TCP first segment flags mask. Used for the first packet/segment of an LSO
                                                                 descriptor when NIX_AF_LSO_FORMAT()_FIELD()[ALG] = NIX_LSOALG_E::TCP_FLAGS.
                                                                 The selected 16-bit field, typically consisting of the TCP data offset and
                                                                 TCP flags, is modified as follows:

                                                                 _ FIELD_new = (FIELD_original) AND [TCP_FSF].

                                                                 The upper four bits are typically 0xF in order to keep the original TCP data
                                                                 offset value. */
        uint64_t reserved_48_61        : 14;
        uint64_t crc_enable            : 1;  /**< [ 62: 62](R/W) Enable NIX_SEND_CRC_S with LSO. When clear, NIX ignores NIX_SEND_CRC_S subdescriptors in a
                                                                 send descriptor with NIX_SEND_EXT_S[LSO]=1. */
        uint64_t enable                : 1;  /**< [ 63: 63](R/W) Large send offload enable. When clear, NIX ignores NIX_SEND_EXT_S[LSO] and treats
                                                                 all send descriptors as non-LSO. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lso_cfg_s cn; */
};
typedef union bdk_nixx_af_lso_cfg bdk_nixx_af_lso_cfg_t;

static inline uint64_t BDK_NIXX_AF_LSO_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LSO_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400000a8ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_LSO_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_LSO_CFG(a) bdk_nixx_af_lso_cfg_t
#define bustype_BDK_NIXX_AF_LSO_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LSO_CFG(a) "NIXX_AF_LSO_CFG"
#define device_bar_BDK_NIXX_AF_LSO_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LSO_CFG(a) (a)
#define arguments_BDK_NIXX_AF_LSO_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_lso_format#_field#
 *
 * NIX AF Large Send Offload Format Field Registers
 * These registers specify LSO packet modification formats. Each format may modify
 * up to eight packet fields with the following constraints:
 * * If fewer than eight fields are modified, [ALG] must be NIX_LSOALG_E::NOP in the
 * unused field registers.
 * * Modified fields must be specified in contiguous field registers starting with
 * NIX_AF_LSO_FORMAT()_FIELD(0).
 * * Multiple fields with the same [LAYER] value must be specified in
 * ascending [OFFSET] order.
 * * Fields in different layers must be specified in ascending [LAYER] order.
 */
union bdk_nixx_af_lso_formatx_fieldx
{
    uint64_t u;
    struct bdk_nixx_af_lso_formatx_fieldx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_19_63        : 45;
        uint64_t alg                   : 3;  /**< [ 18: 16](R/W) Field modification algorithm enumerated by NIX_LSOALG_E. The remaining
                                                                 fields in the register are valid when value is not NIX_LSOALG_E::NOP. */
        uint64_t reserved_14_15        : 2;
        uint64_t sizem1                : 2;  /**< [ 13: 12](R/W) Field size in bytes minus one. */
        uint64_t reserved_10_11        : 2;
        uint64_t layer                 : 2;  /**< [  9:  8](R/W) Header layer that contains the field, enumerated by NIX_TXLAYER_E. */
        uint64_t offset                : 8;  /**< [  7:  0](R/W) Starting byte offset of the field relative to the first byte of [LAYER] in
                                                                 the packet header. */
#else /* Word 0 - Little Endian */
        uint64_t offset                : 8;  /**< [  7:  0](R/W) Starting byte offset of the field relative to the first byte of [LAYER] in
                                                                 the packet header. */
        uint64_t layer                 : 2;  /**< [  9:  8](R/W) Header layer that contains the field, enumerated by NIX_TXLAYER_E. */
        uint64_t reserved_10_11        : 2;
        uint64_t sizem1                : 2;  /**< [ 13: 12](R/W) Field size in bytes minus one. */
        uint64_t reserved_14_15        : 2;
        uint64_t alg                   : 3;  /**< [ 18: 16](R/W) Field modification algorithm enumerated by NIX_LSOALG_E. The remaining
                                                                 fields in the register are valid when value is not NIX_LSOALG_E::NOP. */
        uint64_t reserved_19_63        : 45;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_lso_formatx_fieldx_s cn; */
};
typedef union bdk_nixx_af_lso_formatx_fieldx bdk_nixx_af_lso_formatx_fieldx_t;

static inline uint64_t BDK_NIXX_AF_LSO_FORMATX_FIELDX(unsigned long a, unsigned long b, unsigned long c) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_LSO_FORMATX_FIELDX(unsigned long a, unsigned long b, unsigned long c)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=31) && (c<=7)))
        return 0x850040001b00ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f) + 8ll * ((c) & 0x7);
    __bdk_csr_fatal("NIXX_AF_LSO_FORMATX_FIELDX", 3, a, b, c, 0);
}

#define typedef_BDK_NIXX_AF_LSO_FORMATX_FIELDX(a,b,c) bdk_nixx_af_lso_formatx_fieldx_t
#define bustype_BDK_NIXX_AF_LSO_FORMATX_FIELDX(a,b,c) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_LSO_FORMATX_FIELDX(a,b,c) "NIXX_AF_LSO_FORMATX_FIELDX"
#define device_bar_BDK_NIXX_AF_LSO_FORMATX_FIELDX(a,b,c) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_LSO_FORMATX_FIELDX(a,b,c) (a)
#define arguments_BDK_NIXX_AF_LSO_FORMATX_FIELDX(a,b,c) (a),(b),(c),-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mark_format#_ctl
 *
 * NIX AF Packet Marking Format Registers
 * Describes packet marking calculations for YELLOW and for
 * NIX_COLORRESULT_E::RED_SEND packets. NIX_SEND_EXT_S[MARKFORM] selects the CSR
 * used for the packet descriptor.
 *
 * All the packet marking offset calculations assume big-endian bits within a byte.
 *
 * For example, if NIX_SEND_EXT_S[MARKPTR] is 3 and [OFFSET] is 5 and the packet is YELLOW,
 * the NIX marking hardware would do this:
 *
 * _  byte[3]\<2:0\> |=   [Y_VAL]\<3:1\>
 * _  byte[3]\<2:0\> &= ~[Y_MASK]\<3:1\>
 * _  byte[4]\<7\>   |=   [Y_VAL]\<0\>
 * _  byte[4]\<7\>   &= ~[Y_MASK]\<0\>
 *
 * where byte[3] is the third byte in the packet, and byte[4] the fourth.
 *
 * For another example, if NIX_SEND_EXT_S[MARKPTR] is 3 and [OFFSET] is 0 and the
 * packet is NIX_COLORRESULT_E::RED_SEND,
 *
 * _   byte[3]\<7:4\> |=   [R_VAL]\<3:0\>
 * _   byte[3]\<7:4\> &= ~[R_MASK]\<3:0\>
 */
union bdk_nixx_af_mark_formatx_ctl
{
    uint64_t u;
    struct bdk_nixx_af_mark_formatx_ctl_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_19_63        : 45;
        uint64_t offset                : 3;  /**< [ 18: 16](R/W) Packet marking starts NIX_SEND_EXT_S[MARKPTR]*8 + [OFFSET] bits into the packet.
                                                                 All processing with [Y_MASK,Y_VAL,R_MASK,R_VAL] starts at this offset. */
        uint64_t y_mask                : 4;  /**< [ 15: 12](R/W) Yellow mark mask. Corresponding bits in packet's data are cleared when marking a YELLOW
                                                                 packet. [Y_MASK] & [Y_VAL] must be zero. */
        uint64_t y_val                 : 4;  /**< [ 11:  8](R/W) Yellow mark value. Corresponding bits in packet's data are set when marking a YELLOW
                                                                 packet. [Y_MASK] & [Y_VAL] must be zero. */
        uint64_t r_mask                : 4;  /**< [  7:  4](R/W) Red mark mask. Corresponding bits in packet's data are cleared when marking
                                                                 a NIX_COLORRESULT_E::RED_SEND packet. [R_MASK] & [R_VAL] must be zero. */
        uint64_t r_val                 : 4;  /**< [  3:  0](R/W) Red mark value. Corresponding bits in packet's data are set when marking a
                                                                 NIX_COLORRESULT_E::RED_SEND packet. [R_MASK] & [R_VAL] must be zero. */
#else /* Word 0 - Little Endian */
        uint64_t r_val                 : 4;  /**< [  3:  0](R/W) Red mark value. Corresponding bits in packet's data are set when marking a
                                                                 NIX_COLORRESULT_E::RED_SEND packet. [R_MASK] & [R_VAL] must be zero. */
        uint64_t r_mask                : 4;  /**< [  7:  4](R/W) Red mark mask. Corresponding bits in packet's data are cleared when marking
                                                                 a NIX_COLORRESULT_E::RED_SEND packet. [R_MASK] & [R_VAL] must be zero. */
        uint64_t y_val                 : 4;  /**< [ 11:  8](R/W) Yellow mark value. Corresponding bits in packet's data are set when marking a YELLOW
                                                                 packet. [Y_MASK] & [Y_VAL] must be zero. */
        uint64_t y_mask                : 4;  /**< [ 15: 12](R/W) Yellow mark mask. Corresponding bits in packet's data are cleared when marking a YELLOW
                                                                 packet. [Y_MASK] & [Y_VAL] must be zero. */
        uint64_t offset                : 3;  /**< [ 18: 16](R/W) Packet marking starts NIX_SEND_EXT_S[MARKPTR]*8 + [OFFSET] bits into the packet.
                                                                 All processing with [Y_MASK,Y_VAL,R_MASK,R_VAL] starts at this offset. */
        uint64_t reserved_19_63        : 45;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mark_formatx_ctl_s cn; */
};
typedef union bdk_nixx_af_mark_formatx_ctl bdk_nixx_af_mark_formatx_ctl_t;

static inline uint64_t BDK_NIXX_AF_MARK_FORMATX_CTL(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MARK_FORMATX_CTL(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850040000900ll + 0x10000000ll * ((a) & 0x0) + 0x40000ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_AF_MARK_FORMATX_CTL", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MARK_FORMATX_CTL(a,b) bdk_nixx_af_mark_formatx_ctl_t
#define bustype_BDK_NIXX_AF_MARK_FORMATX_CTL(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MARK_FORMATX_CTL(a,b) "NIXX_AF_MARK_FORMATX_CTL"
#define device_bar_BDK_NIXX_AF_MARK_FORMATX_CTL(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MARK_FORMATX_CTL(a,b) (a)
#define arguments_BDK_NIXX_AF_MARK_FORMATX_CTL(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mc_mirror_const
 *
 * NIX AF Multicast/Mirror Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_mc_mirror_const
{
    uint64_t u;
    struct bdk_nixx_af_mc_mirror_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_16_63        : 48;
        uint64_t buf_size              : 16; /**< [ 15:  0](RO) Size in bytes of multicast/mirror buffers in NDC/LLC/DRAM. */
#else /* Word 0 - Little Endian */
        uint64_t buf_size              : 16; /**< [ 15:  0](RO) Size in bytes of multicast/mirror buffers in NDC/LLC/DRAM. */
        uint64_t reserved_16_63        : 48;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mc_mirror_const_s cn; */
};
typedef union bdk_nixx_af_mc_mirror_const bdk_nixx_af_mc_mirror_const_t;

static inline uint64_t BDK_NIXX_AF_MC_MIRROR_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MC_MIRROR_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000098ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_MC_MIRROR_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_MC_MIRROR_CONST(a) bdk_nixx_af_mc_mirror_const_t
#define bustype_BDK_NIXX_AF_MC_MIRROR_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MC_MIRROR_CONST(a) "NIXX_AF_MC_MIRROR_CONST"
#define device_bar_BDK_NIXX_AF_MC_MIRROR_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MC_MIRROR_CONST(a) (a)
#define arguments_BDK_NIXX_AF_MC_MIRROR_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_cir
 *
 * NIX AF Meta Descriptor Queue Committed Information Rate Registers
 * This register has the same bit fields as NIX_AF_TL1()_CIR.
 */
union bdk_nixx_af_mdqx_cir
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_cir_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_17_28        : 12;
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
#else /* Word 0 - Little Endian */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t reserved_17_28        : 12;
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_cir_s cn; */
};
typedef union bdk_nixx_af_mdqx_cir bdk_nixx_af_mdqx_cir_t;

static inline uint64_t BDK_NIXX_AF_MDQX_CIR(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_CIR(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001420ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_CIR", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_CIR(a,b) bdk_nixx_af_mdqx_cir_t
#define bustype_BDK_NIXX_AF_MDQX_CIR(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_CIR(a,b) "NIXX_AF_MDQX_CIR"
#define device_bar_BDK_NIXX_AF_MDQX_CIR(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_CIR(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_CIR(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_md_debug
 *
 * NIX AF Meta Descriptor Queue Meta Descriptor State Debug Registers
 * This register provides access to the meta descriptor at the front of the MDQ. An MDQ can
 * hold up to 8 packet meta descriptors (PMD) and one flush meta descriptor (FMD).
 */
union bdk_nixx_af_mdqx_md_debug
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_md_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_63           : 1;
        uint64_t md_type               : 2;  /**< [ 62: 61](R/W/H) Meta descriptor type, enumerated by NIX_MDTYPE_E. */
        uint64_t reserved_45_60        : 16;
        uint64_t sqm_pkt_id            : 13; /**< [ 44: 32](R/W/H) SQM Packet Index. */
        uint64_t reserved_29_31        : 3;
        uint64_t shp_chg               : 9;  /**< [ 28: 20](R/W/H) When [ADJUST] is not 0x100, it is the NIX_SEND_EXT_S[SHP_CHG] for the
                                                                 packet. */
        uint64_t reserved_19           : 1;
        uint64_t shp_dis               : 1;  /**< [ 18: 18](R/W/H) Committed shaper disabled, PIR and CIR disable. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS, CIR_DIS] = NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS, CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, [PIR_DIS] not used by the TL1 rate limiters. TL1 uses
                                                                 only  [CIR_DIS]. [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t red_algo_override     : 2;  /**< [ 17: 16](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor.
                                                                 [RED_ALGO_OVERRIDE] is used by the TL4 through TL2
                                                                 shapers, but not used by the TL1 rate limiters. */
        uint64_t pkt_len               : 16; /**< [ 15:  0](R/W/H) Packet length. Generally, the size of the outgoing packet including pad,
                                                                 optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble. See NIX_AF_SMQ()_CFG[MINLEN]. */
#else /* Word 0 - Little Endian */
        uint64_t pkt_len               : 16; /**< [ 15:  0](R/W/H) Packet length. Generally, the size of the outgoing packet including pad,
                                                                 optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble. See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t red_algo_override     : 2;  /**< [ 17: 16](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor.
                                                                 [RED_ALGO_OVERRIDE] is used by the TL4 through TL2
                                                                 shapers, but not used by the TL1 rate limiters. */
        uint64_t shp_dis               : 1;  /**< [ 18: 18](R/W/H) Committed shaper disabled, PIR and CIR disable. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS, CIR_DIS] = NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS, CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, [PIR_DIS] not used by the TL1 rate limiters. TL1 uses
                                                                 only  [CIR_DIS]. [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t reserved_19           : 1;
        uint64_t shp_chg               : 9;  /**< [ 28: 20](R/W/H) When [ADJUST] is not 0x100, it is the NIX_SEND_EXT_S[SHP_CHG] for the
                                                                 packet. */
        uint64_t reserved_29_31        : 3;
        uint64_t sqm_pkt_id            : 13; /**< [ 44: 32](R/W/H) SQM Packet Index. */
        uint64_t reserved_45_60        : 16;
        uint64_t md_type               : 2;  /**< [ 62: 61](R/W/H) Meta descriptor type, enumerated by NIX_MDTYPE_E. */
        uint64_t reserved_63           : 1;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_md_debug_s cn; */
};
typedef union bdk_nixx_af_mdqx_md_debug bdk_nixx_af_mdqx_md_debug_t;

static inline uint64_t BDK_NIXX_AF_MDQX_MD_DEBUG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_MD_DEBUG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x8500400014c0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_MD_DEBUG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_MD_DEBUG(a,b) bdk_nixx_af_mdqx_md_debug_t
#define bustype_BDK_NIXX_AF_MDQX_MD_DEBUG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_MD_DEBUG(a,b) "NIXX_AF_MDQX_MD_DEBUG"
#define device_bar_BDK_NIXX_AF_MDQX_MD_DEBUG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_MD_DEBUG(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_MD_DEBUG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_parent
 *
 * NIX AF Meta Descriptor Queue Topology Registers
 */
union bdk_nixx_af_mdqx_parent
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_parent_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_25_63        : 39;
        uint64_t parent                : 9;  /**< [ 24: 16](R/W) See NIX_AF_TL2()_PARENT[PARENT]. */
        uint64_t reserved_0_15         : 16;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_15         : 16;
        uint64_t parent                : 9;  /**< [ 24: 16](R/W) See NIX_AF_TL2()_PARENT[PARENT]. */
        uint64_t reserved_25_63        : 39;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_parent_s cn; */
};
typedef union bdk_nixx_af_mdqx_parent bdk_nixx_af_mdqx_parent_t;

static inline uint64_t BDK_NIXX_AF_MDQX_PARENT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_PARENT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001480ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_PARENT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_PARENT(a,b) bdk_nixx_af_mdqx_parent_t
#define bustype_BDK_NIXX_AF_MDQX_PARENT(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_PARENT(a,b) "NIXX_AF_MDQX_PARENT"
#define device_bar_BDK_NIXX_AF_MDQX_PARENT(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_PARENT(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_PARENT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_pir
 *
 * NIX AF Meta Descriptor Queue Peak Information Rate Registers
 * This register has the same bit fields as NIX_AF_TL1()_CIR.
 */
union bdk_nixx_af_mdqx_pir
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_pir_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_17_28        : 12;
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
#else /* Word 0 - Little Endian */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t reserved_17_28        : 12;
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_pir_s cn; */
};
typedef union bdk_nixx_af_mdqx_pir bdk_nixx_af_mdqx_pir_t;

static inline uint64_t BDK_NIXX_AF_MDQX_PIR(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_PIR(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001430ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_PIR", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_PIR(a,b) bdk_nixx_af_mdqx_pir_t
#define bustype_BDK_NIXX_AF_MDQX_PIR(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_PIR(a,b) "NIXX_AF_MDQX_PIR"
#define device_bar_BDK_NIXX_AF_MDQX_PIR(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_PIR(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_PIR(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_pointers
 *
 * INTERNAL: NIX AF Meta Descriptor 4 Linked List Pointers Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL4()_POINTERS.
 */
union bdk_nixx_af_mdqx_pointers
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_pointers_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_25_63        : 39;
        uint64_t prev                  : 9;  /**< [ 24: 16](R/W/H) See NIX_AF_TL2()_POINTERS[PREV]. */
        uint64_t reserved_9_15         : 7;
        uint64_t next                  : 9;  /**< [  8:  0](R/W/H) See NIX_AF_TL2()_POINTERS[NEXT]. */
#else /* Word 0 - Little Endian */
        uint64_t next                  : 9;  /**< [  8:  0](R/W/H) See NIX_AF_TL2()_POINTERS[NEXT]. */
        uint64_t reserved_9_15         : 7;
        uint64_t prev                  : 9;  /**< [ 24: 16](R/W/H) See NIX_AF_TL2()_POINTERS[PREV]. */
        uint64_t reserved_25_63        : 39;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_pointers_s cn; */
};
typedef union bdk_nixx_af_mdqx_pointers bdk_nixx_af_mdqx_pointers_t;

static inline uint64_t BDK_NIXX_AF_MDQX_POINTERS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_POINTERS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001460ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_POINTERS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_POINTERS(a,b) bdk_nixx_af_mdqx_pointers_t
#define bustype_BDK_NIXX_AF_MDQX_POINTERS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_POINTERS(a,b) "NIXX_AF_MDQX_POINTERS"
#define device_bar_BDK_NIXX_AF_MDQX_POINTERS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_POINTERS(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_POINTERS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_ptr_fifo
 *
 * INTERNAL: NIX Meta Descriptor Queue Pointer FIFO State Debug Registers
 */
union bdk_nixx_af_mdqx_ptr_fifo
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_ptr_fifo_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_10_63        : 54;
        uint64_t flush_bypass          : 1;  /**< [  9:  9](RO/H) Flush bypass, tail descriptor in DESC FIFO is Flush Descriptor. Flush descriptor
                                                                 should bypass meta descriptor in queue. Meta descriptors do not meet squash
                                                                 requirements or squash filed set to one. */
        uint64_t p_con                 : 1;  /**< [  8:  8](R/W/H) parent connect. Asserted when MDQ is connected to its parent. */
        uint64_t head                  : 4;  /**< [  7:  4](R/W/H) MDQ DESC FIFO head pointer. */
        uint64_t tail                  : 4;  /**< [  3:  0](R/W/H) MDQ DESC FIFO tail pointer. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 4;  /**< [  3:  0](R/W/H) MDQ DESC FIFO tail pointer. */
        uint64_t head                  : 4;  /**< [  7:  4](R/W/H) MDQ DESC FIFO head pointer. */
        uint64_t p_con                 : 1;  /**< [  8:  8](R/W/H) parent connect. Asserted when MDQ is connected to its parent. */
        uint64_t flush_bypass          : 1;  /**< [  9:  9](RO/H) Flush bypass, tail descriptor in DESC FIFO is Flush Descriptor. Flush descriptor
                                                                 should bypass meta descriptor in queue. Meta descriptors do not meet squash
                                                                 requirements or squash filed set to one. */
        uint64_t reserved_10_63        : 54;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_ptr_fifo_s cn; */
};
typedef union bdk_nixx_af_mdqx_ptr_fifo bdk_nixx_af_mdqx_ptr_fifo_t;

static inline uint64_t BDK_NIXX_AF_MDQX_PTR_FIFO(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_PTR_FIFO(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x8500400014d0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_PTR_FIFO", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_PTR_FIFO(a,b) bdk_nixx_af_mdqx_ptr_fifo_t
#define bustype_BDK_NIXX_AF_MDQX_PTR_FIFO(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_PTR_FIFO(a,b) "NIXX_AF_MDQX_PTR_FIFO"
#define device_bar_BDK_NIXX_AF_MDQX_PTR_FIFO(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_PTR_FIFO(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_PTR_FIFO(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_sched_state
 *
 * NIX AF Meta Descriptor Queue Scheduling Control State Registers
 * This register has the same bit fields as NIX_AF_TL2()_SCHED_STATE.
 */
union bdk_nixx_af_mdqx_sched_state
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_sched_state_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_25_63        : 39;
        uint64_t rr_count              : 25; /**< [ 24:  0](R/W/H) Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
#else /* Word 0 - Little Endian */
        uint64_t rr_count              : 25; /**< [ 24:  0](R/W/H) Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
        uint64_t reserved_25_63        : 39;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_sched_state_s cn; */
};
typedef union bdk_nixx_af_mdqx_sched_state bdk_nixx_af_mdqx_sched_state_t;

static inline uint64_t BDK_NIXX_AF_MDQX_SCHED_STATE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_SCHED_STATE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001440ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_SCHED_STATE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_SCHED_STATE(a,b) bdk_nixx_af_mdqx_sched_state_t
#define bustype_BDK_NIXX_AF_MDQX_SCHED_STATE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_SCHED_STATE(a,b) "NIXX_AF_MDQX_SCHED_STATE"
#define device_bar_BDK_NIXX_AF_MDQX_SCHED_STATE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_SCHED_STATE(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_SCHED_STATE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_schedule
 *
 * NIX AF Meta Descriptor Queue Scheduling Control Registers
 * This register has the same bit fields as NIX_AF_TL2()_SCHEDULE.
 */
union bdk_nixx_af_mdqx_schedule
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_schedule_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_28_63        : 36;
        uint64_t prio                  : 4;  /**< [ 27: 24](R/W) Priority. The priority used for this shaping queue in the (lower-level)
                                                                 parent's scheduling algorithm. When this shaping queue is not used, we
                                                                 recommend setting [PRIO] to zero. The legal [PRIO] values are zero to nine
                                                                 when the shaping queue is used. In addition to priority, [PRIO] determines
                                                                 whether the shaping queue is a static queue or not: If [PRIO] equals the
                                                                 parent's NIX_AF_TL*()_TOPOLOGY[RR_PRIO], then this is a round-robin child
                                                                 queue into the shaper at the next level. */
        uint64_t rr_quantum            : 24; /**< [ 23:  0](R/W) Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                                 integer). The packet size used in all DWRR (RR_COUNT) calculations is:

                                                                 _  (NIX_nm_SHAPE[LENGTH_DISABLE] ? 0 : (NIX_nm_MD*[LENGTH] + NIX_nm_MD*[ADJUST]))
                                                                    + NIX_nm_SHAPE[ADJUST]

                                                                 where nm corresponds to this NIX_nm_SCHEDULE CSR.

                                                                 Typically [RR_QUANTUM] should be at or near the MTU or more (to limit or prevent
                                                                 negative accumulations of the deficit count). */
#else /* Word 0 - Little Endian */
        uint64_t rr_quantum            : 24; /**< [ 23:  0](R/W) Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                                 integer). The packet size used in all DWRR (RR_COUNT) calculations is:

                                                                 _  (NIX_nm_SHAPE[LENGTH_DISABLE] ? 0 : (NIX_nm_MD*[LENGTH] + NIX_nm_MD*[ADJUST]))
                                                                    + NIX_nm_SHAPE[ADJUST]

                                                                 where nm corresponds to this NIX_nm_SCHEDULE CSR.

                                                                 Typically [RR_QUANTUM] should be at or near the MTU or more (to limit or prevent
                                                                 negative accumulations of the deficit count). */
        uint64_t prio                  : 4;  /**< [ 27: 24](R/W) Priority. The priority used for this shaping queue in the (lower-level)
                                                                 parent's scheduling algorithm. When this shaping queue is not used, we
                                                                 recommend setting [PRIO] to zero. The legal [PRIO] values are zero to nine
                                                                 when the shaping queue is used. In addition to priority, [PRIO] determines
                                                                 whether the shaping queue is a static queue or not: If [PRIO] equals the
                                                                 parent's NIX_AF_TL*()_TOPOLOGY[RR_PRIO], then this is a round-robin child
                                                                 queue into the shaper at the next level. */
        uint64_t reserved_28_63        : 36;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_schedule_s cn; */
};
typedef union bdk_nixx_af_mdqx_schedule bdk_nixx_af_mdqx_schedule_t;

static inline uint64_t BDK_NIXX_AF_MDQX_SCHEDULE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_SCHEDULE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001400ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_SCHEDULE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_SCHEDULE(a,b) bdk_nixx_af_mdqx_schedule_t
#define bustype_BDK_NIXX_AF_MDQX_SCHEDULE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_SCHEDULE(a,b) "NIXX_AF_MDQX_SCHEDULE"
#define device_bar_BDK_NIXX_AF_MDQX_SCHEDULE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_SCHEDULE(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_SCHEDULE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_shape
 *
 * NIX AF Meta Descriptor Queue Shaping Control Registers
 * This register has the same bit fields as NIX_AF_TL3()_SHAPE.
 */
union bdk_nixx_af_mdqx_shape
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_shape_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_27_63        : 37;
        uint64_t schedule_list         : 2;  /**< [ 26: 25](R/W) Shaper scheduling list. Restricts shaper scheduling to specific lists.
                                                                   0x0 = Normal (selected for nearly all scheduling/shaping applications).
                                                                   0x1 = Green-only.
                                                                   0x2 = Yellow-only.
                                                                   0x3 = Red-only. */
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t reserved_13_23        : 11;
        uint64_t yellow_disable        : 1;  /**< [ 12: 12](R/W) See NIX_AF_TL2()_SHAPE[YELLOW_DISABLE]. */
        uint64_t red_disable           : 1;  /**< [ 11: 11](R/W) See NIX_AF_TL2()_SHAPE[RED_DISABLE]. */
        uint64_t red_algo              : 2;  /**< [ 10:  9](R/W) See NIX_AF_TL2()_SHAPE[RED_ALGO]. */
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) See NIX_AF_TL2()_SHAPE[ADJUST]. */
#else /* Word 0 - Little Endian */
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) See NIX_AF_TL2()_SHAPE[ADJUST]. */
        uint64_t red_algo              : 2;  /**< [ 10:  9](R/W) See NIX_AF_TL2()_SHAPE[RED_ALGO]. */
        uint64_t red_disable           : 1;  /**< [ 11: 11](R/W) See NIX_AF_TL2()_SHAPE[RED_DISABLE]. */
        uint64_t yellow_disable        : 1;  /**< [ 12: 12](R/W) See NIX_AF_TL2()_SHAPE[YELLOW_DISABLE]. */
        uint64_t reserved_13_23        : 11;
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t schedule_list         : 2;  /**< [ 26: 25](R/W) Shaper scheduling list. Restricts shaper scheduling to specific lists.
                                                                   0x0 = Normal (selected for nearly all scheduling/shaping applications).
                                                                   0x1 = Green-only.
                                                                   0x2 = Yellow-only.
                                                                   0x3 = Red-only. */
        uint64_t reserved_27_63        : 37;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_shape_s cn; */
};
typedef union bdk_nixx_af_mdqx_shape bdk_nixx_af_mdqx_shape_t;

static inline uint64_t BDK_NIXX_AF_MDQX_SHAPE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_SHAPE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001410ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_SHAPE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_SHAPE(a,b) bdk_nixx_af_mdqx_shape_t
#define bustype_BDK_NIXX_AF_MDQX_SHAPE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_SHAPE(a,b) "NIXX_AF_MDQX_SHAPE"
#define device_bar_BDK_NIXX_AF_MDQX_SHAPE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_SHAPE(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_SHAPE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_shape_state
 *
 * NIX AF Meta Descriptor Queue Shaping State Registers
 * This register has the same bit fields as NIX_AF_TL2()_SHAPE_STATE.
 * This register must not be written during normal operation.
 */
union bdk_nixx_af_mdqx_shape_state
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_shape_state_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_54_63        : 10;
        uint64_t color                 : 2;  /**< [ 53: 52](R/W/H) Shaper connection status. Debug access to the live shaper state.
                                                                 0x0 = Green - shaper is connected into the green list.
                                                                 0x1 = Yellow - shaper is connected into the yellow list.
                                                                 0x2 = Red - shaper is connected into the red list.
                                                                 0x3 = Pruned - shaper is disconnected. */
        uint64_t pir_accum             : 26; /**< [ 51: 26](R/W/H) Peak information rate accumulator. Debug access to the live PIR accumulator. */
        uint64_t cir_accum             : 26; /**< [ 25:  0](R/W/H) Committed information rate accumulator. Debug access to the live CIR accumulator. */
#else /* Word 0 - Little Endian */
        uint64_t cir_accum             : 26; /**< [ 25:  0](R/W/H) Committed information rate accumulator. Debug access to the live CIR accumulator. */
        uint64_t pir_accum             : 26; /**< [ 51: 26](R/W/H) Peak information rate accumulator. Debug access to the live PIR accumulator. */
        uint64_t color                 : 2;  /**< [ 53: 52](R/W/H) Shaper connection status. Debug access to the live shaper state.
                                                                 0x0 = Green - shaper is connected into the green list.
                                                                 0x1 = Yellow - shaper is connected into the yellow list.
                                                                 0x2 = Red - shaper is connected into the red list.
                                                                 0x3 = Pruned - shaper is disconnected. */
        uint64_t reserved_54_63        : 10;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_shape_state_s cn; */
};
typedef union bdk_nixx_af_mdqx_shape_state bdk_nixx_af_mdqx_shape_state_t;

static inline uint64_t BDK_NIXX_AF_MDQX_SHAPE_STATE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_SHAPE_STATE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001450ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_SHAPE_STATE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_SHAPE_STATE(a,b) bdk_nixx_af_mdqx_shape_state_t
#define bustype_BDK_NIXX_AF_MDQX_SHAPE_STATE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_SHAPE_STATE(a,b) "NIXX_AF_MDQX_SHAPE_STATE"
#define device_bar_BDK_NIXX_AF_MDQX_SHAPE_STATE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_SHAPE_STATE(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_SHAPE_STATE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq#_sw_xoff
 *
 * NIX AF Meta Descriptor Controlled XOFF Registers
 * This register has the same bit fields as NIX_AF_TL1()_SW_XOFF
 */
union bdk_nixx_af_mdqx_sw_xoff
{
    uint64_t u;
    struct bdk_nixx_af_mdqx_sw_xoff_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_4_63         : 60;
        uint64_t drain_irq             : 1;  /**< [  3:  3](WO) Drain IRQ. Enables setting of NIX_AF_GEN_INT[TL1_DRAIN] when the drain
                                                                 operation has completed.
                                                                 [DRAIN_IRQ] should be set whenever [DRAIN] is, and must not be set when [DRAIN] isn't
                                                                 set. [DRAIN_IRQ] has no effect unless [DRAIN] and [XOFF] are also set. */
        uint64_t reserved_2            : 1;
        uint64_t drain                 : 1;  /**< [  1:  1](WO) Drain. This control activates a drain path through the PSE that starts at this queue
                                                                 and ends at the TL1 level. The drain path is prioritized over other paths through PSE
                                                                 and can be used in combination with [DRAIN_IRQ]. [DRAIN] need never be set for the
                                                                 TL1 level, but is useful at all other levels, including the TL4 and MDQ levels.
                                                                 NIX_AF_GEN_INT[TL1_DRAIN] should be clear prior to initiating a [DRAIN]=1 write to
                                                                 this CSR.

                                                                 After [DRAIN] is set for a shaping queue, it should not be set again, for
                                                                 this or any other shaping queue, until NIX_AF_GEN_INT[TL1_DRAIN] is set.

                                                                 [DRAIN] must not be set for any shaping queue when an SMQ FLUSH command is
                                                                 active (any NIX_AF_SMQ()_CFG[FLUSH] is set).

                                                                 DRAIN has no effect unless XOFF is also set. Only one DRAIN command is
                                                                 allowed to be active at a time. */
        uint64_t xoff                  : 1;  /**< [  0:  0](R/W) XOFF. Stops meta flow out of the TL1 shaping queue. When [XOFF] is set, the
                                                                 corresponding NIX_AF_TL*()_MD_DEBUG* (i.e. the meta descriptor) in the TL1
                                                                 shaping queue cannot be transferred to the next level. */
#else /* Word 0 - Little Endian */
        uint64_t xoff                  : 1;  /**< [  0:  0](R/W) XOFF. Stops meta flow out of the TL1 shaping queue. When [XOFF] is set, the
                                                                 corresponding NIX_AF_TL*()_MD_DEBUG* (i.e. the meta descriptor) in the TL1
                                                                 shaping queue cannot be transferred to the next level. */
        uint64_t drain                 : 1;  /**< [  1:  1](WO) Drain. This control activates a drain path through the PSE that starts at this queue
                                                                 and ends at the TL1 level. The drain path is prioritized over other paths through PSE
                                                                 and can be used in combination with [DRAIN_IRQ]. [DRAIN] need never be set for the
                                                                 TL1 level, but is useful at all other levels, including the TL4 and MDQ levels.
                                                                 NIX_AF_GEN_INT[TL1_DRAIN] should be clear prior to initiating a [DRAIN]=1 write to
                                                                 this CSR.

                                                                 After [DRAIN] is set for a shaping queue, it should not be set again, for
                                                                 this or any other shaping queue, until NIX_AF_GEN_INT[TL1_DRAIN] is set.

                                                                 [DRAIN] must not be set for any shaping queue when an SMQ FLUSH command is
                                                                 active (any NIX_AF_SMQ()_CFG[FLUSH] is set).

                                                                 DRAIN has no effect unless XOFF is also set. Only one DRAIN command is
                                                                 allowed to be active at a time. */
        uint64_t reserved_2            : 1;
        uint64_t drain_irq             : 1;  /**< [  3:  3](WO) Drain IRQ. Enables setting of NIX_AF_GEN_INT[TL1_DRAIN] when the drain
                                                                 operation has completed.
                                                                 [DRAIN_IRQ] should be set whenever [DRAIN] is, and must not be set when [DRAIN] isn't
                                                                 set. [DRAIN_IRQ] has no effect unless [DRAIN] and [XOFF] are also set. */
        uint64_t reserved_4_63         : 60;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqx_sw_xoff_s cn; */
};
typedef union bdk_nixx_af_mdqx_sw_xoff bdk_nixx_af_mdqx_sw_xoff_t;

static inline uint64_t BDK_NIXX_AF_MDQX_SW_XOFF(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQX_SW_XOFF(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001470ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_MDQX_SW_XOFF", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQX_SW_XOFF(a,b) bdk_nixx_af_mdqx_sw_xoff_t
#define bustype_BDK_NIXX_AF_MDQX_SW_XOFF(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQX_SW_XOFF(a,b) "NIXX_AF_MDQX_SW_XOFF"
#define device_bar_BDK_NIXX_AF_MDQX_SW_XOFF(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQX_SW_XOFF(a,b) (a)
#define arguments_BDK_NIXX_AF_MDQX_SW_XOFF(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdq_const
 *
 * NIX AF Meta Descriptor Queue Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_mdq_const
{
    uint64_t u;
    struct bdk_nixx_af_mdq_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_16_63        : 48;
        uint64_t count                 : 16; /**< [ 15:  0](RO) Number of meta descriptor queues and SMQs. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 16; /**< [ 15:  0](RO) Number of meta descriptor queues and SMQs. */
        uint64_t reserved_16_63        : 48;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdq_const_s cn; */
};
typedef union bdk_nixx_af_mdq_const bdk_nixx_af_mdq_const_t;

static inline uint64_t BDK_NIXX_AF_MDQ_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQ_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000090ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_MDQ_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQ_CONST(a) bdk_nixx_af_mdq_const_t
#define bustype_BDK_NIXX_AF_MDQ_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQ_CONST(a) "NIXX_AF_MDQ_CONST"
#define device_bar_BDK_NIXX_AF_MDQ_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQ_CONST(a) (a)
#define arguments_BDK_NIXX_AF_MDQ_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdqa_debug
 *
 * INTERNAL: NIX MDQ Internal Debug Register
 */
union bdk_nixx_af_mdqa_debug
{
    uint64_t u;
    struct bdk_nixx_af_mdqa_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#else /* Word 0 - Little Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqa_debug_s cn; */
};
typedef union bdk_nixx_af_mdqa_debug bdk_nixx_af_mdqa_debug_t;

static inline uint64_t BDK_NIXX_AF_MDQA_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQA_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400014e0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_MDQA_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQA_DEBUG(a) bdk_nixx_af_mdqa_debug_t
#define bustype_BDK_NIXX_AF_MDQA_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQA_DEBUG(a) "NIXX_AF_MDQA_DEBUG"
#define device_bar_BDK_NIXX_AF_MDQA_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQA_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_MDQA_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_mdqb_debug
 *
 * INTERNAL: NIX MDQ Internal Debug Register
 */
union bdk_nixx_af_mdqb_debug
{
    uint64_t u;
    struct bdk_nixx_af_mdqb_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#else /* Word 0 - Little Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_mdqb_debug_s cn; */
};
typedef union bdk_nixx_af_mdqb_debug bdk_nixx_af_mdqb_debug_t;

static inline uint64_t BDK_NIXX_AF_MDQB_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_MDQB_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400014f0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_MDQB_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_MDQB_DEBUG(a) bdk_nixx_af_mdqb_debug_t
#define bustype_BDK_NIXX_AF_MDQB_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_MDQB_DEBUG(a) "NIXX_AF_MDQB_DEBUG"
#define device_bar_BDK_NIXX_AF_MDQB_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_MDQB_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_MDQB_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_ndc_cfg
 *
 * NIX AF General Configuration Register
 */
union bdk_nixx_af_ndc_cfg
{
    uint64_t u;
    struct bdk_nixx_af_ndc_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t force_ndc_bypass      : 1;  /**< [  0:  0](R/W) Forces all NDC transations to Bypass the NDC cache. */
#else /* Word 0 - Little Endian */
        uint64_t force_ndc_bypass      : 1;  /**< [  0:  0](R/W) Forces all NDC transations to Bypass the NDC cache. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_ndc_cfg_s cn; */
};
typedef union bdk_nixx_af_ndc_cfg bdk_nixx_af_ndc_cfg_t;

static inline uint64_t BDK_NIXX_AF_NDC_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_NDC_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000018ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_NDC_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_NDC_CFG(a) bdk_nixx_af_ndc_cfg_t
#define bustype_BDK_NIXX_AF_NDC_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_NDC_CFG(a) "NIXX_AF_NDC_CFG"
#define device_bar_BDK_NIXX_AF_NDC_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_NDC_CFG(a) (a)
#define arguments_BDK_NIXX_AF_NDC_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_norm_tx_fifo_status
 *
 * NIX AF Normal Transmit FIFO Status Register
 * Status of FIFO which transmits normal (potentially preemptable) packets to CGX
 * and LBK.
 */
union bdk_nixx_af_norm_tx_fifo_status
{
    uint64_t u;
    struct bdk_nixx_af_norm_tx_fifo_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_12_63        : 52;
        uint64_t count                 : 12; /**< [ 11:  0](RO/H) Number of 128-bit entries in the TX FIFO. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 12; /**< [ 11:  0](RO/H) Number of 128-bit entries in the TX FIFO. */
        uint64_t reserved_12_63        : 52;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_norm_tx_fifo_status_s cn; */
};
typedef union bdk_nixx_af_norm_tx_fifo_status bdk_nixx_af_norm_tx_fifo_status_t;

static inline uint64_t BDK_NIXX_AF_NORM_TX_FIFO_STATUS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_NORM_TX_FIFO_STATUS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000620ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_NORM_TX_FIFO_STATUS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_NORM_TX_FIFO_STATUS(a) bdk_nixx_af_norm_tx_fifo_status_t
#define bustype_BDK_NIXX_AF_NORM_TX_FIFO_STATUS(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_NORM_TX_FIFO_STATUS(a) "NIXX_AF_NORM_TX_FIFO_STATUS"
#define device_bar_BDK_NIXX_AF_NORM_TX_FIFO_STATUS(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_NORM_TX_FIFO_STATUS(a) (a)
#define arguments_BDK_NIXX_AF_NORM_TX_FIFO_STATUS(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_npc_capture_config
 *
 * NIX AF NPC Response Capture Configuration Register
 * This register configures the NPC response capture logic. When enabled, this feature
 * will allow the NPC responses for selected packets to be captured for debug purposes
 * in NIX_AF_DEBUG_NPC_RESP_DATA().
 */
union bdk_nixx_af_npc_capture_config
{
    uint64_t u;
    struct bdk_nixx_af_npc_capture_config_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t lf_id                 : 8;  /**< [ 63: 56](R/W) LF to be matched, when enabled by [LF_ID_EN], to trigger an NPC response capture. */
        uint64_t sq_id                 : 20; /**< [ 55: 36](R/W) SQ to be matched, when enabled by [SQ_ID_EN], to trigger an NPC response capture. */
        uint64_t sqe_id                : 16; /**< [ 35: 20](R/W) SQE to be matched, when enabled by [SQE_ID_EN], to trigger an NPC response capture. */
        uint64_t lso_segnum            : 8;  /**< [ 19: 12](R/W) LSO segment number to be matched, when enabled by [LSO_SEGNUM_EN], to trigger
                                                                 an NPC response capture. */
        uint64_t reserved_6_11         : 6;
        uint64_t lf_id_en              : 1;  /**< [  5:  5](R/W) Enable for LF matching, to trigger an NPC response capture. When one, an NPC
                                                                 response will be captured for a packet whose LF matches [LF_ID]. */
        uint64_t sq_id_en              : 1;  /**< [  4:  4](R/W) Enable for SQ matching, to trigger an NPC response capture. When one, an NPC
                                                                 response will be captured for a packet whose SQ matches [LF_ID]. */
        uint64_t sqe_id_en             : 1;  /**< [  3:  3](R/W) Enable for SQE ID matching, to trigger an NPC response capture. When one, an NPC
                                                                 response will be captured for a packet whose SQE ID matches [SQE_ID]. */
        uint64_t lso_segnum_en         : 1;  /**< [  2:  2](R/W) Enable for LSO segment number  matching, to trigger an NPC response capture.
                                                                 When one, an NPC response will be captured for a packet whose LSO segment number
                                                                 matches [LSO_SEGNUM]. */
        uint64_t continuous            : 1;  /**< [  1:  1](R/W) Enable for continuous mode of NPC response capturing. When one, the NPC response
                                                                 for any matching packet will be captured. When 0, only the first matching
                                                                 packet will be captured. */
        uint64_t en                    : 1;  /**< [  0:  0](R/W) Enable for NPC response capturing. When one, NPC response capturing will be
                                                                 performed. When 0, NPC responses will not be captured. */
#else /* Word 0 - Little Endian */
        uint64_t en                    : 1;  /**< [  0:  0](R/W) Enable for NPC response capturing. When one, NPC response capturing will be
                                                                 performed. When 0, NPC responses will not be captured. */
        uint64_t continuous            : 1;  /**< [  1:  1](R/W) Enable for continuous mode of NPC response capturing. When one, the NPC response
                                                                 for any matching packet will be captured. When 0, only the first matching
                                                                 packet will be captured. */
        uint64_t lso_segnum_en         : 1;  /**< [  2:  2](R/W) Enable for LSO segment number  matching, to trigger an NPC response capture.
                                                                 When one, an NPC response will be captured for a packet whose LSO segment number
                                                                 matches [LSO_SEGNUM]. */
        uint64_t sqe_id_en             : 1;  /**< [  3:  3](R/W) Enable for SQE ID matching, to trigger an NPC response capture. When one, an NPC
                                                                 response will be captured for a packet whose SQE ID matches [SQE_ID]. */
        uint64_t sq_id_en              : 1;  /**< [  4:  4](R/W) Enable for SQ matching, to trigger an NPC response capture. When one, an NPC
                                                                 response will be captured for a packet whose SQ matches [LF_ID]. */
        uint64_t lf_id_en              : 1;  /**< [  5:  5](R/W) Enable for LF matching, to trigger an NPC response capture. When one, an NPC
                                                                 response will be captured for a packet whose LF matches [LF_ID]. */
        uint64_t reserved_6_11         : 6;
        uint64_t lso_segnum            : 8;  /**< [ 19: 12](R/W) LSO segment number to be matched, when enabled by [LSO_SEGNUM_EN], to trigger
                                                                 an NPC response capture. */
        uint64_t sqe_id                : 16; /**< [ 35: 20](R/W) SQE to be matched, when enabled by [SQE_ID_EN], to trigger an NPC response capture. */
        uint64_t sq_id                 : 20; /**< [ 55: 36](R/W) SQ to be matched, when enabled by [SQ_ID_EN], to trigger an NPC response capture. */
        uint64_t lf_id                 : 8;  /**< [ 63: 56](R/W) LF to be matched, when enabled by [LF_ID_EN], to trigger an NPC response capture. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_npc_capture_config_s cn; */
};
typedef union bdk_nixx_af_npc_capture_config bdk_nixx_af_npc_capture_config_t;

static inline uint64_t BDK_NIXX_AF_NPC_CAPTURE_CONFIG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_NPC_CAPTURE_CONFIG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000660ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_NPC_CAPTURE_CONFIG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_NPC_CAPTURE_CONFIG(a) bdk_nixx_af_npc_capture_config_t
#define bustype_BDK_NIXX_AF_NPC_CAPTURE_CONFIG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_NPC_CAPTURE_CONFIG(a) "NIXX_AF_NPC_CAPTURE_CONFIG"
#define device_bar_BDK_NIXX_AF_NPC_CAPTURE_CONFIG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_NPC_CAPTURE_CONFIG(a) (a)
#define arguments_BDK_NIXX_AF_NPC_CAPTURE_CONFIG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_npc_capture_info
 *
 * NIX AF NPC Response Capture Information Register
 * This register contains information regarding the captured NPC response.
 */
union bdk_nixx_af_npc_capture_info
{
    uint64_t u;
    struct bdk_nixx_af_npc_capture_info_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t lf_id                 : 8;  /**< [ 63: 56](R/W/H) LF ID corresponding to the captured packet. */
        uint64_t sq_id                 : 20; /**< [ 55: 36](R/W/H) SQ ID corresponding to the captured packet. */
        uint64_t sqe_id                : 16; /**< [ 35: 20](R/W/H) SQE ID corresponding to the captured packet. */
        uint64_t lso_segnum            : 8;  /**< [ 19: 12](R/W/H) LSO segment number corresponding to the captured packet. */
        uint64_t reserved_1_11         : 11;
        uint64_t vld                   : 1;  /**< [  0:  0](R/W/H) When one, indicates a valid NPC response has been captured. */
#else /* Word 0 - Little Endian */
        uint64_t vld                   : 1;  /**< [  0:  0](R/W/H) When one, indicates a valid NPC response has been captured. */
        uint64_t reserved_1_11         : 11;
        uint64_t lso_segnum            : 8;  /**< [ 19: 12](R/W/H) LSO segment number corresponding to the captured packet. */
        uint64_t sqe_id                : 16; /**< [ 35: 20](R/W/H) SQE ID corresponding to the captured packet. */
        uint64_t sq_id                 : 20; /**< [ 55: 36](R/W/H) SQ ID corresponding to the captured packet. */
        uint64_t lf_id                 : 8;  /**< [ 63: 56](R/W/H) LF ID corresponding to the captured packet. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_npc_capture_info_s cn; */
};
typedef union bdk_nixx_af_npc_capture_info bdk_nixx_af_npc_capture_info_t;

static inline uint64_t BDK_NIXX_AF_NPC_CAPTURE_INFO(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_NPC_CAPTURE_INFO(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000670ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_NPC_CAPTURE_INFO", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_NPC_CAPTURE_INFO(a) bdk_nixx_af_npc_capture_info_t
#define bustype_BDK_NIXX_AF_NPC_CAPTURE_INFO(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_NPC_CAPTURE_INFO(a) "NIXX_AF_NPC_CAPTURE_INFO"
#define device_bar_BDK_NIXX_AF_NPC_CAPTURE_INFO(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_NPC_CAPTURE_INFO(a) (a)
#define arguments_BDK_NIXX_AF_NPC_CAPTURE_INFO(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_pse_channel_level
 *
 * NIX AF PSE Channel Level Register
 */
union bdk_nixx_af_pse_channel_level
{
    uint64_t u;
    struct bdk_nixx_af_pse_channel_level_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t bp_level              : 1;  /**< [  0:  0](R/W) Channel and link backpressure level. Channels and links can be configured to backpressure
                                                                 at level 2 or 3 of the PSE hierarchy.
                                                                 0 = Selects TL2 as the channel level.
                                                                 1 = Selects TL3 as the channel level.

                                                                 [BP_LEVEL] determines whether NIX_AF_TL3_TL2()_LINK()_CFG registers are associated with
                                                                 the TL3 or TL2 shaping queues. Likewise, [BP_LEVEL] determines whether link credits
                                                                 (NIX_AF_TX_LINK()_NORM_CREDIT and NIX_AF_TX_LINK()_EXPR_CREDIT) are managed at the TL3
                                                                 or TL2 level. */
#else /* Word 0 - Little Endian */
        uint64_t bp_level              : 1;  /**< [  0:  0](R/W) Channel and link backpressure level. Channels and links can be configured to backpressure
                                                                 at level 2 or 3 of the PSE hierarchy.
                                                                 0 = Selects TL2 as the channel level.
                                                                 1 = Selects TL3 as the channel level.

                                                                 [BP_LEVEL] determines whether NIX_AF_TL3_TL2()_LINK()_CFG registers are associated with
                                                                 the TL3 or TL2 shaping queues. Likewise, [BP_LEVEL] determines whether link credits
                                                                 (NIX_AF_TX_LINK()_NORM_CREDIT and NIX_AF_TX_LINK()_EXPR_CREDIT) are managed at the TL3
                                                                 or TL2 level. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_pse_channel_level_s cn; */
};
typedef union bdk_nixx_af_pse_channel_level bdk_nixx_af_pse_channel_level_t;

static inline uint64_t BDK_NIXX_AF_PSE_CHANNEL_LEVEL(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_PSE_CHANNEL_LEVEL(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000800ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_PSE_CHANNEL_LEVEL", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_PSE_CHANNEL_LEVEL(a) bdk_nixx_af_pse_channel_level_t
#define bustype_BDK_NIXX_AF_PSE_CHANNEL_LEVEL(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_PSE_CHANNEL_LEVEL(a) "NIXX_AF_PSE_CHANNEL_LEVEL"
#define device_bar_BDK_NIXX_AF_PSE_CHANNEL_LEVEL(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_PSE_CHANNEL_LEVEL(a) (a)
#define arguments_BDK_NIXX_AF_PSE_CHANNEL_LEVEL(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_pse_const
 *
 * NIX AF PSE Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_pse_const
{
    uint64_t u;
    struct bdk_nixx_af_pse_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_16_63        : 48;
        uint64_t formats               : 8;  /**< [ 15:  8](RO) Number of NIX_AF_MARK_FORMAT()_CTL registers. */
        uint64_t reserved_4_7          : 4;
        uint64_t levels                : 4;  /**< [  3:  0](RO) Number of hierarchical transmit shaping levels. */
#else /* Word 0 - Little Endian */
        uint64_t levels                : 4;  /**< [  3:  0](RO) Number of hierarchical transmit shaping levels. */
        uint64_t reserved_4_7          : 4;
        uint64_t formats               : 8;  /**< [ 15:  8](RO) Number of NIX_AF_MARK_FORMAT()_CTL registers. */
        uint64_t reserved_16_63        : 48;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_pse_const_s cn; */
};
typedef union bdk_nixx_af_pse_const bdk_nixx_af_pse_const_t;

static inline uint64_t BDK_NIXX_AF_PSE_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_PSE_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000060ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_PSE_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_PSE_CONST(a) bdk_nixx_af_pse_const_t
#define bustype_BDK_NIXX_AF_PSE_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_PSE_CONST(a) "NIXX_AF_PSE_CONST"
#define device_bar_BDK_NIXX_AF_PSE_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_PSE_CONST(a) (a)
#define arguments_BDK_NIXX_AF_PSE_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_pse_shaper_cfg
 *
 * NIX AF PSE Shaper Configuration Register
 */
union bdk_nixx_af_pse_shaper_cfg
{
    uint64_t u;
    struct bdk_nixx_af_pse_shaper_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_2_63         : 62;
        uint64_t color_aware           : 1;  /**< [  1:  1](R/W) Color aware. Selects whether or not the PSE shapers take into account
                                                                 the color of the incoming packet.
                                                                 0 = Color blind.
                                                                 1 = Color aware. */
        uint64_t red_send_as_yellow    : 1;  /**< [  0:  0](R/W) Red send as yellow. Configures the way packets colored
                                                                 NIX_COLORRESULT_E::RED_SEND are handled by the TL4 through TL2 shapers when
                                                                 operating in [COLOR_AWARE] mode. Normally packets colored
                                                                 NIX_COLORRESULT_E::RED_DROP do not decrement the PIR in TL4 through TL2
                                                                 shapers while packets colored YELLOW do. (Neither
                                                                 NIX_COLORRESULT_E::RED_DROP nor YELLOW packets decrement the CIR in TL4
                                                                 through TL2 shapers.)  Packets colored NIX_COLORRESULT_E::RED_SEND are
                                                                 treated as either NIX_COLORRESULT_E::RED_DROP or YELLOW in the TL4 through
                                                                 TL2 shapers as follows:
                                                                 0 = Treat NIX_COLORRESULT_E::RED_SEND as NIX_COLORRESULT_E::RED_DROP.
                                                                 1 = Treat NIX_COLORRESULT_E::RED_SEND as YELLOW.

                                                                 In the TL1 shapers, NIX_COLORRESULT_E::RED_DROP packets do not decrement
                                                                 the CIR, while YELLOW do. NIX_COLORRESULT_E::RED_SEND packets are always
                                                                 treated the same as YELLOW is in the TL1 shapers, irrespective of
                                                                 [RED_SEND_AS_YELLOW]. */
#else /* Word 0 - Little Endian */
        uint64_t red_send_as_yellow    : 1;  /**< [  0:  0](R/W) Red send as yellow. Configures the way packets colored
                                                                 NIX_COLORRESULT_E::RED_SEND are handled by the TL4 through TL2 shapers when
                                                                 operating in [COLOR_AWARE] mode. Normally packets colored
                                                                 NIX_COLORRESULT_E::RED_DROP do not decrement the PIR in TL4 through TL2
                                                                 shapers while packets colored YELLOW do. (Neither
                                                                 NIX_COLORRESULT_E::RED_DROP nor YELLOW packets decrement the CIR in TL4
                                                                 through TL2 shapers.)  Packets colored NIX_COLORRESULT_E::RED_SEND are
                                                                 treated as either NIX_COLORRESULT_E::RED_DROP or YELLOW in the TL4 through
                                                                 TL2 shapers as follows:
                                                                 0 = Treat NIX_COLORRESULT_E::RED_SEND as NIX_COLORRESULT_E::RED_DROP.
                                                                 1 = Treat NIX_COLORRESULT_E::RED_SEND as YELLOW.

                                                                 In the TL1 shapers, NIX_COLORRESULT_E::RED_DROP packets do not decrement
                                                                 the CIR, while YELLOW do. NIX_COLORRESULT_E::RED_SEND packets are always
                                                                 treated the same as YELLOW is in the TL1 shapers, irrespective of
                                                                 [RED_SEND_AS_YELLOW]. */
        uint64_t color_aware           : 1;  /**< [  1:  1](R/W) Color aware. Selects whether or not the PSE shapers take into account
                                                                 the color of the incoming packet.
                                                                 0 = Color blind.
                                                                 1 = Color aware. */
        uint64_t reserved_2_63         : 62;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_pse_shaper_cfg_s cn; */
};
typedef union bdk_nixx_af_pse_shaper_cfg bdk_nixx_af_pse_shaper_cfg_t;

static inline uint64_t BDK_NIXX_AF_PSE_SHAPER_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_PSE_SHAPER_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000810ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_PSE_SHAPER_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_PSE_SHAPER_CFG(a) bdk_nixx_af_pse_shaper_cfg_t
#define bustype_BDK_NIXX_AF_PSE_SHAPER_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_PSE_SHAPER_CFG(a) "NIXX_AF_PSE_SHAPER_CFG"
#define device_bar_BDK_NIXX_AF_PSE_SHAPER_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_PSE_SHAPER_CFG(a) (a)
#define arguments_BDK_NIXX_AF_PSE_SHAPER_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_ras
 *
 * NIX AF RAS Interrupt Register
 * This register is intended for delivery of RAS events to the SCP, so should be
 * ignored by OS drivers.
 */
union bdk_nixx_af_ras
{
    uint64_t u;
    struct bdk_nixx_af_ras_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_35_63        : 29;
        uint64_t aq_inst_poison        : 1;  /**< [ 34: 34](R/W1C/H) Poisoned data returned on NIX_AQ_INST_S read. Hardware also sets
                                                                 NIX_AF_AQ_STATUS[AQ_ERR]. */
        uint64_t aq_res_poison         : 1;  /**< [ 33: 33](R/W1C/H) Poisoned read data returned following NIX_AQ_RES_S. Hardware also sets
                                                                 NIX_AF_AQ_STATUS[AQ_ERR]. */
        uint64_t aq_ctx_poison         : 1;  /**< [ 32: 32](R/W1C/H) Poisoned data returned on read of hardware context data selected by
                                                                 NIX_AQ_INST_S[LF,CTYPE,CINDEX]. Hardware also returns
                                                                 NIX_AQ_RES_S[COMPCODE] = NIX_AQ_COMP_E::CTX_POISON. */
        uint64_t reserved_15_31        : 17;
        uint64_t cint_poison           : 1;  /**< [ 14: 14](R/W1C/H) Poisoned data returned on NIX_CINT_HW_S read. */
        uint64_t qint_poison           : 1;  /**< [ 13: 13](R/W1C/H) Poisoned data returned on NIX_QINT_HW_S read. */
        uint64_t rx_mirror_data_poison : 1;  /**< [ 12: 12](R/W1C/H) Poisoned data returned on packet data read from a mirror buffer. */
        uint64_t rx_mcast_data_poison  : 1;  /**< [ 11: 11](R/W1C/H) Poisoned data returned on packet data read from a multicast buffer. */
        uint64_t rx_mirror_wqe_poison  : 1;  /**< [ 10: 10](R/W1C/H) Poisoned data returned on WQE read from a mirror buffer. */
        uint64_t rx_mcast_wqe_poison   : 1;  /**< [  9:  9](R/W1C/H) Poisoned data returned on WQE read from a multicast buffer. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1C/H) Poisoned data returned on packet data read for NIX_SEND_SG_S. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1C/H) Poisoned data returned on send descriptor read at or beyond
                                                                 NIX_SEND_JUMP_S[ADDR]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1C/H) Poisoned data returned on IPSEC dynamic ordering counter read. See
                                                                 NIX_AF_LF()_RX_IPSEC_DYNO_CFG. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1C/H) Poisoned data returned on NIX_RSSE_S read. */
        uint64_t rx_mce_poison         : 1;  /**< [  4:  4](R/W1C/H) Poisoned data returned on NIX_RX_MCE_S read. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1C/H) Poisoned data returned on NIX_CQ_CTX_S read. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1C/H) Poisoned data returned on NIX_RQ_CTX_S read. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1C/H) Poisoned data returned on NIX_SQ_CTX_HW_S read. */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1C/H) Poisoned data returned on SQB read. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1C/H) Poisoned data returned on SQB read. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1C/H) Poisoned data returned on NIX_SQ_CTX_HW_S read. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1C/H) Poisoned data returned on NIX_RQ_CTX_S read. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1C/H) Poisoned data returned on NIX_CQ_CTX_S read. */
        uint64_t rx_mce_poison         : 1;  /**< [  4:  4](R/W1C/H) Poisoned data returned on NIX_RX_MCE_S read. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1C/H) Poisoned data returned on NIX_RSSE_S read. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1C/H) Poisoned data returned on IPSEC dynamic ordering counter read. See
                                                                 NIX_AF_LF()_RX_IPSEC_DYNO_CFG. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1C/H) Poisoned data returned on send descriptor read at or beyond
                                                                 NIX_SEND_JUMP_S[ADDR]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1C/H) Poisoned data returned on packet data read for NIX_SEND_SG_S. */
        uint64_t rx_mcast_wqe_poison   : 1;  /**< [  9:  9](R/W1C/H) Poisoned data returned on WQE read from a multicast buffer. */
        uint64_t rx_mirror_wqe_poison  : 1;  /**< [ 10: 10](R/W1C/H) Poisoned data returned on WQE read from a mirror buffer. */
        uint64_t rx_mcast_data_poison  : 1;  /**< [ 11: 11](R/W1C/H) Poisoned data returned on packet data read from a multicast buffer. */
        uint64_t rx_mirror_data_poison : 1;  /**< [ 12: 12](R/W1C/H) Poisoned data returned on packet data read from a mirror buffer. */
        uint64_t qint_poison           : 1;  /**< [ 13: 13](R/W1C/H) Poisoned data returned on NIX_QINT_HW_S read. */
        uint64_t cint_poison           : 1;  /**< [ 14: 14](R/W1C/H) Poisoned data returned on NIX_CINT_HW_S read. */
        uint64_t reserved_15_31        : 17;
        uint64_t aq_ctx_poison         : 1;  /**< [ 32: 32](R/W1C/H) Poisoned data returned on read of hardware context data selected by
                                                                 NIX_AQ_INST_S[LF,CTYPE,CINDEX]. Hardware also returns
                                                                 NIX_AQ_RES_S[COMPCODE] = NIX_AQ_COMP_E::CTX_POISON. */
        uint64_t aq_res_poison         : 1;  /**< [ 33: 33](R/W1C/H) Poisoned read data returned following NIX_AQ_RES_S. Hardware also sets
                                                                 NIX_AF_AQ_STATUS[AQ_ERR]. */
        uint64_t aq_inst_poison        : 1;  /**< [ 34: 34](R/W1C/H) Poisoned data returned on NIX_AQ_INST_S read. Hardware also sets
                                                                 NIX_AF_AQ_STATUS[AQ_ERR]. */
        uint64_t reserved_35_63        : 29;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_ras_s cn; */
};
typedef union bdk_nixx_af_ras bdk_nixx_af_ras_t;

static inline uint64_t BDK_NIXX_AF_RAS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RAS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400001a0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RAS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RAS(a) bdk_nixx_af_ras_t
#define bustype_BDK_NIXX_AF_RAS(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RAS(a) "NIXX_AF_RAS"
#define device_bar_BDK_NIXX_AF_RAS(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RAS(a) (a)
#define arguments_BDK_NIXX_AF_RAS(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_ras_ena_w1c
 *
 * NIX AF RAS Interrupt Enable Clear Register
 * This register clears interrupt enable bits.
 */
union bdk_nixx_af_ras_ena_w1c
{
    uint64_t u;
    struct bdk_nixx_af_ras_ena_w1c_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_35_63        : 29;
        uint64_t aq_inst_poison        : 1;  /**< [ 34: 34](R/W1C/H) Reads or clears enable for NIX_AF_RAS[AQ_INST_POISON]. */
        uint64_t aq_res_poison         : 1;  /**< [ 33: 33](R/W1C/H) Reads or clears enable for NIX_AF_RAS[AQ_RES_POISON]. */
        uint64_t aq_ctx_poison         : 1;  /**< [ 32: 32](R/W1C/H) Reads or clears enable for NIX_AF_RAS[AQ_CTX_POISON]. */
        uint64_t reserved_15_31        : 17;
        uint64_t cint_poison           : 1;  /**< [ 14: 14](R/W1C/H) Reads or clears enable for NIX_AF_RAS[CINT_POISON]. */
        uint64_t qint_poison           : 1;  /**< [ 13: 13](R/W1C/H) Reads or clears enable for NIX_AF_RAS[QINT_POISON]. */
        uint64_t rx_mirror_data_poison : 1;  /**< [ 12: 12](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RX_MIRROR_DATA_POISON]. */
        uint64_t rx_mcast_data_poison  : 1;  /**< [ 11: 11](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RX_MCAST_DATA_POISON]. */
        uint64_t rx_mirror_wqe_poison  : 1;  /**< [ 10: 10](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RX_MIRROR_WQE_POISON]. */
        uint64_t rx_mcast_wqe_poison   : 1;  /**< [  9:  9](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RX_MCAST_WQE_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1C/H) Reads or clears enable for NIX_AF_RAS[SEND_SG_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1C/H) Reads or clears enable for NIX_AF_RAS[SEND_JUMP_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1C/H) Reads or clears enable for NIX_AF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RSSE_POISON]. */
        uint64_t rx_mce_poison         : 1;  /**< [  4:  4](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RX_MCE_POISON]. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1C/H) Reads or clears enable for NIX_AF_RAS[CQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RQ_CTX_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_AF_RAS[SQ_CTX_POISON]. */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_AF_RAS[SQB_POISON]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_AF_RAS[SQB_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_AF_RAS[SQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RQ_CTX_POISON]. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1C/H) Reads or clears enable for NIX_AF_RAS[CQ_CTX_POISON]. */
        uint64_t rx_mce_poison         : 1;  /**< [  4:  4](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RX_MCE_POISON]. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RSSE_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1C/H) Reads or clears enable for NIX_AF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1C/H) Reads or clears enable for NIX_AF_RAS[SEND_JUMP_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1C/H) Reads or clears enable for NIX_AF_RAS[SEND_SG_POISON]. */
        uint64_t rx_mcast_wqe_poison   : 1;  /**< [  9:  9](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RX_MCAST_WQE_POISON]. */
        uint64_t rx_mirror_wqe_poison  : 1;  /**< [ 10: 10](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RX_MIRROR_WQE_POISON]. */
        uint64_t rx_mcast_data_poison  : 1;  /**< [ 11: 11](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RX_MCAST_DATA_POISON]. */
        uint64_t rx_mirror_data_poison : 1;  /**< [ 12: 12](R/W1C/H) Reads or clears enable for NIX_AF_RAS[RX_MIRROR_DATA_POISON]. */
        uint64_t qint_poison           : 1;  /**< [ 13: 13](R/W1C/H) Reads or clears enable for NIX_AF_RAS[QINT_POISON]. */
        uint64_t cint_poison           : 1;  /**< [ 14: 14](R/W1C/H) Reads or clears enable for NIX_AF_RAS[CINT_POISON]. */
        uint64_t reserved_15_31        : 17;
        uint64_t aq_ctx_poison         : 1;  /**< [ 32: 32](R/W1C/H) Reads or clears enable for NIX_AF_RAS[AQ_CTX_POISON]. */
        uint64_t aq_res_poison         : 1;  /**< [ 33: 33](R/W1C/H) Reads or clears enable for NIX_AF_RAS[AQ_RES_POISON]. */
        uint64_t aq_inst_poison        : 1;  /**< [ 34: 34](R/W1C/H) Reads or clears enable for NIX_AF_RAS[AQ_INST_POISON]. */
        uint64_t reserved_35_63        : 29;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_ras_ena_w1c_s cn; */
};
typedef union bdk_nixx_af_ras_ena_w1c bdk_nixx_af_ras_ena_w1c_t;

static inline uint64_t BDK_NIXX_AF_RAS_ENA_W1C(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RAS_ENA_W1C(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400001b8ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RAS_ENA_W1C", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RAS_ENA_W1C(a) bdk_nixx_af_ras_ena_w1c_t
#define bustype_BDK_NIXX_AF_RAS_ENA_W1C(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RAS_ENA_W1C(a) "NIXX_AF_RAS_ENA_W1C"
#define device_bar_BDK_NIXX_AF_RAS_ENA_W1C(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RAS_ENA_W1C(a) (a)
#define arguments_BDK_NIXX_AF_RAS_ENA_W1C(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_ras_ena_w1s
 *
 * NIX AF RAS Interrupt Enable Set Register
 * This register sets interrupt enable bits.
 */
union bdk_nixx_af_ras_ena_w1s
{
    uint64_t u;
    struct bdk_nixx_af_ras_ena_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_35_63        : 29;
        uint64_t aq_inst_poison        : 1;  /**< [ 34: 34](R/W1S/H) Reads or sets enable for NIX_AF_RAS[AQ_INST_POISON]. */
        uint64_t aq_res_poison         : 1;  /**< [ 33: 33](R/W1S/H) Reads or sets enable for NIX_AF_RAS[AQ_RES_POISON]. */
        uint64_t aq_ctx_poison         : 1;  /**< [ 32: 32](R/W1S/H) Reads or sets enable for NIX_AF_RAS[AQ_CTX_POISON]. */
        uint64_t reserved_15_31        : 17;
        uint64_t cint_poison           : 1;  /**< [ 14: 14](R/W1S/H) Reads or sets enable for NIX_AF_RAS[CINT_POISON]. */
        uint64_t qint_poison           : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets enable for NIX_AF_RAS[QINT_POISON]. */
        uint64_t rx_mirror_data_poison : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RX_MIRROR_DATA_POISON]. */
        uint64_t rx_mcast_data_poison  : 1;  /**< [ 11: 11](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RX_MCAST_DATA_POISON]. */
        uint64_t rx_mirror_wqe_poison  : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RX_MIRROR_WQE_POISON]. */
        uint64_t rx_mcast_wqe_poison   : 1;  /**< [  9:  9](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RX_MCAST_WQE_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1S/H) Reads or sets enable for NIX_AF_RAS[SEND_SG_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1S/H) Reads or sets enable for NIX_AF_RAS[SEND_JUMP_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1S/H) Reads or sets enable for NIX_AF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RSSE_POISON]. */
        uint64_t rx_mce_poison         : 1;  /**< [  4:  4](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RX_MCE_POISON]. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1S/H) Reads or sets enable for NIX_AF_RAS[CQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RQ_CTX_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_AF_RAS[SQ_CTX_POISON]. */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_AF_RAS[SQB_POISON]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_AF_RAS[SQB_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_AF_RAS[SQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RQ_CTX_POISON]. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1S/H) Reads or sets enable for NIX_AF_RAS[CQ_CTX_POISON]. */
        uint64_t rx_mce_poison         : 1;  /**< [  4:  4](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RX_MCE_POISON]. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RSSE_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1S/H) Reads or sets enable for NIX_AF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1S/H) Reads or sets enable for NIX_AF_RAS[SEND_JUMP_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1S/H) Reads or sets enable for NIX_AF_RAS[SEND_SG_POISON]. */
        uint64_t rx_mcast_wqe_poison   : 1;  /**< [  9:  9](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RX_MCAST_WQE_POISON]. */
        uint64_t rx_mirror_wqe_poison  : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RX_MIRROR_WQE_POISON]. */
        uint64_t rx_mcast_data_poison  : 1;  /**< [ 11: 11](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RX_MCAST_DATA_POISON]. */
        uint64_t rx_mirror_data_poison : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets enable for NIX_AF_RAS[RX_MIRROR_DATA_POISON]. */
        uint64_t qint_poison           : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets enable for NIX_AF_RAS[QINT_POISON]. */
        uint64_t cint_poison           : 1;  /**< [ 14: 14](R/W1S/H) Reads or sets enable for NIX_AF_RAS[CINT_POISON]. */
        uint64_t reserved_15_31        : 17;
        uint64_t aq_ctx_poison         : 1;  /**< [ 32: 32](R/W1S/H) Reads or sets enable for NIX_AF_RAS[AQ_CTX_POISON]. */
        uint64_t aq_res_poison         : 1;  /**< [ 33: 33](R/W1S/H) Reads or sets enable for NIX_AF_RAS[AQ_RES_POISON]. */
        uint64_t aq_inst_poison        : 1;  /**< [ 34: 34](R/W1S/H) Reads or sets enable for NIX_AF_RAS[AQ_INST_POISON]. */
        uint64_t reserved_35_63        : 29;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_ras_ena_w1s_s cn; */
};
typedef union bdk_nixx_af_ras_ena_w1s bdk_nixx_af_ras_ena_w1s_t;

static inline uint64_t BDK_NIXX_AF_RAS_ENA_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RAS_ENA_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400001b0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RAS_ENA_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RAS_ENA_W1S(a) bdk_nixx_af_ras_ena_w1s_t
#define bustype_BDK_NIXX_AF_RAS_ENA_W1S(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RAS_ENA_W1S(a) "NIXX_AF_RAS_ENA_W1S"
#define device_bar_BDK_NIXX_AF_RAS_ENA_W1S(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RAS_ENA_W1S(a) (a)
#define arguments_BDK_NIXX_AF_RAS_ENA_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_ras_w1s
 *
 * NIX AF RAS Interrupt Set Register
 * This register sets interrupt bits.
 */
union bdk_nixx_af_ras_w1s
{
    uint64_t u;
    struct bdk_nixx_af_ras_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_35_63        : 29;
        uint64_t aq_inst_poison        : 1;  /**< [ 34: 34](R/W1S/H) Reads or sets NIX_AF_RAS[AQ_INST_POISON]. */
        uint64_t aq_res_poison         : 1;  /**< [ 33: 33](R/W1S/H) Reads or sets NIX_AF_RAS[AQ_RES_POISON]. */
        uint64_t aq_ctx_poison         : 1;  /**< [ 32: 32](R/W1S/H) Reads or sets NIX_AF_RAS[AQ_CTX_POISON]. */
        uint64_t reserved_15_31        : 17;
        uint64_t cint_poison           : 1;  /**< [ 14: 14](R/W1S/H) Reads or sets NIX_AF_RAS[CINT_POISON]. */
        uint64_t qint_poison           : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets NIX_AF_RAS[QINT_POISON]. */
        uint64_t rx_mirror_data_poison : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets NIX_AF_RAS[RX_MIRROR_DATA_POISON]. */
        uint64_t rx_mcast_data_poison  : 1;  /**< [ 11: 11](R/W1S/H) Reads or sets NIX_AF_RAS[RX_MCAST_DATA_POISON]. */
        uint64_t rx_mirror_wqe_poison  : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets NIX_AF_RAS[RX_MIRROR_WQE_POISON]. */
        uint64_t rx_mcast_wqe_poison   : 1;  /**< [  9:  9](R/W1S/H) Reads or sets NIX_AF_RAS[RX_MCAST_WQE_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1S/H) Reads or sets NIX_AF_RAS[SEND_SG_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1S/H) Reads or sets NIX_AF_RAS[SEND_JUMP_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1S/H) Reads or sets NIX_AF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1S/H) Reads or sets NIX_AF_RAS[RSSE_POISON]. */
        uint64_t rx_mce_poison         : 1;  /**< [  4:  4](R/W1S/H) Reads or sets NIX_AF_RAS[RX_MCE_POISON]. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1S/H) Reads or sets NIX_AF_RAS[CQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1S/H) Reads or sets NIX_AF_RAS[RQ_CTX_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_AF_RAS[SQ_CTX_POISON]. */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_AF_RAS[SQB_POISON]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_AF_RAS[SQB_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_AF_RAS[SQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1S/H) Reads or sets NIX_AF_RAS[RQ_CTX_POISON]. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1S/H) Reads or sets NIX_AF_RAS[CQ_CTX_POISON]. */
        uint64_t rx_mce_poison         : 1;  /**< [  4:  4](R/W1S/H) Reads or sets NIX_AF_RAS[RX_MCE_POISON]. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1S/H) Reads or sets NIX_AF_RAS[RSSE_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1S/H) Reads or sets NIX_AF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1S/H) Reads or sets NIX_AF_RAS[SEND_JUMP_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1S/H) Reads or sets NIX_AF_RAS[SEND_SG_POISON]. */
        uint64_t rx_mcast_wqe_poison   : 1;  /**< [  9:  9](R/W1S/H) Reads or sets NIX_AF_RAS[RX_MCAST_WQE_POISON]. */
        uint64_t rx_mirror_wqe_poison  : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets NIX_AF_RAS[RX_MIRROR_WQE_POISON]. */
        uint64_t rx_mcast_data_poison  : 1;  /**< [ 11: 11](R/W1S/H) Reads or sets NIX_AF_RAS[RX_MCAST_DATA_POISON]. */
        uint64_t rx_mirror_data_poison : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets NIX_AF_RAS[RX_MIRROR_DATA_POISON]. */
        uint64_t qint_poison           : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets NIX_AF_RAS[QINT_POISON]. */
        uint64_t cint_poison           : 1;  /**< [ 14: 14](R/W1S/H) Reads or sets NIX_AF_RAS[CINT_POISON]. */
        uint64_t reserved_15_31        : 17;
        uint64_t aq_ctx_poison         : 1;  /**< [ 32: 32](R/W1S/H) Reads or sets NIX_AF_RAS[AQ_CTX_POISON]. */
        uint64_t aq_res_poison         : 1;  /**< [ 33: 33](R/W1S/H) Reads or sets NIX_AF_RAS[AQ_RES_POISON]. */
        uint64_t aq_inst_poison        : 1;  /**< [ 34: 34](R/W1S/H) Reads or sets NIX_AF_RAS[AQ_INST_POISON]. */
        uint64_t reserved_35_63        : 29;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_ras_w1s_s cn; */
};
typedef union bdk_nixx_af_ras_w1s bdk_nixx_af_ras_w1s_t;

static inline uint64_t BDK_NIXX_AF_RAS_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RAS_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400001a8ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RAS_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RAS_W1S(a) bdk_nixx_af_ras_w1s_t
#define bustype_BDK_NIXX_AF_RAS_W1S(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RAS_W1S(a) "NIXX_AF_RAS_W1S"
#define device_bar_BDK_NIXX_AF_RAS_W1S(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RAS_W1S(a) (a)
#define arguments_BDK_NIXX_AF_RAS_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rq_const
 *
 * NIX AF RQ Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_rq_const
{
    uint64_t u;
    struct bdk_nixx_af_rq_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_24_63        : 40;
        uint64_t queues_per_lf         : 24; /**< [ 23:  0](RO) Maximum number of receive queues per LF. */
#else /* Word 0 - Little Endian */
        uint64_t queues_per_lf         : 24; /**< [ 23:  0](RO) Maximum number of receive queues per LF. */
        uint64_t reserved_24_63        : 40;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rq_const_s cn; */
};
typedef union bdk_nixx_af_rq_const bdk_nixx_af_rq_const_t;

static inline uint64_t BDK_NIXX_AF_RQ_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RQ_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000050ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RQ_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RQ_CONST(a) bdk_nixx_af_rq_const_t
#define bustype_BDK_NIXX_AF_RQ_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RQ_CONST(a) "NIXX_AF_RQ_CONST"
#define device_bar_BDK_NIXX_AF_RQ_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RQ_CONST(a) (a)
#define arguments_BDK_NIXX_AF_RQ_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rvu_int
 *
 * NIX AF RVU Interrupt Register
 * This register contains RVU error interrupt summary bits.
 */
union bdk_nixx_af_rvu_int
{
    uint64_t u;
    struct bdk_nixx_af_rvu_int_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t unmapped_slot         : 1;  /**< [  0:  0](R/W1C/H) Unmapped slot. Received an IO request to a VF/PF slot in BAR2 that is not
                                                                 reverse mapped to an LF. See TIM_PRIV_LF()_CFG and TIM_AF_RVU_LF_CFG_DEBUG. */
#else /* Word 0 - Little Endian */
        uint64_t unmapped_slot         : 1;  /**< [  0:  0](R/W1C/H) Unmapped slot. Received an IO request to a VF/PF slot in BAR2 that is not
                                                                 reverse mapped to an LF. See TIM_PRIV_LF()_CFG and TIM_AF_RVU_LF_CFG_DEBUG. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rvu_int_s cn; */
};
typedef union bdk_nixx_af_rvu_int bdk_nixx_af_rvu_int_t;

static inline uint64_t BDK_NIXX_AF_RVU_INT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RVU_INT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400001c0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RVU_INT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RVU_INT(a) bdk_nixx_af_rvu_int_t
#define bustype_BDK_NIXX_AF_RVU_INT(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RVU_INT(a) "NIXX_AF_RVU_INT"
#define device_bar_BDK_NIXX_AF_RVU_INT(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RVU_INT(a) (a)
#define arguments_BDK_NIXX_AF_RVU_INT(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rvu_int_ena_w1c
 *
 * NIX AF RVU Interrupt Enable Clear Register
 * This register clears interrupt enable bits.
 */
union bdk_nixx_af_rvu_int_ena_w1c
{
    uint64_t u;
    struct bdk_nixx_af_rvu_int_ena_w1c_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t unmapped_slot         : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_AF_RVU_INT[UNMAPPED_SLOT]. */
#else /* Word 0 - Little Endian */
        uint64_t unmapped_slot         : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_AF_RVU_INT[UNMAPPED_SLOT]. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rvu_int_ena_w1c_s cn; */
};
typedef union bdk_nixx_af_rvu_int_ena_w1c bdk_nixx_af_rvu_int_ena_w1c_t;

static inline uint64_t BDK_NIXX_AF_RVU_INT_ENA_W1C(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RVU_INT_ENA_W1C(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400001d8ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RVU_INT_ENA_W1C", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RVU_INT_ENA_W1C(a) bdk_nixx_af_rvu_int_ena_w1c_t
#define bustype_BDK_NIXX_AF_RVU_INT_ENA_W1C(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RVU_INT_ENA_W1C(a) "NIXX_AF_RVU_INT_ENA_W1C"
#define device_bar_BDK_NIXX_AF_RVU_INT_ENA_W1C(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RVU_INT_ENA_W1C(a) (a)
#define arguments_BDK_NIXX_AF_RVU_INT_ENA_W1C(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rvu_int_ena_w1s
 *
 * NIX AF RVU Interrupt Enable Set Register
 * This register sets interrupt enable bits.
 */
union bdk_nixx_af_rvu_int_ena_w1s
{
    uint64_t u;
    struct bdk_nixx_af_rvu_int_ena_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t unmapped_slot         : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_AF_RVU_INT[UNMAPPED_SLOT]. */
#else /* Word 0 - Little Endian */
        uint64_t unmapped_slot         : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_AF_RVU_INT[UNMAPPED_SLOT]. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rvu_int_ena_w1s_s cn; */
};
typedef union bdk_nixx_af_rvu_int_ena_w1s bdk_nixx_af_rvu_int_ena_w1s_t;

static inline uint64_t BDK_NIXX_AF_RVU_INT_ENA_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RVU_INT_ENA_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400001d0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RVU_INT_ENA_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RVU_INT_ENA_W1S(a) bdk_nixx_af_rvu_int_ena_w1s_t
#define bustype_BDK_NIXX_AF_RVU_INT_ENA_W1S(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RVU_INT_ENA_W1S(a) "NIXX_AF_RVU_INT_ENA_W1S"
#define device_bar_BDK_NIXX_AF_RVU_INT_ENA_W1S(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RVU_INT_ENA_W1S(a) (a)
#define arguments_BDK_NIXX_AF_RVU_INT_ENA_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rvu_int_w1s
 *
 * NIX AF RVU Interrupt Set Register
 * This register sets interrupt bits.
 */
union bdk_nixx_af_rvu_int_w1s
{
    uint64_t u;
    struct bdk_nixx_af_rvu_int_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t unmapped_slot         : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_AF_RVU_INT[UNMAPPED_SLOT]. */
#else /* Word 0 - Little Endian */
        uint64_t unmapped_slot         : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_AF_RVU_INT[UNMAPPED_SLOT]. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rvu_int_w1s_s cn; */
};
typedef union bdk_nixx_af_rvu_int_w1s bdk_nixx_af_rvu_int_w1s_t;

static inline uint64_t BDK_NIXX_AF_RVU_INT_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RVU_INT_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400001c8ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RVU_INT_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RVU_INT_W1S(a) bdk_nixx_af_rvu_int_w1s_t
#define bustype_BDK_NIXX_AF_RVU_INT_W1S(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RVU_INT_W1S(a) "NIXX_AF_RVU_INT_W1S"
#define device_bar_BDK_NIXX_AF_RVU_INT_W1S(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RVU_INT_W1S(a) (a)
#define arguments_BDK_NIXX_AF_RVU_INT_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rvu_lf_cfg_debug
 *
 * NIX Privileged LF Configuration Debug Register
 * This debug register allows software to lookup the reverse mapping from VF/PF
 * slot to LF. The forward mapping is programmed with NIX_PRIV_LF()_CFG.
 */
union bdk_nixx_af_rvu_lf_cfg_debug
{
    uint64_t u;
    struct bdk_nixx_af_rvu_lf_cfg_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_40_63        : 24;
        uint64_t pf_func               : 16; /**< [ 39: 24](R/W) RVU VF/PF for reverse lookup. Format defined by RVU_PF_FUNC_S. */
        uint64_t slot                  : 8;  /**< [ 23: 16](R/W) Slot within the VF/PF selected by [PF_FUNC] for reverse lookup. Must be
                                                                 zero for NIX and NPA. */
        uint64_t reserved_14_15        : 2;
        uint64_t exec                  : 1;  /**< [ 13: 13](R/W1S/H) Execute lookup. Writing a one to this bits initiates the reverse lookup
                                                                 from {[PF_FUNC], [SLOT]}. Hardware writes the lookup result to {[LF_VALID],
                                                                 [LF]} and clears this bit when done. */
        uint64_t lf_valid              : 1;  /**< [ 12: 12](RO/H) When set, indicates local function [LF] is provisioned to the VF/PF slot
                                                                 indexed by this register. When clear, a local function is not provisioned
                                                                 to the VF/PF slot. */
        uint64_t lf                    : 12; /**< [ 11:  0](RO/H) When [LF_VALID] is set, local function provisioned to the VF/PF slot. */
#else /* Word 0 - Little Endian */
        uint64_t lf                    : 12; /**< [ 11:  0](RO/H) When [LF_VALID] is set, local function provisioned to the VF/PF slot. */
        uint64_t lf_valid              : 1;  /**< [ 12: 12](RO/H) When set, indicates local function [LF] is provisioned to the VF/PF slot
                                                                 indexed by this register. When clear, a local function is not provisioned
                                                                 to the VF/PF slot. */
        uint64_t exec                  : 1;  /**< [ 13: 13](R/W1S/H) Execute lookup. Writing a one to this bits initiates the reverse lookup
                                                                 from {[PF_FUNC], [SLOT]}. Hardware writes the lookup result to {[LF_VALID],
                                                                 [LF]} and clears this bit when done. */
        uint64_t reserved_14_15        : 2;
        uint64_t slot                  : 8;  /**< [ 23: 16](R/W) Slot within the VF/PF selected by [PF_FUNC] for reverse lookup. Must be
                                                                 zero for NIX and NPA. */
        uint64_t pf_func               : 16; /**< [ 39: 24](R/W) RVU VF/PF for reverse lookup. Format defined by RVU_PF_FUNC_S. */
        uint64_t reserved_40_63        : 24;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rvu_lf_cfg_debug_s cn; */
};
typedef union bdk_nixx_af_rvu_lf_cfg_debug bdk_nixx_af_rvu_lf_cfg_debug_t;

static inline uint64_t BDK_NIXX_AF_RVU_LF_CFG_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RVU_LF_CFG_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850048000030ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RVU_LF_CFG_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RVU_LF_CFG_DEBUG(a) bdk_nixx_af_rvu_lf_cfg_debug_t
#define bustype_BDK_NIXX_AF_RVU_LF_CFG_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RVU_LF_CFG_DEBUG(a) "NIXX_AF_RVU_LF_CFG_DEBUG"
#define device_bar_BDK_NIXX_AF_RVU_LF_CFG_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RVU_LF_CFG_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_RVU_LF_CFG_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_bpid#_status
 *
 * NIX AF Receive Backpressure ID Status Registers
 */
union bdk_nixx_af_rx_bpidx_status
{
    uint64_t u;
    struct bdk_nixx_af_rx_bpidx_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_40_63        : 24;
        uint64_t cq_cnt                : 20; /**< [ 39: 20](R/W/H) Backpressure CQ count. Number of completion queues that are backpressuring (XOFF) this
                                                                 BPID.
                                                                 Writes to this field are for diagnostic use only. */
        uint64_t aura_cnt              : 20; /**< [ 19:  0](R/W/H) Backpressure aura count. Number of auras that are backpressuring (XOFF) this BPID.
                                                                 Writes to this field are for diagnostic use only. */
#else /* Word 0 - Little Endian */
        uint64_t aura_cnt              : 20; /**< [ 19:  0](R/W/H) Backpressure aura count. Number of auras that are backpressuring (XOFF) this BPID.
                                                                 Writes to this field are for diagnostic use only. */
        uint64_t cq_cnt                : 20; /**< [ 39: 20](R/W/H) Backpressure CQ count. Number of completion queues that are backpressuring (XOFF) this
                                                                 BPID.
                                                                 Writes to this field are for diagnostic use only. */
        uint64_t reserved_40_63        : 24;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_bpidx_status_s cn; */
};
typedef union bdk_nixx_af_rx_bpidx_status bdk_nixx_af_rx_bpidx_status_t;

static inline uint64_t BDK_NIXX_AF_RX_BPIDX_STATUS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_BPIDX_STATUS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001a20ll + 0x10000000ll * ((a) & 0x0) + 0x20000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_RX_BPIDX_STATUS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_BPIDX_STATUS(a,b) bdk_nixx_af_rx_bpidx_status_t
#define bustype_BDK_NIXX_AF_RX_BPIDX_STATUS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_BPIDX_STATUS(a,b) "NIXX_AF_RX_BPIDX_STATUS"
#define device_bar_BDK_NIXX_AF_RX_BPIDX_STATUS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_BPIDX_STATUS(a,b) (a)
#define arguments_BDK_NIXX_AF_RX_BPIDX_STATUS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_cfg
 *
 * NIX AF Receive Configuration Register
 */
union bdk_nixx_af_rx_cfg
{
    uint64_t u;
    struct bdk_nixx_af_rx_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t cbp_ena               : 1;  /**< [  0:  0](R/W) Channel backpressure enable. Software should set this bit before any
                                                                 NIX_AF_RX_CHAN()_CFG[BP_ENA] is set, and must not clear it unless all
                                                                 NIX_AF_RX_CHAN()_CFG[BP_ENA] are clear for at least 10 microseconds.

                                                                 Internal:
                                                                 Enables sending of X2P backpressure. */
#else /* Word 0 - Little Endian */
        uint64_t cbp_ena               : 1;  /**< [  0:  0](R/W) Channel backpressure enable. Software should set this bit before any
                                                                 NIX_AF_RX_CHAN()_CFG[BP_ENA] is set, and must not clear it unless all
                                                                 NIX_AF_RX_CHAN()_CFG[BP_ENA] are clear for at least 10 microseconds.

                                                                 Internal:
                                                                 Enables sending of X2P backpressure. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_cfg_s cn; */
};
typedef union bdk_nixx_af_rx_cfg bdk_nixx_af_rx_cfg_t;

static inline uint64_t BDK_NIXX_AF_RX_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400000d0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_CFG(a) bdk_nixx_af_rx_cfg_t
#define bustype_BDK_NIXX_AF_RX_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_CFG(a) "NIXX_AF_RX_CFG"
#define device_bar_BDK_NIXX_AF_RX_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_CFG(a) (a)
#define arguments_BDK_NIXX_AF_RX_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_chan#_cfg
 *
 * NIX AF Receive Channel Configuration Registers
 */
union bdk_nixx_af_rx_chanx_cfg
{
    uint64_t u;
    struct bdk_nixx_af_rx_chanx_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_19_63        : 45;
        uint64_t imp                   : 1;  /**< [ 18: 18](RO/H) Implemented. This register is sparse (only indexes with values in NIX_CHAN_E which are
                                                                 implemented).
                                                                 0 = Channel is not implemented.
                                                                 1 = Channel is implemented.

                                                                 Write to a non-implemented channel is ignored. Reading a non-implemented channel returns
                                                                 all zero data. */
        uint64_t sw_xoff               : 1;  /**< [ 17: 17](R/W/H) Software XOFF. When set, backpressure is forced on the RX channel. */
        uint64_t bp_ena                : 1;  /**< [ 16: 16](R/W/H) Backpressure enable. When set, the channel receives backpressure from [BPID]. */
        uint64_t reserved_9_15         : 7;
        uint64_t bpid                  : 9;  /**< [  8:  0](R/W) BPID used to receive backpressure when [BP_ENA] is set. */
#else /* Word 0 - Little Endian */
        uint64_t bpid                  : 9;  /**< [  8:  0](R/W) BPID used to receive backpressure when [BP_ENA] is set. */
        uint64_t reserved_9_15         : 7;
        uint64_t bp_ena                : 1;  /**< [ 16: 16](R/W/H) Backpressure enable. When set, the channel receives backpressure from [BPID]. */
        uint64_t sw_xoff               : 1;  /**< [ 17: 17](R/W/H) Software XOFF. When set, backpressure is forced on the RX channel. */
        uint64_t imp                   : 1;  /**< [ 18: 18](RO/H) Implemented. This register is sparse (only indexes with values in NIX_CHAN_E which are
                                                                 implemented).
                                                                 0 = Channel is not implemented.
                                                                 1 = Channel is implemented.

                                                                 Write to a non-implemented channel is ignored. Reading a non-implemented channel returns
                                                                 all zero data. */
        uint64_t reserved_19_63        : 45;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_chanx_cfg_s cn; */
};
typedef union bdk_nixx_af_rx_chanx_cfg bdk_nixx_af_rx_chanx_cfg_t;

static inline uint64_t BDK_NIXX_AF_RX_CHANX_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_CHANX_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=4095)))
        return 0x850040001a30ll + 0x10000000ll * ((a) & 0x0) + 0x8000ll * ((b) & 0xfff);
    __bdk_csr_fatal("NIXX_AF_RX_CHANX_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_CHANX_CFG(a,b) bdk_nixx_af_rx_chanx_cfg_t
#define bustype_BDK_NIXX_AF_RX_CHANX_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_CHANX_CFG(a,b) "NIXX_AF_RX_CHANX_CFG"
#define device_bar_BDK_NIXX_AF_RX_CHANX_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_CHANX_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_RX_CHANX_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_cpt#_inst_addr
 *
 * NIX AF Receive CPT Instruction Address Register
 */
union bdk_nixx_af_rx_cptx_inst_addr
{
    uint64_t u;
    struct bdk_nixx_af_rx_cptx_inst_addr_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 53; /**< [ 52:  0](R/W) RVU PF(0) IOVA of CPT queue to which NIX sends instructions (CPT_INST_S). */
#else /* Word 0 - Little Endian */
        uint64_t addr                  : 53; /**< [ 52:  0](R/W) RVU PF(0) IOVA of CPT queue to which NIX sends instructions (CPT_INST_S). */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_cptx_inst_addr_s cn; */
};
typedef union bdk_nixx_af_rx_cptx_inst_addr bdk_nixx_af_rx_cptx_inst_addr_t;

static inline uint64_t BDK_NIXX_AF_RX_CPTX_INST_ADDR(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_CPTX_INST_ADDR(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b==0)))
        return 0x850040000310ll + 0x10000000ll * ((a) & 0x0) + 8ll * ((b) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_CPTX_INST_ADDR", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_CPTX_INST_ADDR(a,b) bdk_nixx_af_rx_cptx_inst_addr_t
#define bustype_BDK_NIXX_AF_RX_CPTX_INST_ADDR(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_CPTX_INST_ADDR(a,b) "NIXX_AF_RX_CPTX_INST_ADDR"
#define device_bar_BDK_NIXX_AF_RX_CPTX_INST_ADDR(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_CPTX_INST_ADDR(a,b) (a)
#define arguments_BDK_NIXX_AF_RX_CPTX_INST_ADDR(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_iip4
 *
 * NIX AF Receive Inner IPv4 Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an inner IPv4 header.
 * Typically the same as NPC_PCK_DEF_IIP4.
 */
union bdk_nixx_af_rx_def_iip4
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_iip4_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_iip4_s cn; */
};
typedef union bdk_nixx_af_rx_def_iip4 bdk_nixx_af_rx_def_iip4_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_IIP4(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_IIP4(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000220ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_IIP4", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_IIP4(a) bdk_nixx_af_rx_def_iip4_t
#define bustype_BDK_NIXX_AF_RX_DEF_IIP4(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_IIP4(a) "NIXX_AF_RX_DEF_IIP4"
#define device_bar_BDK_NIXX_AF_RX_DEF_IIP4(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_IIP4(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_IIP4(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_iip6
 *
 * NIX AF Receive Inner IPv6 Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an inner IPv6 header.
 */
union bdk_nixx_af_rx_def_iip6
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_iip6_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_iip6_s cn; */
};
typedef union bdk_nixx_af_rx_def_iip6 bdk_nixx_af_rx_def_iip6_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_IIP6(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_IIP6(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000240ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_IIP6", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_IIP6(a) bdk_nixx_af_rx_def_iip6_t
#define bustype_BDK_NIXX_AF_RX_DEF_IIP6(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_IIP6(a) "NIXX_AF_RX_DEF_IIP6"
#define device_bar_BDK_NIXX_AF_RX_DEF_IIP6(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_IIP6(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_IIP6(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_ipsec#
 *
 * NIX AF Receive IPSEC Header Definition Registers
 * These two registers define layer information in NPC_RESULT_S to identify an
 * IPSEC header for up to two IPSEC packet formats. The two formats are typically
 * IPSEC ESP (RFC 4303) and UDP-encapsulated IPSEC ESP (RFC 3948).
 */
union bdk_nixx_af_rx_def_ipsecx
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_ipsecx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_17_63        : 47;
        uint64_t spi_nz                : 1;  /**< [ 16: 16](R/W) SPI non-zero. When set, match only is the IPSEC SPI field in the packet is non-zero. */
        uint64_t spi_offset            : 4;  /**< [ 15: 12](R/W) SPI offset. Starting byte offset of IPSEC SPI field relative to the start
                                                                 of the IPSEC header identifies by [LID], [LTYPE_MATCH] and [LTYPE_MASK]. */
        uint64_t reserved_11           : 1;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LG as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LG as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11           : 1;
        uint64_t spi_offset            : 4;  /**< [ 15: 12](R/W) SPI offset. Starting byte offset of IPSEC SPI field relative to the start
                                                                 of the IPSEC header identifies by [LID], [LTYPE_MATCH] and [LTYPE_MASK]. */
        uint64_t spi_nz                : 1;  /**< [ 16: 16](R/W) SPI non-zero. When set, match only is the IPSEC SPI field in the packet is non-zero. */
        uint64_t reserved_17_63        : 47;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_ipsecx_s cn; */
};
typedef union bdk_nixx_af_rx_def_ipsecx bdk_nixx_af_rx_def_ipsecx_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_IPSECX(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_IPSECX(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=1)))
        return 0x8500400002b0ll + 0x10000000ll * ((a) & 0x0) + 8ll * ((b) & 0x1);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_IPSECX", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_IPSECX(a,b) bdk_nixx_af_rx_def_ipsecx_t
#define bustype_BDK_NIXX_AF_RX_DEF_IPSECX(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_IPSECX(a,b) "NIXX_AF_RX_DEF_IPSECX"
#define device_bar_BDK_NIXX_AF_RX_DEF_IPSECX(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_IPSECX(a,b) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_IPSECX(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_isctp
 *
 * NIX AF Receive Inner SCTP Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an inner SCTP header.
 */
union bdk_nixx_af_rx_def_isctp
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_isctp_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_isctp_s cn; */
};
typedef union bdk_nixx_af_rx_def_isctp bdk_nixx_af_rx_def_isctp_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_ISCTP(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_ISCTP(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400002a0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_ISCTP", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_ISCTP(a) bdk_nixx_af_rx_def_isctp_t
#define bustype_BDK_NIXX_AF_RX_DEF_ISCTP(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_ISCTP(a) "NIXX_AF_RX_DEF_ISCTP"
#define device_bar_BDK_NIXX_AF_RX_DEF_ISCTP(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_ISCTP(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_ISCTP(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_itcp
 *
 * NIX AF Receive Inner TCP Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an inner TCP header.
 */
union bdk_nixx_af_rx_def_itcp
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_itcp_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_itcp_s cn; */
};
typedef union bdk_nixx_af_rx_def_itcp bdk_nixx_af_rx_def_itcp_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_ITCP(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_ITCP(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000260ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_ITCP", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_ITCP(a) bdk_nixx_af_rx_def_itcp_t
#define bustype_BDK_NIXX_AF_RX_DEF_ITCP(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_ITCP(a) "NIXX_AF_RX_DEF_ITCP"
#define device_bar_BDK_NIXX_AF_RX_DEF_ITCP(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_ITCP(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_ITCP(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_iudp
 *
 * NIX AF Receive Inner UDP Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an inner UDP header.
 */
union bdk_nixx_af_rx_def_iudp
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_iudp_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_iudp_s cn; */
};
typedef union bdk_nixx_af_rx_def_iudp bdk_nixx_af_rx_def_iudp_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_IUDP(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_IUDP(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000280ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_IUDP", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_IUDP(a) bdk_nixx_af_rx_def_iudp_t
#define bustype_BDK_NIXX_AF_RX_DEF_IUDP(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_IUDP(a) "NIXX_AF_RX_DEF_IUDP"
#define device_bar_BDK_NIXX_AF_RX_DEF_IUDP(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_IUDP(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_IUDP(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_oip4
 *
 * NIX AF Receive Outer IPv4 Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an outer IPv4 L3 header.
 * Typically the same as NPC_PCK_DEF_OIP4.
 */
union bdk_nixx_af_rx_def_oip4
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_oip4_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_oip4_s cn; */
};
typedef union bdk_nixx_af_rx_def_oip4 bdk_nixx_af_rx_def_oip4_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_OIP4(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_OIP4(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000210ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_OIP4", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_OIP4(a) bdk_nixx_af_rx_def_oip4_t
#define bustype_BDK_NIXX_AF_RX_DEF_OIP4(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_OIP4(a) "NIXX_AF_RX_DEF_OIP4"
#define device_bar_BDK_NIXX_AF_RX_DEF_OIP4(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_OIP4(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_OIP4(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_oip6
 *
 * NIX AF Receive Outer IPv6 Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an outer IPv6 header.
 * Typically the same as NPC_PCK_DEF_OIP6.
 */
union bdk_nixx_af_rx_def_oip6
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_oip6_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_oip6_s cn; */
};
typedef union bdk_nixx_af_rx_def_oip6 bdk_nixx_af_rx_def_oip6_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_OIP6(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_OIP6(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000230ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_OIP6", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_OIP6(a) bdk_nixx_af_rx_def_oip6_t
#define bustype_BDK_NIXX_AF_RX_DEF_OIP6(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_OIP6(a) "NIXX_AF_RX_DEF_OIP6"
#define device_bar_BDK_NIXX_AF_RX_DEF_OIP6(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_OIP6(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_OIP6(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_ol2
 *
 * NIX AF Receive Outer L2 Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an outer L2/Ethernet
 * header. Typically the same as NPC_PCK_DEF_OL2.
 */
union bdk_nixx_af_rx_def_ol2
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_ol2_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_ol2_s cn; */
};
typedef union bdk_nixx_af_rx_def_ol2 bdk_nixx_af_rx_def_ol2_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_OL2(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_OL2(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000200ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_OL2", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_OL2(a) bdk_nixx_af_rx_def_ol2_t
#define bustype_BDK_NIXX_AF_RX_DEF_OL2(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_OL2(a) "NIXX_AF_RX_DEF_OL2"
#define device_bar_BDK_NIXX_AF_RX_DEF_OL2(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_OL2(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_OL2(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_osctp
 *
 * NIX AF Receive Outer SCTP Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an outer SCTP header.
 */
union bdk_nixx_af_rx_def_osctp
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_osctp_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_osctp_s cn; */
};
typedef union bdk_nixx_af_rx_def_osctp bdk_nixx_af_rx_def_osctp_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_OSCTP(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_OSCTP(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000290ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_OSCTP", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_OSCTP(a) bdk_nixx_af_rx_def_osctp_t
#define bustype_BDK_NIXX_AF_RX_DEF_OSCTP(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_OSCTP(a) "NIXX_AF_RX_DEF_OSCTP"
#define device_bar_BDK_NIXX_AF_RX_DEF_OSCTP(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_OSCTP(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_OSCTP(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_otcp
 *
 * NIX AF Receive Outer TCP Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an outer TCP header.
 */
union bdk_nixx_af_rx_def_otcp
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_otcp_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_otcp_s cn; */
};
typedef union bdk_nixx_af_rx_def_otcp bdk_nixx_af_rx_def_otcp_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_OTCP(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_OTCP(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000250ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_OTCP", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_OTCP(a) bdk_nixx_af_rx_def_otcp_t
#define bustype_BDK_NIXX_AF_RX_DEF_OTCP(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_OTCP(a) "NIXX_AF_RX_DEF_OTCP"
#define device_bar_BDK_NIXX_AF_RX_DEF_OTCP(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_OTCP(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_OTCP(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_def_oudp
 *
 * NIX AF Receive Outer UDP Header Definition Register
 * Defines layer information in NPC_RESULT_S to identify an outer UDP header.
 */
union bdk_nixx_af_rx_def_oudp
{
    uint64_t u;
    struct bdk_nixx_af_rx_def_oudp_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
#else /* Word 0 - Little Endian */
        uint64_t ltype_mask            : 4;  /**< [  3:  0](R/W) Layer type mask. See [LTYPE_MATCH]. */
        uint64_t ltype_match           : 4;  /**< [  7:  4](R/W) Layer type match value. Hardware detects a layer match when
                                                                 \<pre\>
                                                                 ([LTYPE_MATCH] & [LTYPE_MASK]) == (NPC_RESULT_S[LX[LTYPE]] & [LTYPE_MASK])
                                                                 \</pre\>

                                                                 where LX is one of LA, LB, ..., LH as selected by [LID]. */
        uint64_t lid                   : 3;  /**< [ 10:  8](R/W) Layer ID. Enumerated by NPC_LID_E. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_def_oudp_s cn; */
};
typedef union bdk_nixx_af_rx_def_oudp bdk_nixx_af_rx_def_oudp_t;

static inline uint64_t BDK_NIXX_AF_RX_DEF_OUDP(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_DEF_OUDP(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000270ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_DEF_OUDP", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_DEF_OUDP(a) bdk_nixx_af_rx_def_oudp_t
#define bustype_BDK_NIXX_AF_RX_DEF_OUDP(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_DEF_OUDP(a) "NIXX_AF_RX_DEF_OUDP"
#define device_bar_BDK_NIXX_AF_RX_DEF_OUDP(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_DEF_OUDP(a) (a)
#define arguments_BDK_NIXX_AF_RX_DEF_OUDP(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_flow_key_alg#_field#
 *
 * NIX AF Receive Flow Key Algorithm Field Registers
 * A flow key algorithm defines how the 40-byte FLOW_KEY is formed from the received
 * packet header. FLOW_KEY is formed using up to five header fields (this register's
 * last index) with up to 16 bytes per field.
 *
 * The algorithm (index {a} (ALG) of these registers) is selected by
 * NIX_RX_ACTION_S[FLOW_KEY_ALG] from the packet's NPC_RESULT_S[ACTION].
 *
 * Internal:
 * 40-byte FLOW_KEY is wide enough to support an IPv6 5-tuple that includes a
 * VXLAN/GENEVE/NVGRE tunnel ID, e.g:
 * _ Source IP: 16B.
 * _ Dest IP: 16B.
 * _ Source port: 2B.
 * _ Dest port: 2B.
 * _ Tunnel VNI/VSI: 3B.
 * _ Total: 39B.
 */
union bdk_nixx_af_rx_flow_key_algx_fieldx
{
    uint64_t u;
    struct bdk_nixx_af_rx_flow_key_algx_fieldx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_26_63        : 38;
        uint64_t sel_chan              : 1;  /**< [ 25: 25](R/W) Select channel number. When set, {4'h0, NIX_RX_PARSE_S[CHAN]\<11:0\>} is
                                                                 selected as the field data instead of the packet header data specified by
                                                                 [LID] and [HDR_OFFSET].
                                                                 [BYTESM1] must be 0x1 (2 bytes) when this bit is set. */
        uint64_t ena                   : 1;  /**< [ 24: 24](R/W) Field extract enable. */
        uint64_t reserved_23           : 1;
        uint64_t lid                   : 3;  /**< [ 22: 20](R/W) Layer ID of packet header. Enumerated by NPC_LID_E. Not used when [SEL_CHAN] is set. */
        uint64_t bytesm1               : 4;  /**< [ 19: 16](R/W) Field size in bytes minus one. 0x0=1 byte; 0x1=2 bytes, ..., 0xF=16 bytes.

                                                                 Must be 0x1 when [SEL_CHAN] is set. */
        uint64_t hdr_offset            : 8;  /**< [ 15:  8](R/W) Header offset. Starting byte offset of field relative to the start
                                                                 of the header layer. For example, when [LID] = NPC_LID_E::LC, the header
                                                                 layer's NPC_LAYER_INFO_S = NPC_RESULT_S[LC], and the field's first byte is
                                                                 at the following byte offset from packet start:
                                                                 _ NPC_LAYER_INFO_S[LPTR] + [HDR_OFFSET].

                                                                 Not used when [SEL_CHAN] is set. */
        uint64_t fn_mask               : 1;  /**< [  7:  7](R/W) First nibble mask. When set, the most significant 4 bits of the first
                                                                 extracted header byte are written to FLOW_KEY as zeros. */
        uint64_t ln_mask               : 1;  /**< [  6:  6](R/W) Last nibble mask. When set, the least significant 4 bits of the last
                                                                 extracted header byte are written to FLOW_KEY as zeros. */
        uint64_t key_offset            : 6;  /**< [  5:  0](R/W) Key offset. Starting byte offset of field in FLOW_KEY\<319:0\>. Bytes in
                                                                 FLOW_KEY are enumerated in network byte order as follows:
                                                                 Byte 0: FLOW_KEY\<319:312\>.
                                                                 Byte 1: FLOW_KEY\<311:304\>.
                                                                 ...
                                                                 Byte 39: FLOW_KEY\<7:0\>.

                                                                 For example, if [KEY_OFFSET] = 5, [BYTESM1] = 3:
                                                                 _ First header byte is written to FLOW_KEY\<279:272\>.
                                                                 _ Second header byte is written to FLOW_KEY\<271:264\>.
                                                                 _ Third header byte is written to FLOW_KEY\<263:256\>. */
#else /* Word 0 - Little Endian */
        uint64_t key_offset            : 6;  /**< [  5:  0](R/W) Key offset. Starting byte offset of field in FLOW_KEY\<319:0\>. Bytes in
                                                                 FLOW_KEY are enumerated in network byte order as follows:
                                                                 Byte 0: FLOW_KEY\<319:312\>.
                                                                 Byte 1: FLOW_KEY\<311:304\>.
                                                                 ...
                                                                 Byte 39: FLOW_KEY\<7:0\>.

                                                                 For example, if [KEY_OFFSET] = 5, [BYTESM1] = 3:
                                                                 _ First header byte is written to FLOW_KEY\<279:272\>.
                                                                 _ Second header byte is written to FLOW_KEY\<271:264\>.
                                                                 _ Third header byte is written to FLOW_KEY\<263:256\>. */
        uint64_t ln_mask               : 1;  /**< [  6:  6](R/W) Last nibble mask. When set, the least significant 4 bits of the last
                                                                 extracted header byte are written to FLOW_KEY as zeros. */
        uint64_t fn_mask               : 1;  /**< [  7:  7](R/W) First nibble mask. When set, the most significant 4 bits of the first
                                                                 extracted header byte are written to FLOW_KEY as zeros. */
        uint64_t hdr_offset            : 8;  /**< [ 15:  8](R/W) Header offset. Starting byte offset of field relative to the start
                                                                 of the header layer. For example, when [LID] = NPC_LID_E::LC, the header
                                                                 layer's NPC_LAYER_INFO_S = NPC_RESULT_S[LC], and the field's first byte is
                                                                 at the following byte offset from packet start:
                                                                 _ NPC_LAYER_INFO_S[LPTR] + [HDR_OFFSET].

                                                                 Not used when [SEL_CHAN] is set. */
        uint64_t bytesm1               : 4;  /**< [ 19: 16](R/W) Field size in bytes minus one. 0x0=1 byte; 0x1=2 bytes, ..., 0xF=16 bytes.

                                                                 Must be 0x1 when [SEL_CHAN] is set. */
        uint64_t lid                   : 3;  /**< [ 22: 20](R/W) Layer ID of packet header. Enumerated by NPC_LID_E. Not used when [SEL_CHAN] is set. */
        uint64_t reserved_23           : 1;
        uint64_t ena                   : 1;  /**< [ 24: 24](R/W) Field extract enable. */
        uint64_t sel_chan              : 1;  /**< [ 25: 25](R/W) Select channel number. When set, {4'h0, NIX_RX_PARSE_S[CHAN]\<11:0\>} is
                                                                 selected as the field data instead of the packet header data specified by
                                                                 [LID] and [HDR_OFFSET].
                                                                 [BYTESM1] must be 0x1 (2 bytes) when this bit is set. */
        uint64_t reserved_26_63        : 38;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_flow_key_algx_fieldx_s cn; */
};
typedef union bdk_nixx_af_rx_flow_key_algx_fieldx bdk_nixx_af_rx_flow_key_algx_fieldx_t;

static inline uint64_t BDK_NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX(unsigned long a, unsigned long b, unsigned long c) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX(unsigned long a, unsigned long b, unsigned long c)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=31) && (c<=4)))
        return 0x850040001800ll + 0x10000000ll * ((a) & 0x0) + 0x40000ll * ((b) & 0x1f) + 8ll * ((c) & 0x7);
    __bdk_csr_fatal("NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX", 3, a, b, c, 0);
}

#define typedef_BDK_NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX(a,b,c) bdk_nixx_af_rx_flow_key_algx_fieldx_t
#define bustype_BDK_NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX(a,b,c) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX(a,b,c) "NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX"
#define device_bar_BDK_NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX(a,b,c) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX(a,b,c) (a)
#define arguments_BDK_NIXX_AF_RX_FLOW_KEY_ALGX_FIELDX(a,b,c) (a),(b),(c),-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_ipsec_gen_cfg
 *
 * NIX AF Receive IPSEC General Configuration Register
 * This register specifie the values of certain fields in CPT instructions
 * (CPT_INST_S) generated by NIX for IPSEC hardware fast-path packets.
 */
union bdk_nixx_af_rx_ipsec_gen_cfg
{
    uint64_t u;
    struct bdk_nixx_af_rx_ipsec_gen_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_51_63        : 13;
        uint64_t egrp                  : 3;  /**< [ 50: 48](R/W) CPT_INST_S[EGRP] value. */
        uint64_t opcode                : 16; /**< [ 47: 32](R/W) CPT_INST_S[OPCODE] value. */
        uint64_t param1                : 16; /**< [ 31: 16](R/W) CPT_INST_S[PARAM1] value. */
        uint64_t param2                : 16; /**< [ 15:  0](R/W) CPT_INST_S[PARAM2] value. */
#else /* Word 0 - Little Endian */
        uint64_t param2                : 16; /**< [ 15:  0](R/W) CPT_INST_S[PARAM2] value. */
        uint64_t param1                : 16; /**< [ 31: 16](R/W) CPT_INST_S[PARAM1] value. */
        uint64_t opcode                : 16; /**< [ 47: 32](R/W) CPT_INST_S[OPCODE] value. */
        uint64_t egrp                  : 3;  /**< [ 50: 48](R/W) CPT_INST_S[EGRP] value. */
        uint64_t reserved_51_63        : 13;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_ipsec_gen_cfg_s cn; */
};
typedef union bdk_nixx_af_rx_ipsec_gen_cfg bdk_nixx_af_rx_ipsec_gen_cfg_t;

static inline uint64_t BDK_NIXX_AF_RX_IPSEC_GEN_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_IPSEC_GEN_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000300ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_IPSEC_GEN_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_IPSEC_GEN_CFG(a) bdk_nixx_af_rx_ipsec_gen_cfg_t
#define bustype_BDK_NIXX_AF_RX_IPSEC_GEN_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_IPSEC_GEN_CFG(a) "NIXX_AF_RX_IPSEC_GEN_CFG"
#define device_bar_BDK_NIXX_AF_RX_IPSEC_GEN_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_IPSEC_GEN_CFG(a) (a)
#define arguments_BDK_NIXX_AF_RX_IPSEC_GEN_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_link#_cfg
 *
 * NIX AF Receive Link Configuration Registers
 * Index enumerated by NIX_LINK_E.
 */
union bdk_nixx_af_rx_linkx_cfg
{
    uint64_t u;
    struct bdk_nixx_af_rx_linkx_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_32_63        : 32;
        uint64_t maxlen                : 16; /**< [ 31: 16](R/W) Byte count for max-sized frame check on packets received from this link.
                                                                 See NIX_RE_OPCODE_E::OVERSIZE. This length must include any any Vtags
                                                                 which may be stripped and optional timestamp inserted by CGX. FCS bytes
                                                                 stripped by CGX are not included. Must not exceed 9212 bytes (9216 minus 4
                                                                 byte FCS) for CGX and LBK links. */
        uint64_t minlen                : 16; /**< [ 15:  0](R/W) Byte count for min-sized frame check on packets received from this link.
                                                                 See NIX_RE_OPCODE_E::UNDERSIZE. Zero disables the check.
                                                                 See [MAXLEN] for packet bytes that may be included or excluded in the
                                                                 specified length. */
#else /* Word 0 - Little Endian */
        uint64_t minlen                : 16; /**< [ 15:  0](R/W) Byte count for min-sized frame check on packets received from this link.
                                                                 See NIX_RE_OPCODE_E::UNDERSIZE. Zero disables the check.
                                                                 See [MAXLEN] for packet bytes that may be included or excluded in the
                                                                 specified length. */
        uint64_t maxlen                : 16; /**< [ 31: 16](R/W) Byte count for max-sized frame check on packets received from this link.
                                                                 See NIX_RE_OPCODE_E::OVERSIZE. This length must include any any Vtags
                                                                 which may be stripped and optional timestamp inserted by CGX. FCS bytes
                                                                 stripped by CGX are not included. Must not exceed 9212 bytes (9216 minus 4
                                                                 byte FCS) for CGX and LBK links. */
        uint64_t reserved_32_63        : 32;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_linkx_cfg_s cn; */
};
typedef union bdk_nixx_af_rx_linkx_cfg bdk_nixx_af_rx_linkx_cfg_t;

static inline uint64_t BDK_NIXX_AF_RX_LINKX_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_LINKX_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=13)))
        return 0x850040000540ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xf);
    __bdk_csr_fatal("NIXX_AF_RX_LINKX_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_LINKX_CFG(a,b) bdk_nixx_af_rx_linkx_cfg_t
#define bustype_BDK_NIXX_AF_RX_LINKX_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_LINKX_CFG(a,b) "NIXX_AF_RX_LINKX_CFG"
#define device_bar_BDK_NIXX_AF_RX_LINKX_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_LINKX_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_RX_LINKX_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_link#_sl#_spkt_cnt
 *
 * INTERNAL: NIX Receive Software Sync Link Packet Count Registers
 *
 * For diagnostic use only for debug of NIX_AF_RX_SW_SYNC[ENA] function. Index
 * {a} (LINK) is enumerated by NIX_LINK_E. Index {b} (SL) is zero for non-express
 * packets, one for express packets
 */
union bdk_nixx_af_rx_linkx_slx_spkt_cnt
{
    uint64_t u;
    struct bdk_nixx_af_rx_linkx_slx_spkt_cnt_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t out_cnt               : 32; /**< [ 63: 32](RO/H) Running count at output of machine. */
        uint64_t in_cnt                : 32; /**< [ 31:  0](RO/H) Running count at input of machine. */
#else /* Word 0 - Little Endian */
        uint64_t in_cnt                : 32; /**< [ 31:  0](RO/H) Running count at input of machine. */
        uint64_t out_cnt               : 32; /**< [ 63: 32](RO/H) Running count at output of machine. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_linkx_slx_spkt_cnt_s cn; */
};
typedef union bdk_nixx_af_rx_linkx_slx_spkt_cnt bdk_nixx_af_rx_linkx_slx_spkt_cnt_t;

static inline uint64_t BDK_NIXX_AF_RX_LINKX_SLX_SPKT_CNT(unsigned long a, unsigned long b, unsigned long c) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_LINKX_SLX_SPKT_CNT(unsigned long a, unsigned long b, unsigned long c)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=13) && (c<=1)))
        return 0x850040000500ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xf) + 8ll * ((c) & 0x1);
    __bdk_csr_fatal("NIXX_AF_RX_LINKX_SLX_SPKT_CNT", 3, a, b, c, 0);
}

#define typedef_BDK_NIXX_AF_RX_LINKX_SLX_SPKT_CNT(a,b,c) bdk_nixx_af_rx_linkx_slx_spkt_cnt_t
#define bustype_BDK_NIXX_AF_RX_LINKX_SLX_SPKT_CNT(a,b,c) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_LINKX_SLX_SPKT_CNT(a,b,c) "NIXX_AF_RX_LINKX_SLX_SPKT_CNT"
#define device_bar_BDK_NIXX_AF_RX_LINKX_SLX_SPKT_CNT(a,b,c) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_LINKX_SLX_SPKT_CNT(a,b,c) (a)
#define arguments_BDK_NIXX_AF_RX_LINKX_SLX_SPKT_CNT(a,b,c) (a),(b),(c),-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_link#_sl#_sxqe_cnt
 *
 * INTERNAL: NIX Receive Software Sync Link CQ Count Registers
 *
 * For diagnostic use only for debug of NIX_AF_RX_SW_SYNC[ENA] function. Index
 * {a} (LINK) is enumerated by NIX_LINK_E. Index {b} (SL) is zero for non-express
 * packets, one for express packets
 */
union bdk_nixx_af_rx_linkx_slx_sxqe_cnt
{
    uint64_t u;
    struct bdk_nixx_af_rx_linkx_slx_sxqe_cnt_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t out_cnt               : 32; /**< [ 63: 32](RO/H) Running count at output of machine. */
        uint64_t in_cnt                : 32; /**< [ 31:  0](RO/H) Running count at input of machine. */
#else /* Word 0 - Little Endian */
        uint64_t in_cnt                : 32; /**< [ 31:  0](RO/H) Running count at input of machine. */
        uint64_t out_cnt               : 32; /**< [ 63: 32](RO/H) Running count at output of machine. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_linkx_slx_sxqe_cnt_s cn; */
};
typedef union bdk_nixx_af_rx_linkx_slx_sxqe_cnt bdk_nixx_af_rx_linkx_slx_sxqe_cnt_t;

static inline uint64_t BDK_NIXX_AF_RX_LINKX_SLX_SXQE_CNT(unsigned long a, unsigned long b, unsigned long c) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_LINKX_SLX_SXQE_CNT(unsigned long a, unsigned long b, unsigned long c)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=13) && (c<=1)))
        return 0x850040000510ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xf) + 8ll * ((c) & 0x1);
    __bdk_csr_fatal("NIXX_AF_RX_LINKX_SLX_SXQE_CNT", 3, a, b, c, 0);
}

#define typedef_BDK_NIXX_AF_RX_LINKX_SLX_SXQE_CNT(a,b,c) bdk_nixx_af_rx_linkx_slx_sxqe_cnt_t
#define bustype_BDK_NIXX_AF_RX_LINKX_SLX_SXQE_CNT(a,b,c) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_LINKX_SLX_SXQE_CNT(a,b,c) "NIXX_AF_RX_LINKX_SLX_SXQE_CNT"
#define device_bar_BDK_NIXX_AF_RX_LINKX_SLX_SXQE_CNT(a,b,c) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_LINKX_SLX_SXQE_CNT(a,b,c) (a)
#define arguments_BDK_NIXX_AF_RX_LINKX_SLX_SXQE_CNT(a,b,c) (a),(b),(c),-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_mcast_base
 *
 * NIX AF Receive Multicast/Mirror Table Base Address Register
 * This register specifies the base RVU PF(0) IOVA of the receive multicast/mirror
 * table in NDC/LLC/DRAM. The table consists of 1 \<\< (NIX_AF_RX_MCAST_CFG[SIZE]+8)
 * contiguous NIX_RX_MCE_S structures. The size of each structure is
 * 1 \<\< NIX_AF_CONST3[MCE_LOG2BYTES] bytes.
 *
 * The table contains multicast/mirror replication lists. Each list consists of
 * linked entries with NIX_RX_MCE_S[EOL] = 1 in the last entry. All lists
 * must reside within the table size specified by NIX_AF_RX_MCAST_CFG[SIZE]. A
 * mirror replication list will typically consist of two entries, but that is not
 * checked or enforced by hardware.
 *
 * A receive packet is multicast when the action returned by NPC has
 * NIX_RX_ACTION_S[OP] = NIX_RX_ACTIONOP_E::MCAST.
 * A receive packet is mirrored when the action returned by NPC has
 * NIX_RX_ACTION_S[OP] = NIX_RX_ACTIONOP_E::MIRROR.
 * In both cases, NIX_RX_ACTION_S[INDEX] specifies the index of the replication
 * list's first NIX_RX_MCE_S in the table, and a linked entry with
 * NIX_RX_MCE_S[EOL] = 1 indicates the end of list.
 *
 * If a mirrored flow is part of a multicast replication list, software should
 * include the two mirror entries in that list.
 *
 * Internal:
 * A multicast list may have multiple entries for the same LF (e.g. for future
 * RoCE/IB multicast).
 */
union bdk_nixx_af_rx_mcast_base
{
    uint64_t u;
    struct bdk_nixx_af_rx_mcast_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of table in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_mcast_base_s cn; */
};
typedef union bdk_nixx_af_rx_mcast_base bdk_nixx_af_rx_mcast_base_t;

static inline uint64_t BDK_NIXX_AF_RX_MCAST_BASE(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_MCAST_BASE(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000100ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_MCAST_BASE", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_MCAST_BASE(a) bdk_nixx_af_rx_mcast_base_t
#define bustype_BDK_NIXX_AF_RX_MCAST_BASE(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_MCAST_BASE(a) "NIXX_AF_RX_MCAST_BASE"
#define device_bar_BDK_NIXX_AF_RX_MCAST_BASE(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_MCAST_BASE(a) (a)
#define arguments_BDK_NIXX_AF_RX_MCAST_BASE(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_mcast_buf_base
 *
 * NIX AF Receive Multicast Buffer Base Address Register
 * This register specifies the base RVU PF(0) IOVA of the receive multicast
 * buffers in NDC/LLC/DRAM. These buffers are used to temporarily store packets
 * whose action returned by NPC has NIX_RX_ACTION_S[OP] =
 * NIX_RX_ACTIONOP_E::MCAST. The number of buffers is configured by
 * NIX_AF_RX_MCAST_BUF_CFG[SIZE].
 *
 * If the number of free buffers is insufficient for a received multicast packet,
 * hardware tail drops the packet and sets NIX_AF_GEN_INT[RX_MCAST_DROP].
 *
 * Hardware prioritizes the processing of RX mirror packets over RX multicast
 * packets.
 */
union bdk_nixx_af_rx_mcast_buf_base
{
    uint64_t u;
    struct bdk_nixx_af_rx_mcast_buf_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of buffer in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of buffer in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_mcast_buf_base_s cn; */
};
typedef union bdk_nixx_af_rx_mcast_buf_base bdk_nixx_af_rx_mcast_buf_base_t;

static inline uint64_t BDK_NIXX_AF_RX_MCAST_BUF_BASE(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_MCAST_BUF_BASE(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000120ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_MCAST_BUF_BASE", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_MCAST_BUF_BASE(a) bdk_nixx_af_rx_mcast_buf_base_t
#define bustype_BDK_NIXX_AF_RX_MCAST_BUF_BASE(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_MCAST_BUF_BASE(a) "NIXX_AF_RX_MCAST_BUF_BASE"
#define device_bar_BDK_NIXX_AF_RX_MCAST_BUF_BASE(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_MCAST_BUF_BASE(a) (a)
#define arguments_BDK_NIXX_AF_RX_MCAST_BUF_BASE(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_mcast_buf_cfg
 *
 * NIX AF Receive Multicast Buffer Configuration Register
 * See NIX_AF_RX_MCAST_BUF_BASE.
 */
union bdk_nixx_af_rx_mcast_buf_cfg
{
    uint64_t u;
    struct bdk_nixx_af_rx_mcast_buf_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t ena                   : 1;  /**< [ 63: 63](R/W) Multicast buffer enable.
                                                                 0 = All new incoming MC packets will get discarded. Software should
                                                                 wait until all MC packets in flight are played out before re-enabling [ENA].
                                                                 1 = The temporary memory defined in NIX_AF_RX_MCAST_BUF_CFG[SIZE] is
                                                                 divided into equal size buffers as defined by NIX_AF_MC_MIRROR_CONST[BUF_SIZE]. */
        uint64_t reserved_43_62        : 20;
        uint64_t free_buf_level        : 11; /**< [ 42: 32](RO/H) Free buffer level. Number of available free buffers (out of the total
                                                                 [SIZE]). */
        uint64_t reserved_30_31        : 2;
        uint64_t npc_replay_pkind      : 6;  /**< [ 29: 24](R/W) Packet kind used on NPC request interface for all replayed copies of a
                                                                 multicasted packet. The NIX_RX_PARSE_S[PKIND] field for the replayed copies will
                                                                 be the original ingress PKIND. */
        uint64_t reserved_21_23        : 3;
        uint64_t caching               : 1;  /**< [ 20: 20](R/W) Selects the style of write and read to the LLC.
                                                                 0 = Writes and reads of buffer data will not allocate into the LLC.
                                                                 1 = Writes and reads of buffer data are allocated into the LLC. */
        uint64_t way_mask              : 16; /**< [ 19:  4](R/W) Way partitioning mask for allocating buffer data in NDC (1 means do not
                                                                 use). All ones disables allocation in NDC. */
        uint64_t size                  : 4;  /**< [  3:  0](R/W) Total number of buffers of size NIX_AF_MC_MIRROR_CONST[BUF_SIZE]:
                                                                 0x0 = 8 buffers.
                                                                 0x1 = 16 buffers.
                                                                 0x2 = 32 buffers.
                                                                 0x3 = 64 buffers.
                                                                 0x4 = 128 buffers.
                                                                 0x5 = 256 buffers.
                                                                 0x6 = 512 buffers.
                                                                 0x7 = 1024 buffers.
                                                                 0x8 = 2048 buffers.
                                                                 0x9-0xF = Reserved. */
#else /* Word 0 - Little Endian */
        uint64_t size                  : 4;  /**< [  3:  0](R/W) Total number of buffers of size NIX_AF_MC_MIRROR_CONST[BUF_SIZE]:
                                                                 0x0 = 8 buffers.
                                                                 0x1 = 16 buffers.
                                                                 0x2 = 32 buffers.
                                                                 0x3 = 64 buffers.
                                                                 0x4 = 128 buffers.
                                                                 0x5 = 256 buffers.
                                                                 0x6 = 512 buffers.
                                                                 0x7 = 1024 buffers.
                                                                 0x8 = 2048 buffers.
                                                                 0x9-0xF = Reserved. */
        uint64_t way_mask              : 16; /**< [ 19:  4](R/W) Way partitioning mask for allocating buffer data in NDC (1 means do not
                                                                 use). All ones disables allocation in NDC. */
        uint64_t caching               : 1;  /**< [ 20: 20](R/W) Selects the style of write and read to the LLC.
                                                                 0 = Writes and reads of buffer data will not allocate into the LLC.
                                                                 1 = Writes and reads of buffer data are allocated into the LLC. */
        uint64_t reserved_21_23        : 3;
        uint64_t npc_replay_pkind      : 6;  /**< [ 29: 24](R/W) Packet kind used on NPC request interface for all replayed copies of a
                                                                 multicasted packet. The NIX_RX_PARSE_S[PKIND] field for the replayed copies will
                                                                 be the original ingress PKIND. */
        uint64_t reserved_30_31        : 2;
        uint64_t free_buf_level        : 11; /**< [ 42: 32](RO/H) Free buffer level. Number of available free buffers (out of the total
                                                                 [SIZE]). */
        uint64_t reserved_43_62        : 20;
        uint64_t ena                   : 1;  /**< [ 63: 63](R/W) Multicast buffer enable.
                                                                 0 = All new incoming MC packets will get discarded. Software should
                                                                 wait until all MC packets in flight are played out before re-enabling [ENA].
                                                                 1 = The temporary memory defined in NIX_AF_RX_MCAST_BUF_CFG[SIZE] is
                                                                 divided into equal size buffers as defined by NIX_AF_MC_MIRROR_CONST[BUF_SIZE]. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_mcast_buf_cfg_s cn; */
};
typedef union bdk_nixx_af_rx_mcast_buf_cfg bdk_nixx_af_rx_mcast_buf_cfg_t;

static inline uint64_t BDK_NIXX_AF_RX_MCAST_BUF_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_MCAST_BUF_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000130ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_MCAST_BUF_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_MCAST_BUF_CFG(a) bdk_nixx_af_rx_mcast_buf_cfg_t
#define bustype_BDK_NIXX_AF_RX_MCAST_BUF_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_MCAST_BUF_CFG(a) "NIXX_AF_RX_MCAST_BUF_CFG"
#define device_bar_BDK_NIXX_AF_RX_MCAST_BUF_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_MCAST_BUF_CFG(a) (a)
#define arguments_BDK_NIXX_AF_RX_MCAST_BUF_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_mcast_cfg
 *
 * NIX AF Receive Multicast/Mirror Table Configuration Register
 * See NIX_AF_RX_MCAST_BASE.
 */
union bdk_nixx_af_rx_mcast_cfg
{
    uint64_t u;
    struct bdk_nixx_af_rx_mcast_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_37_63        : 27;
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing NIX_RX_MCE_S structures
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of NIX_RX_MCE_S data will not allocate into the LLC.
                                                                 1 = Writes and reads of NIX_RX_MCE_S data are allocated into the LLC. */
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating NIX_RX_MCE_S structures in NDC (1
                                                                 means do not use). All ones disables allocation in NDC. */
        uint64_t reserved_12_19        : 8;
        uint64_t max_list_lenm1        : 8;  /**< [ 11:  4](R/W) Maximum list length minus 1. If a multicast or mirror replication list exceeds this
                                                                 length (e.g. due to a loop in the NIX_RX_MCE_S link list), hardware
                                                                 terminates the list and sets NIX_AF_ERR_INT[RX_MCE_LIST_ERR]. */
        uint64_t size                  : 4;  /**< [  3:  0](R/W) Specifies table size in NIX_RX_MCE_S entries of eight bytes:
                                                                 0x0 = 256 entries.
                                                                 0x1 = 512 entries.
                                                                 0x2 = 1K entries.
                                                                 0x3 = 2K entries.
                                                                 0x4 = 4K entries.
                                                                 0x5 = 8K entries.
                                                                 0x6 = 16K entries.
                                                                 0x7 = 32K entries.
                                                                 0x8 = 64K entries.
                                                                 0x9-0xF = Reserved. */
#else /* Word 0 - Little Endian */
        uint64_t size                  : 4;  /**< [  3:  0](R/W) Specifies table size in NIX_RX_MCE_S entries of eight bytes:
                                                                 0x0 = 256 entries.
                                                                 0x1 = 512 entries.
                                                                 0x2 = 1K entries.
                                                                 0x3 = 2K entries.
                                                                 0x4 = 4K entries.
                                                                 0x5 = 8K entries.
                                                                 0x6 = 16K entries.
                                                                 0x7 = 32K entries.
                                                                 0x8 = 64K entries.
                                                                 0x9-0xF = Reserved. */
        uint64_t max_list_lenm1        : 8;  /**< [ 11:  4](R/W) Maximum list length minus 1. If a multicast or mirror replication list exceeds this
                                                                 length (e.g. due to a loop in the NIX_RX_MCE_S link list), hardware
                                                                 terminates the list and sets NIX_AF_ERR_INT[RX_MCE_LIST_ERR]. */
        uint64_t reserved_12_19        : 8;
        uint64_t way_mask              : 16; /**< [ 35: 20](R/W) Way partitioning mask for allocating NIX_RX_MCE_S structures in NDC (1
                                                                 means do not use). All ones disables allocation in NDC. */
        uint64_t caching               : 1;  /**< [ 36: 36](R/W) Selects the style of write and read for accessing NIX_RX_MCE_S structures
                                                                 in LLC/DRAM:
                                                                 0 = Writes and reads of NIX_RX_MCE_S data will not allocate into the LLC.
                                                                 1 = Writes and reads of NIX_RX_MCE_S data are allocated into the LLC. */
        uint64_t reserved_37_63        : 27;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_mcast_cfg_s cn; */
};
typedef union bdk_nixx_af_rx_mcast_cfg bdk_nixx_af_rx_mcast_cfg_t;

static inline uint64_t BDK_NIXX_AF_RX_MCAST_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_MCAST_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000110ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_MCAST_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_MCAST_CFG(a) bdk_nixx_af_rx_mcast_cfg_t
#define bustype_BDK_NIXX_AF_RX_MCAST_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_MCAST_CFG(a) "NIXX_AF_RX_MCAST_CFG"
#define device_bar_BDK_NIXX_AF_RX_MCAST_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_MCAST_CFG(a) (a)
#define arguments_BDK_NIXX_AF_RX_MCAST_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_mcast_jobs#_sw_cnt
 *
 * INTERNAL: NIX AF Receive Software Sync Multicast Jobs Registers
 *
 * For diagnostic use only for debug of the  NIX_AF_RX_SW_SYNC[ENA] function.
 * Index {a} (JOBS) value 0 is for MC jobs that have been received, value 1 is for
 * replayed MC packets.
 */
union bdk_nixx_af_rx_mcast_jobsx_sw_cnt
{
    uint64_t u;
    struct bdk_nixx_af_rx_mcast_jobsx_sw_cnt_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t out_cnt               : 32; /**< [ 63: 32](RO/H) Running count at output of machine. */
        uint64_t in_cnt                : 32; /**< [ 31:  0](RO/H) Running count at input of machine. */
#else /* Word 0 - Little Endian */
        uint64_t in_cnt                : 32; /**< [ 31:  0](RO/H) Running count at input of machine. */
        uint64_t out_cnt               : 32; /**< [ 63: 32](RO/H) Running count at output of machine. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_mcast_jobsx_sw_cnt_s cn; */
};
typedef union bdk_nixx_af_rx_mcast_jobsx_sw_cnt bdk_nixx_af_rx_mcast_jobsx_sw_cnt_t;

static inline uint64_t BDK_NIXX_AF_RX_MCAST_JOBSX_SW_CNT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_MCAST_JOBSX_SW_CNT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=1)))
        return 0x850040000520ll + 0x10000000ll * ((a) & 0x0) + 8ll * ((b) & 0x1);
    __bdk_csr_fatal("NIXX_AF_RX_MCAST_JOBSX_SW_CNT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_MCAST_JOBSX_SW_CNT(a,b) bdk_nixx_af_rx_mcast_jobsx_sw_cnt_t
#define bustype_BDK_NIXX_AF_RX_MCAST_JOBSX_SW_CNT(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_MCAST_JOBSX_SW_CNT(a,b) "NIXX_AF_RX_MCAST_JOBSX_SW_CNT"
#define device_bar_BDK_NIXX_AF_RX_MCAST_JOBSX_SW_CNT(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_MCAST_JOBSX_SW_CNT(a,b) (a)
#define arguments_BDK_NIXX_AF_RX_MCAST_JOBSX_SW_CNT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_mirror_buf_base
 *
 * NIX AF Receive Mirror Buffer Base Address Register
 * This register specifies the base RVU PF(0) IOVA of the receive mirror buffers
 * in NDC/LLC/DRAM. These buffers are used to temporarily store packets whose
 * action returned by NPC has NIX_RX_ACTION_S[OP] = NIX_RX_ACTIONOP_E::MIRROR. The
 * number of buffers is configured by NIX_AF_RX_MIRROR_BUF_CFG[SIZE].
 *
 * If the number of free buffers is insufficient for a received multicast packet,
 * hardware tail drops the packet and sets NIX_AF_GEN_INT[RX_MIRROR_DROP].
 *
 * Hardware prioritizes the processing of RX mirror packets over RX multicast
 * packets.
 */
union bdk_nixx_af_rx_mirror_buf_base
{
    uint64_t u;
    struct bdk_nixx_af_rx_mirror_buf_base_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of buffer in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_0_6          : 7;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_6          : 7;
        uint64_t addr                  : 46; /**< [ 52:  7](R/W) Base IOVA of buffer in LLC/DRAM. IOVA bits \<6:0\> are always
                                                                 zero. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_mirror_buf_base_s cn; */
};
typedef union bdk_nixx_af_rx_mirror_buf_base bdk_nixx_af_rx_mirror_buf_base_t;

static inline uint64_t BDK_NIXX_AF_RX_MIRROR_BUF_BASE(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_MIRROR_BUF_BASE(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000140ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_MIRROR_BUF_BASE", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_MIRROR_BUF_BASE(a) bdk_nixx_af_rx_mirror_buf_base_t
#define bustype_BDK_NIXX_AF_RX_MIRROR_BUF_BASE(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_MIRROR_BUF_BASE(a) "NIXX_AF_RX_MIRROR_BUF_BASE"
#define device_bar_BDK_NIXX_AF_RX_MIRROR_BUF_BASE(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_MIRROR_BUF_BASE(a) (a)
#define arguments_BDK_NIXX_AF_RX_MIRROR_BUF_BASE(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_mirror_buf_cfg
 *
 * NIX AF Receive Mirror Buffer Configuration Register
 * See NIX_AF_RX_MIRROR_BUF_BASE.
 */
union bdk_nixx_af_rx_mirror_buf_cfg
{
    uint64_t u;
    struct bdk_nixx_af_rx_mirror_buf_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t ena                   : 1;  /**< [ 63: 63](R/W) Multicast buffer enable.
                                                                 0 = All new incoming MC packets will get discarded. Software should
                                                                 wait until all MC packets in flight are played out before re-enabling [ENA].
                                                                 1 = The temporary memory defined in NIX_AF_RX_MCAST_BUF_CFG[SIZE] is
                                                                 divided into equal size buffers as defined by NIX_AF_MC_MIRROR_CONST[BUF_SIZE]. */
        uint64_t reserved_43_62        : 20;
        uint64_t free_buf_level        : 11; /**< [ 42: 32](RO/H) Free buffer level. Number of available free buffers (out of the total
                                                                 [SIZE]). */
        uint64_t reserved_30_31        : 2;
        uint64_t npc_replay_pkind      : 6;  /**< [ 29: 24](R/W) Packet kind used on NPC request interface for all replayed copies of a
                                                                 multicasted packet. The NIX_RX_PARSE_S[PKIND] field for the replayed copies will
                                                                 be the original ingress PKIND. */
        uint64_t reserved_21_23        : 3;
        uint64_t caching               : 1;  /**< [ 20: 20](R/W) Selects the style of write and read to the LLC.
                                                                 0 = Writes and reads of buffer data will not allocate into the LLC.
                                                                 1 = Writes and reads of buffer data are allocated into the LLC. */
        uint64_t way_mask              : 16; /**< [ 19:  4](R/W) Way partitioning mask for allocating buffer data in NDC (1 means do not
                                                                 use). All ones disables allocation in NDC. */
        uint64_t size                  : 4;  /**< [  3:  0](R/W) Total number of buffers of size NIX_AF_MC_MIRROR_CONST[BUF_SIZE]:
                                                                 0x0 = 8 buffers.
                                                                 0x1 = 16 buffers.
                                                                 0x2 = 32 buffers.
                                                                 0x3 = 64 buffers.
                                                                 0x4 = 128 buffers.
                                                                 0x5 = 256 buffers.
                                                                 0x6 = 512 buffers.
                                                                 0x7 = 1024 buffers.
                                                                 0x8 = 2048 buffers.
                                                                 0x9-0xF = Reserved. */
#else /* Word 0 - Little Endian */
        uint64_t size                  : 4;  /**< [  3:  0](R/W) Total number of buffers of size NIX_AF_MC_MIRROR_CONST[BUF_SIZE]:
                                                                 0x0 = 8 buffers.
                                                                 0x1 = 16 buffers.
                                                                 0x2 = 32 buffers.
                                                                 0x3 = 64 buffers.
                                                                 0x4 = 128 buffers.
                                                                 0x5 = 256 buffers.
                                                                 0x6 = 512 buffers.
                                                                 0x7 = 1024 buffers.
                                                                 0x8 = 2048 buffers.
                                                                 0x9-0xF = Reserved. */
        uint64_t way_mask              : 16; /**< [ 19:  4](R/W) Way partitioning mask for allocating buffer data in NDC (1 means do not
                                                                 use). All ones disables allocation in NDC. */
        uint64_t caching               : 1;  /**< [ 20: 20](R/W) Selects the style of write and read to the LLC.
                                                                 0 = Writes and reads of buffer data will not allocate into the LLC.
                                                                 1 = Writes and reads of buffer data are allocated into the LLC. */
        uint64_t reserved_21_23        : 3;
        uint64_t npc_replay_pkind      : 6;  /**< [ 29: 24](R/W) Packet kind used on NPC request interface for all replayed copies of a
                                                                 multicasted packet. The NIX_RX_PARSE_S[PKIND] field for the replayed copies will
                                                                 be the original ingress PKIND. */
        uint64_t reserved_30_31        : 2;
        uint64_t free_buf_level        : 11; /**< [ 42: 32](RO/H) Free buffer level. Number of available free buffers (out of the total
                                                                 [SIZE]). */
        uint64_t reserved_43_62        : 20;
        uint64_t ena                   : 1;  /**< [ 63: 63](R/W) Multicast buffer enable.
                                                                 0 = All new incoming MC packets will get discarded. Software should
                                                                 wait until all MC packets in flight are played out before re-enabling [ENA].
                                                                 1 = The temporary memory defined in NIX_AF_RX_MCAST_BUF_CFG[SIZE] is
                                                                 divided into equal size buffers as defined by NIX_AF_MC_MIRROR_CONST[BUF_SIZE]. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_mirror_buf_cfg_s cn; */
};
typedef union bdk_nixx_af_rx_mirror_buf_cfg bdk_nixx_af_rx_mirror_buf_cfg_t;

static inline uint64_t BDK_NIXX_AF_RX_MIRROR_BUF_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_MIRROR_BUF_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000148ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_MIRROR_BUF_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_MIRROR_BUF_CFG(a) bdk_nixx_af_rx_mirror_buf_cfg_t
#define bustype_BDK_NIXX_AF_RX_MIRROR_BUF_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_MIRROR_BUF_CFG(a) "NIXX_AF_RX_MIRROR_BUF_CFG"
#define device_bar_BDK_NIXX_AF_RX_MIRROR_BUF_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_MIRROR_BUF_CFG(a) (a)
#define arguments_BDK_NIXX_AF_RX_MIRROR_BUF_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_mirror_jobs#_sw_cnt
 *
 * INTERNAL: NIX AF Receive Software Sync MIRROR Jobs Registers
 *
 * For diagnostic use only for debug of the  NIX_AF_RX_SW_SYNC[ENA] function.
 * Index {a} (JOBS) value 0 is for mirror jobs that have been received, value 1 is
 * for replayed mirror packets.
 */
union bdk_nixx_af_rx_mirror_jobsx_sw_cnt
{
    uint64_t u;
    struct bdk_nixx_af_rx_mirror_jobsx_sw_cnt_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t out_cnt               : 32; /**< [ 63: 32](RO/H) Running count at output of machine. */
        uint64_t in_cnt                : 32; /**< [ 31:  0](RO/H) Running count at input of machine. */
#else /* Word 0 - Little Endian */
        uint64_t in_cnt                : 32; /**< [ 31:  0](RO/H) Running count at input of machine. */
        uint64_t out_cnt               : 32; /**< [ 63: 32](RO/H) Running count at output of machine. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_mirror_jobsx_sw_cnt_s cn; */
};
typedef union bdk_nixx_af_rx_mirror_jobsx_sw_cnt bdk_nixx_af_rx_mirror_jobsx_sw_cnt_t;

static inline uint64_t BDK_NIXX_AF_RX_MIRROR_JOBSX_SW_CNT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_MIRROR_JOBSX_SW_CNT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=1)))
        return 0x850040000530ll + 0x10000000ll * ((a) & 0x0) + 8ll * ((b) & 0x1);
    __bdk_csr_fatal("NIXX_AF_RX_MIRROR_JOBSX_SW_CNT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_MIRROR_JOBSX_SW_CNT(a,b) bdk_nixx_af_rx_mirror_jobsx_sw_cnt_t
#define bustype_BDK_NIXX_AF_RX_MIRROR_JOBSX_SW_CNT(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_MIRROR_JOBSX_SW_CNT(a,b) "NIXX_AF_RX_MIRROR_JOBSX_SW_CNT"
#define device_bar_BDK_NIXX_AF_RX_MIRROR_JOBSX_SW_CNT(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_MIRROR_JOBSX_SW_CNT(a,b) (a)
#define arguments_BDK_NIXX_AF_RX_MIRROR_JOBSX_SW_CNT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_npc_mc_drop
 *
 * NIX AF Multicast Drop Statistics Register
 * The counter increments for every dropped MC packet marked by the NPC.
 */
union bdk_nixx_af_rx_npc_mc_drop
{
    uint64_t u;
    struct bdk_nixx_af_rx_npc_mc_drop_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. */
#else /* Word 0 - Little Endian */
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_npc_mc_drop_s cn; */
};
typedef union bdk_nixx_af_rx_npc_mc_drop bdk_nixx_af_rx_npc_mc_drop_t;

static inline uint64_t BDK_NIXX_AF_RX_NPC_MC_DROP(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_NPC_MC_DROP(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040004710ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_NPC_MC_DROP", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_NPC_MC_DROP(a) bdk_nixx_af_rx_npc_mc_drop_t
#define bustype_BDK_NIXX_AF_RX_NPC_MC_DROP(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_NPC_MC_DROP(a) "NIXX_AF_RX_NPC_MC_DROP"
#define device_bar_BDK_NIXX_AF_RX_NPC_MC_DROP(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_NPC_MC_DROP(a) (a)
#define arguments_BDK_NIXX_AF_RX_NPC_MC_DROP(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_npc_mc_rcv
 *
 * NIX AF Multicast Receive Statistics Register
 * The counter increments for every recieved MC packet marked by the NPC.
 */
union bdk_nixx_af_rx_npc_mc_rcv
{
    uint64_t u;
    struct bdk_nixx_af_rx_npc_mc_rcv_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. */
#else /* Word 0 - Little Endian */
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_npc_mc_rcv_s cn; */
};
typedef union bdk_nixx_af_rx_npc_mc_rcv bdk_nixx_af_rx_npc_mc_rcv_t;

static inline uint64_t BDK_NIXX_AF_RX_NPC_MC_RCV(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_NPC_MC_RCV(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040004700ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_NPC_MC_RCV", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_NPC_MC_RCV(a) bdk_nixx_af_rx_npc_mc_rcv_t
#define bustype_BDK_NIXX_AF_RX_NPC_MC_RCV(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_NPC_MC_RCV(a) "NIXX_AF_RX_NPC_MC_RCV"
#define device_bar_BDK_NIXX_AF_RX_NPC_MC_RCV(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_NPC_MC_RCV(a) (a)
#define arguments_BDK_NIXX_AF_RX_NPC_MC_RCV(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_npc_mirror_drop
 *
 * NIX AF Mirror Drop Statistics Register
 * The counter increments for every dropped MIRROR packet marked by the NPC.
 */
union bdk_nixx_af_rx_npc_mirror_drop
{
    uint64_t u;
    struct bdk_nixx_af_rx_npc_mirror_drop_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. */
#else /* Word 0 - Little Endian */
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_npc_mirror_drop_s cn; */
};
typedef union bdk_nixx_af_rx_npc_mirror_drop bdk_nixx_af_rx_npc_mirror_drop_t;

static inline uint64_t BDK_NIXX_AF_RX_NPC_MIRROR_DROP(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_NPC_MIRROR_DROP(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040004730ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_NPC_MIRROR_DROP", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_NPC_MIRROR_DROP(a) bdk_nixx_af_rx_npc_mirror_drop_t
#define bustype_BDK_NIXX_AF_RX_NPC_MIRROR_DROP(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_NPC_MIRROR_DROP(a) "NIXX_AF_RX_NPC_MIRROR_DROP"
#define device_bar_BDK_NIXX_AF_RX_NPC_MIRROR_DROP(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_NPC_MIRROR_DROP(a) (a)
#define arguments_BDK_NIXX_AF_RX_NPC_MIRROR_DROP(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_npc_mirror_rcv
 *
 * NIX AF Mirror Receive Statistics Register
 * The counter increments for every recieved MIRROR packet marked by the NPC.
 */
union bdk_nixx_af_rx_npc_mirror_rcv
{
    uint64_t u;
    struct bdk_nixx_af_rx_npc_mirror_rcv_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. */
#else /* Word 0 - Little Endian */
        uint64_t stat                  : 48; /**< [ 47:  0](R/W/H) Statistic value. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_npc_mirror_rcv_s cn; */
};
typedef union bdk_nixx_af_rx_npc_mirror_rcv bdk_nixx_af_rx_npc_mirror_rcv_t;

static inline uint64_t BDK_NIXX_AF_RX_NPC_MIRROR_RCV(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_NPC_MIRROR_RCV(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040004720ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_NPC_MIRROR_RCV", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_NPC_MIRROR_RCV(a) bdk_nixx_af_rx_npc_mirror_rcv_t
#define bustype_BDK_NIXX_AF_RX_NPC_MIRROR_RCV(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_NPC_MIRROR_RCV(a) "NIXX_AF_RX_NPC_MIRROR_RCV"
#define device_bar_BDK_NIXX_AF_RX_NPC_MIRROR_RCV(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_NPC_MIRROR_RCV(a) (a)
#define arguments_BDK_NIXX_AF_RX_NPC_MIRROR_RCV(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_sw_sync
 *
 * NIX AF Receive Software Sync Register
 */
union bdk_nixx_af_rx_sw_sync
{
    uint64_t u;
    struct bdk_nixx_af_rx_sw_sync_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t ena                   : 1;  /**< [  0:  0](R/W) Sync enable. Software sets this bit to kick off a sync cycle on the RX path. This sync
                                                                 insures that all packets that were in flight are flushed out to memory. This can be used
                                                                 to assist in the tearing down of an active RQ. */
#else /* Word 0 - Little Endian */
        uint64_t ena                   : 1;  /**< [  0:  0](R/W) Sync enable. Software sets this bit to kick off a sync cycle on the RX path. This sync
                                                                 insures that all packets that were in flight are flushed out to memory. This can be used
                                                                 to assist in the tearing down of an active RQ. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_sw_sync_s cn; */
};
typedef union bdk_nixx_af_rx_sw_sync bdk_nixx_af_rx_sw_sync_t;

static inline uint64_t BDK_NIXX_AF_RX_SW_SYNC(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_SW_SYNC(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000550ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_SW_SYNC", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_SW_SYNC(a) bdk_nixx_af_rx_sw_sync_t
#define bustype_BDK_NIXX_AF_RX_SW_SYNC(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_SW_SYNC(a) "NIXX_AF_RX_SW_SYNC"
#define device_bar_BDK_NIXX_AF_RX_SW_SYNC(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_SW_SYNC(a) (a)
#define arguments_BDK_NIXX_AF_RX_SW_SYNC(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_rx_sw_sync_done
 *
 * NIX AF Receive Software Sync Done Register
 */
union bdk_nixx_af_rx_sw_sync_done
{
    uint64_t u;
    struct bdk_nixx_af_rx_sw_sync_done_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t done                  : 1;  /**< [  0:  0](RO/H) Set when NIX_AF_RX_SW_SYNC[ENA] is set and all packets that were in flight before
                                                                 the sync has been flushed out to memory. */
#else /* Word 0 - Little Endian */
        uint64_t done                  : 1;  /**< [  0:  0](RO/H) Set when NIX_AF_RX_SW_SYNC[ENA] is set and all packets that were in flight before
                                                                 the sync has been flushed out to memory. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_rx_sw_sync_done_s cn; */
};
typedef union bdk_nixx_af_rx_sw_sync_done bdk_nixx_af_rx_sw_sync_done_t;

static inline uint64_t BDK_NIXX_AF_RX_SW_SYNC_DONE(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_RX_SW_SYNC_DONE(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000560ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_RX_SW_SYNC_DONE", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_RX_SW_SYNC_DONE(a) bdk_nixx_af_rx_sw_sync_done_t
#define bustype_BDK_NIXX_AF_RX_SW_SYNC_DONE(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_RX_SW_SYNC_DONE(a) "NIXX_AF_RX_SW_SYNC_DONE"
#define device_bar_BDK_NIXX_AF_RX_SW_SYNC_DONE(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_RX_SW_SYNC_DONE(a) (a)
#define arguments_BDK_NIXX_AF_RX_SW_SYNC_DONE(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_sdp_hw_xoff#
 *
 * NIX AF SDP Transmit Link Hardware Controlled XOFF Registers
 * .
 */
union bdk_nixx_af_sdp_hw_xoffx
{
    uint64_t u;
    struct bdk_nixx_af_sdp_hw_xoffx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t chan_xoff             : 64; /**< [ 63:  0](RO/H) Channel hardware XOFF status.
                                                                 One bit per SDP channel (NIX_AF_SDP_HW_XOFF({n})[CHAN_XOFF]\<{m}\> for
                                                                 NIX_CHAN_E::SDP_CH(64*{n}+{m}).
                                                                 When a bit is set, indicates that the transmit channel is being
                                                                 backpressured (XOFF) by the link. */
#else /* Word 0 - Little Endian */
        uint64_t chan_xoff             : 64; /**< [ 63:  0](RO/H) Channel hardware XOFF status.
                                                                 One bit per SDP channel (NIX_AF_SDP_HW_XOFF({n})[CHAN_XOFF]\<{m}\> for
                                                                 NIX_CHAN_E::SDP_CH(64*{n}+{m}).
                                                                 When a bit is set, indicates that the transmit channel is being
                                                                 backpressured (XOFF) by the link. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_sdp_hw_xoffx_s cn; */
};
typedef union bdk_nixx_af_sdp_hw_xoffx bdk_nixx_af_sdp_hw_xoffx_t;

static inline uint64_t BDK_NIXX_AF_SDP_HW_XOFFX(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_SDP_HW_XOFFX(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=3)))
        return 0x850040000ac0ll + 0x10000000ll * ((a) & 0x0) + 8ll * ((b) & 0x3);
    __bdk_csr_fatal("NIXX_AF_SDP_HW_XOFFX", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_SDP_HW_XOFFX(a,b) bdk_nixx_af_sdp_hw_xoffx_t
#define bustype_BDK_NIXX_AF_SDP_HW_XOFFX(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_SDP_HW_XOFFX(a,b) "NIXX_AF_SDP_HW_XOFFX"
#define device_bar_BDK_NIXX_AF_SDP_HW_XOFFX(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_SDP_HW_XOFFX(a,b) (a)
#define arguments_BDK_NIXX_AF_SDP_HW_XOFFX(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_sdp_link_credit
 *
 * NIX AF Transmit Link SDP Credit Register
 * This register tracks SDP link credits.
 */
union bdk_nixx_af_sdp_link_credit
{
    uint64_t u;
    struct bdk_nixx_af_sdp_link_credit_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_32_63        : 32;
        uint64_t cc_unit_cnt           : 20; /**< [ 31: 12](R/W/H) See NIX_AF_TX_LINK()_NORM_CREDIT[CC_UNIT_CNT].

                                                                 The recommended configuration for SDP is
                                                                 [CC_UNIT_CNT] = (10 * Max_SDP_Data_Rate),
                                                                 e.g. [CC_UNIT_CNT] = 500 for 50 Gbps max SDP data rate. */
        uint64_t cc_packet_cnt         : 10; /**< [ 11:  2](R/W/H) See NIX_AF_TX_LINK()_NORM_CREDIT[CC_PACKET_CNT]. Must be less than to 512.
                                                                 Internal:
                                                                 Limited by the 512 PSE packet IDs for SDP. */
        uint64_t cc_enable             : 1;  /**< [  1:  1](R/W) Credit enable. Enables [CC_UNIT_CNT] and [CC_PACKET_CNT] link credit
                                                                 processing. Must be one when SDP is used. */
        uint64_t reserved_0            : 1;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0            : 1;
        uint64_t cc_enable             : 1;  /**< [  1:  1](R/W) Credit enable. Enables [CC_UNIT_CNT] and [CC_PACKET_CNT] link credit
                                                                 processing. Must be one when SDP is used. */
        uint64_t cc_packet_cnt         : 10; /**< [ 11:  2](R/W/H) See NIX_AF_TX_LINK()_NORM_CREDIT[CC_PACKET_CNT]. Must be less than to 512.
                                                                 Internal:
                                                                 Limited by the 512 PSE packet IDs for SDP. */
        uint64_t cc_unit_cnt           : 20; /**< [ 31: 12](R/W/H) See NIX_AF_TX_LINK()_NORM_CREDIT[CC_UNIT_CNT].

                                                                 The recommended configuration for SDP is
                                                                 [CC_UNIT_CNT] = (10 * Max_SDP_Data_Rate),
                                                                 e.g. [CC_UNIT_CNT] = 500 for 50 Gbps max SDP data rate. */
        uint64_t reserved_32_63        : 32;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_sdp_link_credit_s cn; */
};
typedef union bdk_nixx_af_sdp_link_credit bdk_nixx_af_sdp_link_credit_t;

static inline uint64_t BDK_NIXX_AF_SDP_LINK_CREDIT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_SDP_LINK_CREDIT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000a40ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_SDP_LINK_CREDIT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_SDP_LINK_CREDIT(a) bdk_nixx_af_sdp_link_credit_t
#define bustype_BDK_NIXX_AF_SDP_LINK_CREDIT(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_SDP_LINK_CREDIT(a) "NIXX_AF_SDP_LINK_CREDIT"
#define device_bar_BDK_NIXX_AF_SDP_LINK_CREDIT(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_SDP_LINK_CREDIT(a) (a)
#define arguments_BDK_NIXX_AF_SDP_LINK_CREDIT(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_sdp_sw_xoff#
 *
 * NIX AF SDP Transmit Link Software Controlled XOFF Registers
 */
union bdk_nixx_af_sdp_sw_xoffx
{
    uint64_t u;
    struct bdk_nixx_af_sdp_sw_xoffx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t chan_xoff             : 64; /**< [ 63:  0](R/W) Channel software controlled XOFF.
                                                                 One bit per SDP channel (NIX_AF_SDP_HW_XOFF({n})[CHAN_XOFF]\<{m}\> for
                                                                 NIX_CHAN_E::SDP_CH(64*{n}+{m}).
                                                                 When a bit is set, packets will not be transmitted on the associated
                                                                 channel. */
#else /* Word 0 - Little Endian */
        uint64_t chan_xoff             : 64; /**< [ 63:  0](R/W) Channel software controlled XOFF.
                                                                 One bit per SDP channel (NIX_AF_SDP_HW_XOFF({n})[CHAN_XOFF]\<{m}\> for
                                                                 NIX_CHAN_E::SDP_CH(64*{n}+{m}).
                                                                 When a bit is set, packets will not be transmitted on the associated
                                                                 channel. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_sdp_sw_xoffx_s cn; */
};
typedef union bdk_nixx_af_sdp_sw_xoffx bdk_nixx_af_sdp_sw_xoffx_t;

static inline uint64_t BDK_NIXX_AF_SDP_SW_XOFFX(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_SDP_SW_XOFFX(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=3)))
        return 0x850040000a60ll + 0x10000000ll * ((a) & 0x0) + 8ll * ((b) & 0x3);
    __bdk_csr_fatal("NIXX_AF_SDP_SW_XOFFX", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_SDP_SW_XOFFX(a,b) bdk_nixx_af_sdp_sw_xoffx_t
#define bustype_BDK_NIXX_AF_SDP_SW_XOFFX(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_SDP_SW_XOFFX(a,b) "NIXX_AF_SDP_SW_XOFFX"
#define device_bar_BDK_NIXX_AF_SDP_SW_XOFFX(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_SDP_SW_XOFFX(a,b) (a)
#define arguments_BDK_NIXX_AF_SDP_SW_XOFFX(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_sdp_tx_fifo_status
 *
 * NIX AF SDP Transmit FIFO Status Register
 * Status of FIFO which transmits packets to SDP.
 */
union bdk_nixx_af_sdp_tx_fifo_status
{
    uint64_t u;
    struct bdk_nixx_af_sdp_tx_fifo_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_12_63        : 52;
        uint64_t count                 : 12; /**< [ 11:  0](RO/H) Number of 128-bit entries in the TX FIFO. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 12; /**< [ 11:  0](RO/H) Number of 128-bit entries in the TX FIFO. */
        uint64_t reserved_12_63        : 52;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_sdp_tx_fifo_status_s cn; */
};
typedef union bdk_nixx_af_sdp_tx_fifo_status bdk_nixx_af_sdp_tx_fifo_status_t;

static inline uint64_t BDK_NIXX_AF_SDP_TX_FIFO_STATUS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_SDP_TX_FIFO_STATUS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000640ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_SDP_TX_FIFO_STATUS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_SDP_TX_FIFO_STATUS(a) bdk_nixx_af_sdp_tx_fifo_status_t
#define bustype_BDK_NIXX_AF_SDP_TX_FIFO_STATUS(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_SDP_TX_FIFO_STATUS(a) "NIXX_AF_SDP_TX_FIFO_STATUS"
#define device_bar_BDK_NIXX_AF_SDP_TX_FIFO_STATUS(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_SDP_TX_FIFO_STATUS(a) (a)
#define arguments_BDK_NIXX_AF_SDP_TX_FIFO_STATUS(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_seb_eco
 *
 * INTERNAL: SEB AF ECO Register
 *
 * Internal:
 * TODO - When CSI ECO register is added, switch this one to inherit from it.
 */
union bdk_nixx_af_seb_eco
{
    uint64_t u;
    struct bdk_nixx_af_seb_eco_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_32_63        : 32;
        uint64_t eco_rw                : 32; /**< [ 31:  0](R/W) Internal:
                                                                 Reserved for ECO usage. */
#else /* Word 0 - Little Endian */
        uint64_t eco_rw                : 32; /**< [ 31:  0](R/W) Internal:
                                                                 Reserved for ECO usage. */
        uint64_t reserved_32_63        : 32;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_seb_eco_s cn; */
};
typedef union bdk_nixx_af_seb_eco bdk_nixx_af_seb_eco_t;

static inline uint64_t BDK_NIXX_AF_SEB_ECO(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_SEB_ECO(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000600ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_SEB_ECO", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_SEB_ECO(a) bdk_nixx_af_seb_eco_t
#define bustype_BDK_NIXX_AF_SEB_ECO(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_SEB_ECO(a) "NIXX_AF_SEB_ECO"
#define device_bar_BDK_NIXX_AF_SEB_ECO(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_SEB_ECO(a) (a)
#define arguments_BDK_NIXX_AF_SEB_ECO(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_seb_test_bp
 *
 * NIX AF SEB Backpressure Test Register
 */
union bdk_nixx_af_seb_test_bp
{
    uint64_t u;
    struct bdk_nixx_af_seb_test_bp_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t tbd                   : 64; /**< [ 63:  0](R/W) TBD.
                                                                 Internal:
                                                                 FIXME. */
#else /* Word 0 - Little Endian */
        uint64_t tbd                   : 64; /**< [ 63:  0](R/W) TBD.
                                                                 Internal:
                                                                 FIXME. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_seb_test_bp_s cn; */
};
typedef union bdk_nixx_af_seb_test_bp bdk_nixx_af_seb_test_bp_t;

static inline uint64_t BDK_NIXX_AF_SEB_TEST_BP(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_SEB_TEST_BP(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000610ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_SEB_TEST_BP", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_SEB_TEST_BP(a) bdk_nixx_af_seb_test_bp_t
#define bustype_BDK_NIXX_AF_SEB_TEST_BP(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_SEB_TEST_BP(a) "NIXX_AF_SEB_TEST_BP"
#define device_bar_BDK_NIXX_AF_SEB_TEST_BP(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_SEB_TEST_BP(a) (a)
#define arguments_BDK_NIXX_AF_SEB_TEST_BP(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_smq#_cfg
 *
 * NIX AF SQM PSE Queue Configuration Registers
 */
union bdk_nixx_af_smqx_cfg
{
    uint64_t u;
    struct bdk_nixx_af_smqx_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_42_63        : 22;
        uint64_t enq_xoff              : 1;  /**< [ 41: 41](R/W) Enqueue transmit off. When set, hardware will not enqueue meta-descriptorrs
                                                                 to the SMQ. */
        uint64_t flush                 : 1;  /**< [ 40: 40](R/W1S/H) Software can write a one to set this bit and initiate an SMQ flush.
                                                                 When set, hardware flushes all meta-descriptors/packets from this SMQ
                                                                 through PSE and the send data path. Hardware clears this bit and sets
                                                                 NIX_AF_GEN_INT[SMQ_FLUSH_DONE] when the flush operation is complete,

                                                                 [ENQ_XOFF] must be set whenever this bit is set.

                                                                 Must not be set when a DRAIN command is active
                                                                 (NIX_AF_TL*()_SW_XOFF[DRAIN,DRAIN_IRQ] command has been issued and
                                                                 resulting NIX_AF_GEN_INT[TL1_DRAIN] is not set).

                                                                 Under some circumstances, the SMQ flush operation may stall when a
                                                                 downstream TL3/TL2 queue is backpressured by link credits from
                                                                 NIX_AF_TX_LINK()_NORMAL_CREDIT or NIX_AF_TX_LINK()_EXPRESS_CREDIT. If the
                                                                 backpressure does not go away, software may need to disable it at the
                                                                 destination link(s), e.g. by clearing CGX()_SMU()_RX_FRM_CTL[CTL_DRP] for a
                                                                 10G+ CGX LMAC.

                                                                 Internal:
                                                                 The SMQ injects a flush meta descriptor (FMD) that follows the same path as
                                                                 any normal packet meta descriptor (PMD) from the same SMQ.

                                                                 PSE removes and returns to SQM any PMD from the same SMQ intercepted by the
                                                                 FMD up to the PSE backpressure level (TL3/TL2 for non-SDP PMDs, TL4 for SDP
                                                                 PMDs). This ensures that removed PMDs have not consumed any link/TX credits
                                                                 and allows the FMD to move forward unaffected by backpressure. SQM uses the
                                                                 removed/returned PMD to free associated resources.

                                                                 When the TL3/TL2 level for a non-SDP PMD feeds to multiple normal or
                                                                 express links and some of the links are backpressured by link credits, the
                                                                 PMD may stall an FMD for the same SMQ until the PMD has consumed credits
                                                                 for all the links.

                                                                 After the backpressure level, the FMD follows any PMDs from the same SMQ
                                                                 through the remaining PSE levels, SQM_DSE and SEB. SQM clears this bit and
                                                                 sets NIX_AF_GEN_INT[SMQ_FLUSH_DONE] after it gets the packet done for the
                                                                 FMD from SEB. */
        uint64_t express               : 1;  /**< [ 39: 39](R/W) Express.
                                                                 0 = The SMQ transmits normal packets.
                                                                 1 = The SMQ transmits express packets.

                                                                 Must have the same value as the corresponding
                                                                 NIX_AF_TL3_TL2()_CFG[EXPRESS]. Hardware prioritizes enqueue to express SMQs
                                                                 over normal SMQs. */
        uint64_t max_vtag_ins          : 3;  /**< [ 38: 36](R/W) Maximum Vtag insertion size as a as a multiple of four bytes. Must be less
                                                                 than or equal to four (16 bytes), and must be large enough to account for the
                                                                 maximum number of bytes inserted by NIX_TX_VTAG_ACTION_S for any packet
                                                                 sent through this SMQ.

                                                                 Internal:
                                                                 SQM computes allowed maximum Vtag insertion bytes (ok_vtag_max) such that
                                                                 the computed packet size does not exceed [MAXLEN], including VLAN bytes
                                                                 inserted by NIX_SEND_EXT_S[VLAN*]. SEB enforces ok_vtag_max when inserting
                                                                 Vtag bytes based on NIX_TX_VTAG_ACTION_S. */
        uint64_t reserved_32_35        : 4;
        uint64_t lf                    : 8;  /**< [ 31: 24](R/W) Local function with SQs that may feed this SMQ. Software must ensure NIX_SQ_CTX_S[SMQ]
                                                                 does not point to this SMQ for any SQ outside of this LF. */
        uint64_t maxlen                : 16; /**< [ 23:  8](R/W) Maximum packet length in bytes, including optional VLAN bytes inserted by
                                                                 NIX_SEND_EXT_S[VLAN*] and Vtag bytes inserted by NIX_TX_VTAG_ACTION_S,
                                                                 but excluding FCS potentially appended outside NIX by CGX.

                                                                 Must not be less than [MINLEN].
                                                                 Must not exceed 9212 (9216 minus four byte FCS) if the SMQ transmits to
                                                                 CGX and LBK. May be set to a larger value (up to 65535 bytes) if the SMQ
                                                                 transmits to SDP (corresponding NIX_AF_TL4()_SDP_LINK_CFG[ENA] is set).

                                                                 Software should set a value that does not exceed the MTU of any link to
                                                                 which the SMQ can transmit. */
        uint64_t desc_shp_ctl_dis      : 1;  /**< [  7:  7](R/W) Descriptor shaper control disable for packets transmitted by this SMQ.
                                                                 0 = NIX_SEND_EXT_S[SHP_RA,SHP_DIS,SHP_CHG] values are used when present in
                                                                 the descriptor.
                                                                 1 = NIX_SEND_EXT_S[SHP_RA,SHP_DIS,SHP_CHG] values in the send descriptor,
                                                                 are ignored and treated as 0. */
        uint64_t minlen                : 7;  /**< [  6:  0](R/W) Minimum packet length in bytes, excluding FCS potentially appended outside
                                                                 NIX by CGX. Packets smaller than the minimum length from this SMQ,
                                                                 including optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and Vtag
                                                                 bytes inserted by NIX_TX_VTAG_ACTION_S, are padded with zeros. Software
                                                                 should program this to match the minimum length for all links that this SMQ
                                                                 can transmit to.

                                                                 Must be greater than 16. The default value ensures the pre-FCS packet is at
                                                                 least 60 bytes. */
#else /* Word 0 - Little Endian */
        uint64_t minlen                : 7;  /**< [  6:  0](R/W) Minimum packet length in bytes, excluding FCS potentially appended outside
                                                                 NIX by CGX. Packets smaller than the minimum length from this SMQ,
                                                                 including optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and Vtag
                                                                 bytes inserted by NIX_TX_VTAG_ACTION_S, are padded with zeros. Software
                                                                 should program this to match the minimum length for all links that this SMQ
                                                                 can transmit to.

                                                                 Must be greater than 16. The default value ensures the pre-FCS packet is at
                                                                 least 60 bytes. */
        uint64_t desc_shp_ctl_dis      : 1;  /**< [  7:  7](R/W) Descriptor shaper control disable for packets transmitted by this SMQ.
                                                                 0 = NIX_SEND_EXT_S[SHP_RA,SHP_DIS,SHP_CHG] values are used when present in
                                                                 the descriptor.
                                                                 1 = NIX_SEND_EXT_S[SHP_RA,SHP_DIS,SHP_CHG] values in the send descriptor,
                                                                 are ignored and treated as 0. */
        uint64_t maxlen                : 16; /**< [ 23:  8](R/W) Maximum packet length in bytes, including optional VLAN bytes inserted by
                                                                 NIX_SEND_EXT_S[VLAN*] and Vtag bytes inserted by NIX_TX_VTAG_ACTION_S,
                                                                 but excluding FCS potentially appended outside NIX by CGX.

                                                                 Must not be less than [MINLEN].
                                                                 Must not exceed 9212 (9216 minus four byte FCS) if the SMQ transmits to
                                                                 CGX and LBK. May be set to a larger value (up to 65535 bytes) if the SMQ
                                                                 transmits to SDP (corresponding NIX_AF_TL4()_SDP_LINK_CFG[ENA] is set).

                                                                 Software should set a value that does not exceed the MTU of any link to
                                                                 which the SMQ can transmit. */
        uint64_t lf                    : 8;  /**< [ 31: 24](R/W) Local function with SQs that may feed this SMQ. Software must ensure NIX_SQ_CTX_S[SMQ]
                                                                 does not point to this SMQ for any SQ outside of this LF. */
        uint64_t reserved_32_35        : 4;
        uint64_t max_vtag_ins          : 3;  /**< [ 38: 36](R/W) Maximum Vtag insertion size as a as a multiple of four bytes. Must be less
                                                                 than or equal to four (16 bytes), and must be large enough to account for the
                                                                 maximum number of bytes inserted by NIX_TX_VTAG_ACTION_S for any packet
                                                                 sent through this SMQ.

                                                                 Internal:
                                                                 SQM computes allowed maximum Vtag insertion bytes (ok_vtag_max) such that
                                                                 the computed packet size does not exceed [MAXLEN], including VLAN bytes
                                                                 inserted by NIX_SEND_EXT_S[VLAN*]. SEB enforces ok_vtag_max when inserting
                                                                 Vtag bytes based on NIX_TX_VTAG_ACTION_S. */
        uint64_t express               : 1;  /**< [ 39: 39](R/W) Express.
                                                                 0 = The SMQ transmits normal packets.
                                                                 1 = The SMQ transmits express packets.

                                                                 Must have the same value as the corresponding
                                                                 NIX_AF_TL3_TL2()_CFG[EXPRESS]. Hardware prioritizes enqueue to express SMQs
                                                                 over normal SMQs. */
        uint64_t flush                 : 1;  /**< [ 40: 40](R/W1S/H) Software can write a one to set this bit and initiate an SMQ flush.
                                                                 When set, hardware flushes all meta-descriptors/packets from this SMQ
                                                                 through PSE and the send data path. Hardware clears this bit and sets
                                                                 NIX_AF_GEN_INT[SMQ_FLUSH_DONE] when the flush operation is complete,

                                                                 [ENQ_XOFF] must be set whenever this bit is set.

                                                                 Must not be set when a DRAIN command is active
                                                                 (NIX_AF_TL*()_SW_XOFF[DRAIN,DRAIN_IRQ] command has been issued and
                                                                 resulting NIX_AF_GEN_INT[TL1_DRAIN] is not set).

                                                                 Under some circumstances, the SMQ flush operation may stall when a
                                                                 downstream TL3/TL2 queue is backpressured by link credits from
                                                                 NIX_AF_TX_LINK()_NORMAL_CREDIT or NIX_AF_TX_LINK()_EXPRESS_CREDIT. If the
                                                                 backpressure does not go away, software may need to disable it at the
                                                                 destination link(s), e.g. by clearing CGX()_SMU()_RX_FRM_CTL[CTL_DRP] for a
                                                                 10G+ CGX LMAC.

                                                                 Internal:
                                                                 The SMQ injects a flush meta descriptor (FMD) that follows the same path as
                                                                 any normal packet meta descriptor (PMD) from the same SMQ.

                                                                 PSE removes and returns to SQM any PMD from the same SMQ intercepted by the
                                                                 FMD up to the PSE backpressure level (TL3/TL2 for non-SDP PMDs, TL4 for SDP
                                                                 PMDs). This ensures that removed PMDs have not consumed any link/TX credits
                                                                 and allows the FMD to move forward unaffected by backpressure. SQM uses the
                                                                 removed/returned PMD to free associated resources.

                                                                 When the TL3/TL2 level for a non-SDP PMD feeds to multiple normal or
                                                                 express links and some of the links are backpressured by link credits, the
                                                                 PMD may stall an FMD for the same SMQ until the PMD has consumed credits
                                                                 for all the links.

                                                                 After the backpressure level, the FMD follows any PMDs from the same SMQ
                                                                 through the remaining PSE levels, SQM_DSE and SEB. SQM clears this bit and
                                                                 sets NIX_AF_GEN_INT[SMQ_FLUSH_DONE] after it gets the packet done for the
                                                                 FMD from SEB. */
        uint64_t enq_xoff              : 1;  /**< [ 41: 41](R/W) Enqueue transmit off. When set, hardware will not enqueue meta-descriptorrs
                                                                 to the SMQ. */
        uint64_t reserved_42_63        : 22;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_smqx_cfg_s cn; */
};
typedef union bdk_nixx_af_smqx_cfg bdk_nixx_af_smqx_cfg_t;

static inline uint64_t BDK_NIXX_AF_SMQX_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_SMQX_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040000700ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_SMQX_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_SMQX_CFG(a,b) bdk_nixx_af_smqx_cfg_t
#define bustype_BDK_NIXX_AF_SMQX_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_SMQX_CFG(a,b) "NIXX_AF_SMQX_CFG"
#define device_bar_BDK_NIXX_AF_SMQX_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_SMQX_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_SMQX_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_sq_const
 *
 * NIX AF SQ Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_sq_const
{
    uint64_t u;
    struct bdk_nixx_af_sq_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_50_63        : 14;
        uint64_t sqb_size              : 16; /**< [ 49: 34](RO) Number of bytes in each hardware-allocated SQE buffer (SQB) in NDC/LLC/DRAM. */
        uint64_t smq                   : 10; /**< [ 33: 24](RO) Number of send meta-descriptor queues. */
        uint64_t queues_per_lf         : 24; /**< [ 23:  0](RO) Maximum number of send queues per LF. */
#else /* Word 0 - Little Endian */
        uint64_t queues_per_lf         : 24; /**< [ 23:  0](RO) Maximum number of send queues per LF. */
        uint64_t smq                   : 10; /**< [ 33: 24](RO) Number of send meta-descriptor queues. */
        uint64_t sqb_size              : 16; /**< [ 49: 34](RO) Number of bytes in each hardware-allocated SQE buffer (SQB) in NDC/LLC/DRAM. */
        uint64_t reserved_50_63        : 14;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_sq_const_s cn; */
};
typedef union bdk_nixx_af_sq_const bdk_nixx_af_sq_const_t;

static inline uint64_t BDK_NIXX_AF_SQ_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_SQ_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000040ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_SQ_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_SQ_CONST(a) bdk_nixx_af_sq_const_t
#define bustype_BDK_NIXX_AF_SQ_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_SQ_CONST(a) "NIXX_AF_SQ_CONST"
#define device_bar_BDK_NIXX_AF_SQ_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_SQ_CONST(a) (a)
#define arguments_BDK_NIXX_AF_SQ_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_status
 *
 * NIX AF General Status Register
 */
union bdk_nixx_af_status
{
    uint64_t u;
    struct bdk_nixx_af_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_21_63        : 43;
        uint64_t calibrate_status      : 5;  /**< [ 20: 16](RO/H) X2P device calibration state.
                                                                 0 = Device inactive.
                                                                 1 = Device ready.

                                                                 Internal:
                                                                 a "Device Inactive" status means that the X2P agent did not respond to the calibration
                                                                 cycle.
                                                                 This is most likely caused because the X2P agents (CGX, LBK, etc) was in reset during the
                                                                 calibration cycle. */
        uint64_t reserved_11_15        : 5;
        uint64_t calibrate_done        : 1;  /**< [ 10: 10](RO/H) Calibrate cycle is complete. */
        uint64_t blk_busy              : 10; /**< [  9:  0](RO/H) If nonzero, block is not ready for configuration.
                                                                 Internal:
                                                                 Each bit corresponds to a subblock:
                                                                 \<9\> = Reserved.
                                                                 \<8\> = Reserved.
                                                                 \<7\> = TBD.
                                                                 \<6\> = TBD.
                                                                 \<5\> = TBD.
                                                                 \<4\> = TBD.
                                                                 \<3\> = TBD.
                                                                 \<2\> = TBD.
                                                                 \<1\> = TBD.
                                                                 \<0\> = TBD. */
#else /* Word 0 - Little Endian */
        uint64_t blk_busy              : 10; /**< [  9:  0](RO/H) If nonzero, block is not ready for configuration.
                                                                 Internal:
                                                                 Each bit corresponds to a subblock:
                                                                 \<9\> = Reserved.
                                                                 \<8\> = Reserved.
                                                                 \<7\> = TBD.
                                                                 \<6\> = TBD.
                                                                 \<5\> = TBD.
                                                                 \<4\> = TBD.
                                                                 \<3\> = TBD.
                                                                 \<2\> = TBD.
                                                                 \<1\> = TBD.
                                                                 \<0\> = TBD. */
        uint64_t calibrate_done        : 1;  /**< [ 10: 10](RO/H) Calibrate cycle is complete. */
        uint64_t reserved_11_15        : 5;
        uint64_t calibrate_status      : 5;  /**< [ 20: 16](RO/H) X2P device calibration state.
                                                                 0 = Device inactive.
                                                                 1 = Device ready.

                                                                 Internal:
                                                                 a "Device Inactive" status means that the X2P agent did not respond to the calibration
                                                                 cycle.
                                                                 This is most likely caused because the X2P agents (CGX, LBK, etc) was in reset during the
                                                                 calibration cycle. */
        uint64_t reserved_21_63        : 43;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_status_s cn; */
};
typedef union bdk_nixx_af_status bdk_nixx_af_status_t;

static inline uint64_t BDK_NIXX_AF_STATUS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_STATUS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000010ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_STATUS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_STATUS(a) bdk_nixx_af_status_t
#define bustype_BDK_NIXX_AF_STATUS(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_STATUS(a) "NIXX_AF_STATUS"
#define device_bar_BDK_NIXX_AF_STATUS(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_STATUS(a) (a)
#define arguments_BDK_NIXX_AF_STATUS(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tcp_timer
 *
 * NIX TCP Timer Register
 */
union bdk_nixx_af_tcp_timer
{
    uint64_t u;
    struct bdk_nixx_af_tcp_timer_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t ena                   : 1;  /**< [ 63: 63](R/W) TCP timer enable. When clear, [LF_COUNTER] and [DUR_COUNTER] are forced to
                                                                 0 and timer events will not be posted. */
        uint64_t reserved_48_62        : 15;
        uint64_t duration              : 16; /**< [ 47: 32](R/W) Duration of the TCP timer as a multiple of 100 nanoseconds. */
        uint64_t reserved_24_31        : 8;
        uint64_t lf_counter            : 8;  /**< [ 23: 16](RO/H) LF Counter. Updated when [DUR_COUNTER] wraps around to select the next LF
                                                                 for which a timer event is generated. Cycles through values 0 through
                                                                 NIX_AF_CONST2[LFS]-1. */
        uint64_t dur_counter           : 16; /**< [ 15:  0](RO/H) Periodic counter that wraps around every [DURATION]*100 nanoseconds.
                                                                 Enabled when [ENA] is set. */
#else /* Word 0 - Little Endian */
        uint64_t dur_counter           : 16; /**< [ 15:  0](RO/H) Periodic counter that wraps around every [DURATION]*100 nanoseconds.
                                                                 Enabled when [ENA] is set. */
        uint64_t lf_counter            : 8;  /**< [ 23: 16](RO/H) LF Counter. Updated when [DUR_COUNTER] wraps around to select the next LF
                                                                 for which a timer event is generated. Cycles through values 0 through
                                                                 NIX_AF_CONST2[LFS]-1. */
        uint64_t reserved_24_31        : 8;
        uint64_t duration              : 16; /**< [ 47: 32](R/W) Duration of the TCP timer as a multiple of 100 nanoseconds. */
        uint64_t reserved_48_62        : 15;
        uint64_t ena                   : 1;  /**< [ 63: 63](R/W) TCP timer enable. When clear, [LF_COUNTER] and [DUR_COUNTER] are forced to
                                                                 0 and timer events will not be posted. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tcp_timer_s cn; */
};
typedef union bdk_nixx_af_tcp_timer bdk_nixx_af_tcp_timer_t;

static inline uint64_t BDK_NIXX_AF_TCP_TIMER(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TCP_TIMER(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400001e0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TCP_TIMER", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TCP_TIMER(a) bdk_nixx_af_tcp_timer_t
#define bustype_BDK_NIXX_AF_TCP_TIMER(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TCP_TIMER(a) "NIXX_AF_TCP_TIMER"
#define device_bar_BDK_NIXX_AF_TCP_TIMER(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TCP_TIMER(a) (a)
#define arguments_BDK_NIXX_AF_TCP_TIMER(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_cir
 *
 * NIX AF Transmit Level 1 Committed Information Rate Register
 */
union bdk_nixx_af_tl1x_cir
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_cir_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_17_28        : 12;
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
#else /* Word 0 - Little Endian */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t reserved_17_28        : 12;
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_cir_s cn; */
};
typedef union bdk_nixx_af_tl1x_cir bdk_nixx_af_tl1x_cir_t;

static inline uint64_t BDK_NIXX_AF_TL1X_CIR(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_CIR(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000c20ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_CIR", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_CIR(a,b) bdk_nixx_af_tl1x_cir_t
#define bustype_BDK_NIXX_AF_TL1X_CIR(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_CIR(a,b) "NIXX_AF_TL1X_CIR"
#define device_bar_BDK_NIXX_AF_TL1X_CIR(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_CIR(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_CIR(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_dropped_bytes
 *
 * NIX AF Transmit Level 1 Dropped Bytes Registers
 * This register has the same bit fields as NIX_AF_TL1()_GREEN_BYTES.
 */
union bdk_nixx_af_tl1x_dropped_bytes
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_dropped_bytes_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t count                 : 48; /**< [ 47:  0](R/W/H) Count. The running count of bytes. Note that this count wraps. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 48; /**< [ 47:  0](R/W/H) Count. The running count of bytes. Note that this count wraps. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_dropped_bytes_s cn; */
};
typedef union bdk_nixx_af_tl1x_dropped_bytes bdk_nixx_af_tl1x_dropped_bytes_t;

static inline uint64_t BDK_NIXX_AF_TL1X_DROPPED_BYTES(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_DROPPED_BYTES(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000d30ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_DROPPED_BYTES", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_DROPPED_BYTES(a,b) bdk_nixx_af_tl1x_dropped_bytes_t
#define bustype_BDK_NIXX_AF_TL1X_DROPPED_BYTES(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_DROPPED_BYTES(a,b) "NIXX_AF_TL1X_DROPPED_BYTES"
#define device_bar_BDK_NIXX_AF_TL1X_DROPPED_BYTES(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_DROPPED_BYTES(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_DROPPED_BYTES(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_dropped_packets
 *
 * NIX AF Transmit Level 1 Dropped Packets Registers
 * This register has the same bit fields as NIX_AF_TL1()_GREEN_PACKETS.
 */
union bdk_nixx_af_tl1x_dropped_packets
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_dropped_packets_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_40_63        : 24;
        uint64_t count                 : 40; /**< [ 39:  0](R/W/H) Count. The running count of packets. Note that this count wraps. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 40; /**< [ 39:  0](R/W/H) Count. The running count of packets. Note that this count wraps. */
        uint64_t reserved_40_63        : 24;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_dropped_packets_s cn; */
};
typedef union bdk_nixx_af_tl1x_dropped_packets bdk_nixx_af_tl1x_dropped_packets_t;

static inline uint64_t BDK_NIXX_AF_TL1X_DROPPED_PACKETS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_DROPPED_PACKETS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000d20ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_DROPPED_PACKETS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_DROPPED_PACKETS(a,b) bdk_nixx_af_tl1x_dropped_packets_t
#define bustype_BDK_NIXX_AF_TL1X_DROPPED_PACKETS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_DROPPED_PACKETS(a,b) "NIXX_AF_TL1X_DROPPED_PACKETS"
#define device_bar_BDK_NIXX_AF_TL1X_DROPPED_PACKETS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_DROPPED_PACKETS(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_DROPPED_PACKETS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_green
 *
 * INTERNAL: NIX Transmit Level 1 Green State Debug Register
 */
union bdk_nixx_af_tl1x_green
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_green_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t rr_active             : 1;  /**< [ 40: 40](R/W/H) Round-robin red active. Set when the RED_SEND+RED_DROP DWRR child list is not empty.
                                                                 For internal use only. */
        uint64_t active_vec            : 20; /**< [ 39: 20](R/W/H) Active vector. A 20-bit vector, two bits per each of the 10 supported priorities.
                                                                 For the non-RR_PRIO priorities, the two bits encode whether the child is active
                                                                 GREEN, active YELLOW, active RED_SEND+RED_DROP, or inactive. At RR_PRIO, one
                                                                 bit is set if the GREEN DWRR child list is not empty, and the other is set if the
                                                                 YELLOW DWRR child list is not empty. For internal use only. */
        uint64_t reserved_18_19        : 2;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_18_19        : 2;
        uint64_t active_vec            : 20; /**< [ 39: 20](R/W/H) Active vector. A 20-bit vector, two bits per each of the 10 supported priorities.
                                                                 For the non-RR_PRIO priorities, the two bits encode whether the child is active
                                                                 GREEN, active YELLOW, active RED_SEND+RED_DROP, or inactive. At RR_PRIO, one
                                                                 bit is set if the GREEN DWRR child list is not empty, and the other is set if the
                                                                 YELLOW DWRR child list is not empty. For internal use only. */
        uint64_t rr_active             : 1;  /**< [ 40: 40](R/W/H) Round-robin red active. Set when the RED_SEND+RED_DROP DWRR child list is not empty.
                                                                 For internal use only. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_green_s cn; */
};
typedef union bdk_nixx_af_tl1x_green bdk_nixx_af_tl1x_green_t;

static inline uint64_t BDK_NIXX_AF_TL1X_GREEN(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_GREEN(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000c90ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_GREEN", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_GREEN(a,b) bdk_nixx_af_tl1x_green_t
#define bustype_BDK_NIXX_AF_TL1X_GREEN(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_GREEN(a,b) "NIXX_AF_TL1X_GREEN"
#define device_bar_BDK_NIXX_AF_TL1X_GREEN(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_GREEN(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_GREEN(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_green_bytes
 *
 * NIX AF Transmit Level 1 Green Sent Bytes Registers
 */
union bdk_nixx_af_tl1x_green_bytes
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_green_bytes_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t count                 : 48; /**< [ 47:  0](R/W/H) Count. The running count of bytes. Note that this count wraps. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 48; /**< [ 47:  0](R/W/H) Count. The running count of bytes. Note that this count wraps. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_green_bytes_s cn; */
};
typedef union bdk_nixx_af_tl1x_green_bytes bdk_nixx_af_tl1x_green_bytes_t;

static inline uint64_t BDK_NIXX_AF_TL1X_GREEN_BYTES(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_GREEN_BYTES(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000d90ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_GREEN_BYTES", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_GREEN_BYTES(a,b) bdk_nixx_af_tl1x_green_bytes_t
#define bustype_BDK_NIXX_AF_TL1X_GREEN_BYTES(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_GREEN_BYTES(a,b) "NIXX_AF_TL1X_GREEN_BYTES"
#define device_bar_BDK_NIXX_AF_TL1X_GREEN_BYTES(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_GREEN_BYTES(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_GREEN_BYTES(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_green_packets
 *
 * NIX AF Transmit Level 1 Green Sent Packets Registers
 */
union bdk_nixx_af_tl1x_green_packets
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_green_packets_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_40_63        : 24;
        uint64_t count                 : 40; /**< [ 39:  0](R/W/H) Count. The running count of packets. Note that this count wraps. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 40; /**< [ 39:  0](R/W/H) Count. The running count of packets. Note that this count wraps. */
        uint64_t reserved_40_63        : 24;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_green_packets_s cn; */
};
typedef union bdk_nixx_af_tl1x_green_packets bdk_nixx_af_tl1x_green_packets_t;

static inline uint64_t BDK_NIXX_AF_TL1X_GREEN_PACKETS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_GREEN_PACKETS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000d80ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_GREEN_PACKETS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_GREEN_PACKETS(a,b) bdk_nixx_af_tl1x_green_packets_t
#define bustype_BDK_NIXX_AF_TL1X_GREEN_PACKETS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_GREEN_PACKETS(a,b) "NIXX_AF_TL1X_GREEN_PACKETS"
#define device_bar_BDK_NIXX_AF_TL1X_GREEN_PACKETS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_GREEN_PACKETS(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_GREEN_PACKETS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_md_debug0
 *
 * NIX AF Transmit Level 1 Meta Descriptor Debug 0 Registers
 * NIX_AF_TL1()_MD_DEBUG0, NIX_AF_TL1()_MD_DEBUG1, NIX_AF_TL1()_MD_DEBUG2 and
 * NIX_AF_TL1()_MD_DEBUG3 provide access to the TLn queue meta descriptor. A TLn
 * queue can hold up to two packet meta descriptors (PMD) and one flush meta
 * descriptor (FMD).
 */
union bdk_nixx_af_tl1x_md_debug0
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_md_debug0_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t pmd_count             : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor count, used to show the packet meta descriptor sequence. Which PMD
                                                                 is next to read and or write. */
        uint64_t reserved_62           : 1;
        uint64_t child                 : 10; /**< [ 61: 52](R/W/H) Child index, highest priority child. When [C_CON] of this result is set,
                                                                 indicating that this result is
                                                                 connected in a flow that extends through the child result, this is the index of that child
                                                                 result. */
        uint64_t reserved_50_51        : 2;
        uint64_t p_con                 : 1;  /**< [ 49: 49](R/W/H) Parent connected flag. This pick has more picks in front of it. */
        uint64_t c_con                 : 1;  /**< [ 48: 48](R/W/H) Child connected flag. This pick has more picks behind it. */
        uint64_t drain                 : 1;  /**< [ 47: 47](R/W/H) DRAIN command indicator. */
        uint64_t drain_pri             : 1;  /**< [ 46: 46](R/W/H) DRAIN priority indicator. */
        uint64_t reserved_34_45        : 12;
        uint64_t pmd1_vld              : 1;  /**< [ 33: 33](R/W/H) Packet meta-descriptor 1 valid. */
        uint64_t pmd0_vld              : 1;  /**< [ 32: 32](R/W/H) Packet meta-descriptor 0 valid. */
        uint64_t pmd1_length           : 16; /**< [ 31: 16](R/W/H) Packet meta-descriptor 1 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd0_length           : 16; /**< [ 15:  0](R/W/H) Packet meta-descriptor 0 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
#else /* Word 0 - Little Endian */
        uint64_t pmd0_length           : 16; /**< [ 15:  0](R/W/H) Packet meta-descriptor 0 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd1_length           : 16; /**< [ 31: 16](R/W/H) Packet meta-descriptor 1 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd0_vld              : 1;  /**< [ 32: 32](R/W/H) Packet meta-descriptor 0 valid. */
        uint64_t pmd1_vld              : 1;  /**< [ 33: 33](R/W/H) Packet meta-descriptor 1 valid. */
        uint64_t reserved_34_45        : 12;
        uint64_t drain_pri             : 1;  /**< [ 46: 46](R/W/H) DRAIN priority indicator. */
        uint64_t drain                 : 1;  /**< [ 47: 47](R/W/H) DRAIN command indicator. */
        uint64_t c_con                 : 1;  /**< [ 48: 48](R/W/H) Child connected flag. This pick has more picks behind it. */
        uint64_t p_con                 : 1;  /**< [ 49: 49](R/W/H) Parent connected flag. This pick has more picks in front of it. */
        uint64_t reserved_50_51        : 2;
        uint64_t child                 : 10; /**< [ 61: 52](R/W/H) Child index, highest priority child. When [C_CON] of this result is set,
                                                                 indicating that this result is
                                                                 connected in a flow that extends through the child result, this is the index of that child
                                                                 result. */
        uint64_t reserved_62           : 1;
        uint64_t pmd_count             : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor count, used to show the packet meta descriptor sequence. Which PMD
                                                                 is next to read and or write. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_md_debug0_s cn; */
};
typedef union bdk_nixx_af_tl1x_md_debug0 bdk_nixx_af_tl1x_md_debug0_t;

static inline uint64_t BDK_NIXX_AF_TL1X_MD_DEBUG0(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_MD_DEBUG0(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000cc0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_MD_DEBUG0", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_MD_DEBUG0(a,b) bdk_nixx_af_tl1x_md_debug0_t
#define bustype_BDK_NIXX_AF_TL1X_MD_DEBUG0(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_MD_DEBUG0(a,b) "NIXX_AF_TL1X_MD_DEBUG0"
#define device_bar_BDK_NIXX_AF_TL1X_MD_DEBUG0(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_MD_DEBUG0(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_MD_DEBUG0(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_md_debug1
 *
 * NIX AF Transmit Level 1 Meta Descriptor Debug 1 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl1x_md_debug1
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_md_debug1_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t reserved_36           : 1;
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t reserved_0_5          : 6;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_5          : 6;
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t reserved_36           : 1;
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_md_debug1_s cn; */
};
typedef union bdk_nixx_af_tl1x_md_debug1 bdk_nixx_af_tl1x_md_debug1_t;

static inline uint64_t BDK_NIXX_AF_TL1X_MD_DEBUG1(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_MD_DEBUG1(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000cc8ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_MD_DEBUG1", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_MD_DEBUG1(a,b) bdk_nixx_af_tl1x_md_debug1_t
#define bustype_BDK_NIXX_AF_TL1X_MD_DEBUG1(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_MD_DEBUG1(a,b) "NIXX_AF_TL1X_MD_DEBUG1"
#define device_bar_BDK_NIXX_AF_TL1X_MD_DEBUG1(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_MD_DEBUG1(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_MD_DEBUG1(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_md_debug2
 *
 * NIX AF Transmit Level 1 Meta Descriptor Debug 2 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl1x_md_debug2
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_md_debug2_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t reserved_36           : 1;
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE Packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t reserved_0_5          : 6;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_5          : 6;
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE Packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t reserved_36           : 1;
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_md_debug2_s cn; */
};
typedef union bdk_nixx_af_tl1x_md_debug2 bdk_nixx_af_tl1x_md_debug2_t;

static inline uint64_t BDK_NIXX_AF_TL1X_MD_DEBUG2(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_MD_DEBUG2(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000cd0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_MD_DEBUG2", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_MD_DEBUG2(a,b) bdk_nixx_af_tl1x_md_debug2_t
#define bustype_BDK_NIXX_AF_TL1X_MD_DEBUG2(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_MD_DEBUG2(a,b) "NIXX_AF_TL1X_MD_DEBUG2"
#define device_bar_BDK_NIXX_AF_TL1X_MD_DEBUG2(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_MD_DEBUG2(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_MD_DEBUG2(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_md_debug3
 *
 * NIX AF Transmit Level 1 Meta Descriptor Debug 3 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl1x_md_debug3
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_md_debug3_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t reserved_0_38         : 39;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_38         : 39;
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_md_debug3_s cn; */
};
typedef union bdk_nixx_af_tl1x_md_debug3 bdk_nixx_af_tl1x_md_debug3_t;

static inline uint64_t BDK_NIXX_AF_TL1X_MD_DEBUG3(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_MD_DEBUG3(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000cd8ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_MD_DEBUG3", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_MD_DEBUG3(a,b) bdk_nixx_af_tl1x_md_debug3_t
#define bustype_BDK_NIXX_AF_TL1X_MD_DEBUG3(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_MD_DEBUG3(a,b) "NIXX_AF_TL1X_MD_DEBUG3"
#define device_bar_BDK_NIXX_AF_TL1X_MD_DEBUG3(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_MD_DEBUG3(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_MD_DEBUG3(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_red
 *
 * INTERNAL: NIX Transmit Level 1 Red State Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1()_YELLOW.
 */
union bdk_nixx_af_tl1x_red
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_red_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_18_63        : 46;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_18_63        : 46;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_red_s cn; */
};
typedef union bdk_nixx_af_tl1x_red bdk_nixx_af_tl1x_red_t;

static inline uint64_t BDK_NIXX_AF_TL1X_RED(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_RED(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000cb0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_RED", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_RED(a,b) bdk_nixx_af_tl1x_red_t
#define bustype_BDK_NIXX_AF_TL1X_RED(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_RED(a,b) "NIXX_AF_TL1X_RED"
#define device_bar_BDK_NIXX_AF_TL1X_RED(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_RED(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_RED(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_red_bytes
 *
 * NIX AF Transmit Level 1 Red Sent Bytes Registers
 * This register has the same bit fields as NIX_AF_TL1()_GREEN_BYTES.
 */
union bdk_nixx_af_tl1x_red_bytes
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_red_bytes_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t count                 : 48; /**< [ 47:  0](R/W/H) Count. The running count of bytes. Note that this count wraps. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 48; /**< [ 47:  0](R/W/H) Count. The running count of bytes. Note that this count wraps. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_red_bytes_s cn; */
};
typedef union bdk_nixx_af_tl1x_red_bytes bdk_nixx_af_tl1x_red_bytes_t;

static inline uint64_t BDK_NIXX_AF_TL1X_RED_BYTES(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_RED_BYTES(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000d50ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_RED_BYTES", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_RED_BYTES(a,b) bdk_nixx_af_tl1x_red_bytes_t
#define bustype_BDK_NIXX_AF_TL1X_RED_BYTES(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_RED_BYTES(a,b) "NIXX_AF_TL1X_RED_BYTES"
#define device_bar_BDK_NIXX_AF_TL1X_RED_BYTES(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_RED_BYTES(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_RED_BYTES(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_red_packets
 *
 * NIX AF Transmit Level 1 Red Sent Packets Registers
 * This register has the same bit fields as NIX_AF_TL1()_GREEN_PACKETS.
 */
union bdk_nixx_af_tl1x_red_packets
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_red_packets_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_40_63        : 24;
        uint64_t count                 : 40; /**< [ 39:  0](R/W/H) Count. The running count of packets. Note that this count wraps. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 40; /**< [ 39:  0](R/W/H) Count. The running count of packets. Note that this count wraps. */
        uint64_t reserved_40_63        : 24;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_red_packets_s cn; */
};
typedef union bdk_nixx_af_tl1x_red_packets bdk_nixx_af_tl1x_red_packets_t;

static inline uint64_t BDK_NIXX_AF_TL1X_RED_PACKETS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_RED_PACKETS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000d40ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_RED_PACKETS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_RED_PACKETS(a,b) bdk_nixx_af_tl1x_red_packets_t
#define bustype_BDK_NIXX_AF_TL1X_RED_PACKETS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_RED_PACKETS(a,b) "NIXX_AF_TL1X_RED_PACKETS"
#define device_bar_BDK_NIXX_AF_TL1X_RED_PACKETS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_RED_PACKETS(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_RED_PACKETS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_schedule
 *
 * NIX AF Transmit Level 1 Scheduling Control Register
 */
union bdk_nixx_af_tl1x_schedule
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_schedule_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_24_63        : 40;
        uint64_t rr_quantum            : 24; /**< [ 23:  0](R/W) Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                                 integer).

                                                                 Typically [RR_QUANTUM] should be at or near the MTU or more (to limit or prevent
                                                                 negative accumulations of the deficit count).

                                                                 Transmit limiter 1 packet meta descriptor are active in the scheduler when the rate limiter
                                                                 has not been exceeded. The position of the child in the TL1 array is always the position in
                                                                 the circle (i.e. no linked list, used by packet queue arbiter). PSE moves the current head
                                                                 of the circle on quantum expiration or when the head cannot follow with an active packet.

                                                                 Packet queue arbiter takes a snap shot of TL1 active packet meta descriptor and performs
                                                                 round robin arbitration. */
#else /* Word 0 - Little Endian */
        uint64_t rr_quantum            : 24; /**< [ 23:  0](R/W) Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                                 integer).

                                                                 Typically [RR_QUANTUM] should be at or near the MTU or more (to limit or prevent
                                                                 negative accumulations of the deficit count).

                                                                 Transmit limiter 1 packet meta descriptor are active in the scheduler when the rate limiter
                                                                 has not been exceeded. The position of the child in the TL1 array is always the position in
                                                                 the circle (i.e. no linked list, used by packet queue arbiter). PSE moves the current head
                                                                 of the circle on quantum expiration or when the head cannot follow with an active packet.

                                                                 Packet queue arbiter takes a snap shot of TL1 active packet meta descriptor and performs
                                                                 round robin arbitration. */
        uint64_t reserved_24_63        : 40;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_schedule_s cn; */
};
typedef union bdk_nixx_af_tl1x_schedule bdk_nixx_af_tl1x_schedule_t;

static inline uint64_t BDK_NIXX_AF_TL1X_SCHEDULE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_SCHEDULE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000c00ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_SCHEDULE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_SCHEDULE(a,b) bdk_nixx_af_tl1x_schedule_t
#define bustype_BDK_NIXX_AF_TL1X_SCHEDULE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_SCHEDULE(a,b) "NIXX_AF_TL1X_SCHEDULE"
#define device_bar_BDK_NIXX_AF_TL1X_SCHEDULE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_SCHEDULE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_SCHEDULE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_shape
 *
 * NIX AF Transmit Level 1 Shaping Control Register
 */
union bdk_nixx_af_tl1x_shape
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_shape_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_25_63        : 39;
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t reserved_9_23         : 15;
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) Shaping and scheduling calculation adjustment. This nine-bit signed value
                                                                 allows -255 .. 255 bytes to be added to the packet length for rate
                                                                 limiting and scheduling calculations. [ADJUST] value 0x100 should
                                                                 not be used. */
#else /* Word 0 - Little Endian */
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) Shaping and scheduling calculation adjustment. This nine-bit signed value
                                                                 allows -255 .. 255 bytes to be added to the packet length for rate
                                                                 limiting and scheduling calculations. [ADJUST] value 0x100 should
                                                                 not be used. */
        uint64_t reserved_9_23         : 15;
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t reserved_25_63        : 39;
#endif /* Word 0 - End */
    } s;
    struct bdk_nixx_af_tl1x_shape_cn
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_25_63        : 39;
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t reserved_18_23        : 6;
        uint64_t reserved_9_17         : 9;
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) Shaping and scheduling calculation adjustment. This nine-bit signed value
                                                                 allows -255 .. 255 bytes to be added to the packet length for rate
                                                                 limiting and scheduling calculations. [ADJUST] value 0x100 should
                                                                 not be used. */
#else /* Word 0 - Little Endian */
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) Shaping and scheduling calculation adjustment. This nine-bit signed value
                                                                 allows -255 .. 255 bytes to be added to the packet length for rate
                                                                 limiting and scheduling calculations. [ADJUST] value 0x100 should
                                                                 not be used. */
        uint64_t reserved_9_17         : 9;
        uint64_t reserved_18_23        : 6;
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t reserved_25_63        : 39;
#endif /* Word 0 - End */
    } cn;
};
typedef union bdk_nixx_af_tl1x_shape bdk_nixx_af_tl1x_shape_t;

static inline uint64_t BDK_NIXX_AF_TL1X_SHAPE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_SHAPE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000c10ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_SHAPE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_SHAPE(a,b) bdk_nixx_af_tl1x_shape_t
#define bustype_BDK_NIXX_AF_TL1X_SHAPE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_SHAPE(a,b) "NIXX_AF_TL1X_SHAPE"
#define device_bar_BDK_NIXX_AF_TL1X_SHAPE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_SHAPE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_SHAPE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_shape_state
 *
 * NIX AF Transmit Level 1 Shape State Register
 * This register must not be written during normal operation.
 */
union bdk_nixx_af_tl1x_shape_state
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_shape_state_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_53_63        : 11;
        uint64_t color                 : 1;  /**< [ 52: 52](R/W/H) Shaper connection status. Debug access to the live shaper state.
                                                                 0 = Connected - shaper is connected.
                                                                 1 = Pruned - shaper is disconnected. */
        uint64_t reserved_26_51        : 26;
        uint64_t cir_accum             : 26; /**< [ 25:  0](R/W/H) Committed information rate accumulator. Debug access to the live CIR accumulator. */
#else /* Word 0 - Little Endian */
        uint64_t cir_accum             : 26; /**< [ 25:  0](R/W/H) Committed information rate accumulator. Debug access to the live CIR accumulator. */
        uint64_t reserved_26_51        : 26;
        uint64_t color                 : 1;  /**< [ 52: 52](R/W/H) Shaper connection status. Debug access to the live shaper state.
                                                                 0 = Connected - shaper is connected.
                                                                 1 = Pruned - shaper is disconnected. */
        uint64_t reserved_53_63        : 11;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_shape_state_s cn; */
};
typedef union bdk_nixx_af_tl1x_shape_state bdk_nixx_af_tl1x_shape_state_t;

static inline uint64_t BDK_NIXX_AF_TL1X_SHAPE_STATE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_SHAPE_STATE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000c50ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_SHAPE_STATE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_SHAPE_STATE(a,b) bdk_nixx_af_tl1x_shape_state_t
#define bustype_BDK_NIXX_AF_TL1X_SHAPE_STATE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_SHAPE_STATE(a,b) "NIXX_AF_TL1X_SHAPE_STATE"
#define device_bar_BDK_NIXX_AF_TL1X_SHAPE_STATE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_SHAPE_STATE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_SHAPE_STATE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_sw_xoff
 *
 * NIX AF Transmit Level 1 Software Controlled XOFF Registers
 */
union bdk_nixx_af_tl1x_sw_xoff
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_sw_xoff_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_4_63         : 60;
        uint64_t drain_irq             : 1;  /**< [  3:  3](WO) Drain IRQ. Enables setting of NIX_AF_GEN_INT[TL1_DRAIN] when the drain
                                                                 operation has completed.
                                                                 [DRAIN_IRQ] should be set whenever [DRAIN] is, and must not be set when [DRAIN] isn't
                                                                 set. [DRAIN_IRQ] has no effect unless [DRAIN] and [XOFF] are also set. */
        uint64_t reserved_2            : 1;
        uint64_t drain                 : 1;  /**< [  1:  1](WO) Drain. This control activates a drain path through the PSE that starts at this queue
                                                                 and ends at the TL1 level. The drain path is prioritized over other paths through PSE
                                                                 and can be used in combination with [DRAIN_IRQ]. [DRAIN] need never be set for the
                                                                 TL1 level, but is useful at all other levels, including the TL4 and MDQ levels.
                                                                 NIX_AF_GEN_INT[TL1_DRAIN] should be clear prior to initiating a [DRAIN]=1 write to
                                                                 this CSR.

                                                                 After [DRAIN] is set for a shaping queue, it should not be set again, for
                                                                 this or any other shaping queue, until NIX_AF_GEN_INT[TL1_DRAIN] is set.

                                                                 [DRAIN] must not be set for any shaping queue when an SMQ FLUSH command is
                                                                 active (any NIX_AF_SMQ()_CFG[FLUSH] is set).

                                                                 DRAIN has no effect unless XOFF is also set. Only one DRAIN command is
                                                                 allowed to be active at a time. */
        uint64_t xoff                  : 1;  /**< [  0:  0](R/W) XOFF. Stops meta flow out of the TL1 shaping queue. When [XOFF] is set, the
                                                                 corresponding NIX_AF_TL*()_MD_DEBUG* (i.e. the meta descriptor) in the TL1
                                                                 shaping queue cannot be transferred to the next level. */
#else /* Word 0 - Little Endian */
        uint64_t xoff                  : 1;  /**< [  0:  0](R/W) XOFF. Stops meta flow out of the TL1 shaping queue. When [XOFF] is set, the
                                                                 corresponding NIX_AF_TL*()_MD_DEBUG* (i.e. the meta descriptor) in the TL1
                                                                 shaping queue cannot be transferred to the next level. */
        uint64_t drain                 : 1;  /**< [  1:  1](WO) Drain. This control activates a drain path through the PSE that starts at this queue
                                                                 and ends at the TL1 level. The drain path is prioritized over other paths through PSE
                                                                 and can be used in combination with [DRAIN_IRQ]. [DRAIN] need never be set for the
                                                                 TL1 level, but is useful at all other levels, including the TL4 and MDQ levels.
                                                                 NIX_AF_GEN_INT[TL1_DRAIN] should be clear prior to initiating a [DRAIN]=1 write to
                                                                 this CSR.

                                                                 After [DRAIN] is set for a shaping queue, it should not be set again, for
                                                                 this or any other shaping queue, until NIX_AF_GEN_INT[TL1_DRAIN] is set.

                                                                 [DRAIN] must not be set for any shaping queue when an SMQ FLUSH command is
                                                                 active (any NIX_AF_SMQ()_CFG[FLUSH] is set).

                                                                 DRAIN has no effect unless XOFF is also set. Only one DRAIN command is
                                                                 allowed to be active at a time. */
        uint64_t reserved_2            : 1;
        uint64_t drain_irq             : 1;  /**< [  3:  3](WO) Drain IRQ. Enables setting of NIX_AF_GEN_INT[TL1_DRAIN] when the drain
                                                                 operation has completed.
                                                                 [DRAIN_IRQ] should be set whenever [DRAIN] is, and must not be set when [DRAIN] isn't
                                                                 set. [DRAIN_IRQ] has no effect unless [DRAIN] and [XOFF] are also set. */
        uint64_t reserved_4_63         : 60;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_sw_xoff_s cn; */
};
typedef union bdk_nixx_af_tl1x_sw_xoff bdk_nixx_af_tl1x_sw_xoff_t;

static inline uint64_t BDK_NIXX_AF_TL1X_SW_XOFF(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_SW_XOFF(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000c70ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_SW_XOFF", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_SW_XOFF(a,b) bdk_nixx_af_tl1x_sw_xoff_t
#define bustype_BDK_NIXX_AF_TL1X_SW_XOFF(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_SW_XOFF(a,b) "NIXX_AF_TL1X_SW_XOFF"
#define device_bar_BDK_NIXX_AF_TL1X_SW_XOFF(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_SW_XOFF(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_SW_XOFF(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_topology
 *
 * NIX AF Transmit Level 1 Topology Registers
 */
union bdk_nixx_af_tl1x_topology
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_topology_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_40_63        : 24;
        uint64_t prio_anchor           : 8;  /**< [ 39: 32](R/W) Priority anchor. The base index positioning the static priority child
                                                                 queues of this shaper. A higher-level queue is a child queue of this shaper
                                                                 when its NIX_AF_TL*()_PARENT/NIX_AF_MDQ()_PARENT selects this shaper, and
                                                                 it further is a static priority child queue when its
                                                                 NIX_AF_TL*()_SCHEDULE[PRIO] does not equal [RR_PRIO]. A static priority
                                                                 child queue with priority PRIO must be located at n=[PRIO_ANCHOR]+PRIO,
                                                                 where PRIO=NIX_AF_TL*(n)_SCHEDULE[PRIO]. There can be at most one static
                                                                 priority child queue at each priority. When there are no static priority
                                                                 child queues attached at any priority, or if this shaper isn't used, the
                                                                 hardware does not use [PRIO_ANCHOR]. In this case, we recommend
                                                                 [PRIO_ANCHOR] be zero. Note that there are 10 available priorities, 0
                                                                 through 9, with priority 0 being the highest and priority 9 being the
                                                                 lowest. */
        uint64_t reserved_5_31         : 27;
        uint64_t rr_prio               : 4;  /**< [  4:  1](R/W) Round-robin priority. The priority assigned to the round-robin scheduler. A
                                                                 higher-level queue is a child queue of this shaper when its
                                                                 NIX_AF_TL*()_PARENT/NIX_AF_MDQ()_PARENT
                                                                 selects this shaper, and it further is a round robin child queue when its
                                                                 NIX_AF_TL*()_SCHEDULE[PRIO] equals [RR_PRIO]. All round-robin queues
                                                                 attached to this shaper must have the same priority. But the number of
                                                                 round-robin child queues attached (at this priority) is limited only by the
                                                                 number of higher-level queues. When this shaper is not used, we recommend
                                                                 [RR_PRIO] be zero.

                                                                 When a shaper is used, [RR_PRIO] should be 0xF when there are no priorities
                                                                 with more than one child queue (i.e. when there are no round-robin child
                                                                 queues), and should otherwise be a legal priority (values 0-9). */
        uint64_t reserved_0            : 1;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0            : 1;
        uint64_t rr_prio               : 4;  /**< [  4:  1](R/W) Round-robin priority. The priority assigned to the round-robin scheduler. A
                                                                 higher-level queue is a child queue of this shaper when its
                                                                 NIX_AF_TL*()_PARENT/NIX_AF_MDQ()_PARENT
                                                                 selects this shaper, and it further is a round robin child queue when its
                                                                 NIX_AF_TL*()_SCHEDULE[PRIO] equals [RR_PRIO]. All round-robin queues
                                                                 attached to this shaper must have the same priority. But the number of
                                                                 round-robin child queues attached (at this priority) is limited only by the
                                                                 number of higher-level queues. When this shaper is not used, we recommend
                                                                 [RR_PRIO] be zero.

                                                                 When a shaper is used, [RR_PRIO] should be 0xF when there are no priorities
                                                                 with more than one child queue (i.e. when there are no round-robin child
                                                                 queues), and should otherwise be a legal priority (values 0-9). */
        uint64_t reserved_5_31         : 27;
        uint64_t prio_anchor           : 8;  /**< [ 39: 32](R/W) Priority anchor. The base index positioning the static priority child
                                                                 queues of this shaper. A higher-level queue is a child queue of this shaper
                                                                 when its NIX_AF_TL*()_PARENT/NIX_AF_MDQ()_PARENT selects this shaper, and
                                                                 it further is a static priority child queue when its
                                                                 NIX_AF_TL*()_SCHEDULE[PRIO] does not equal [RR_PRIO]. A static priority
                                                                 child queue with priority PRIO must be located at n=[PRIO_ANCHOR]+PRIO,
                                                                 where PRIO=NIX_AF_TL*(n)_SCHEDULE[PRIO]. There can be at most one static
                                                                 priority child queue at each priority. When there are no static priority
                                                                 child queues attached at any priority, or if this shaper isn't used, the
                                                                 hardware does not use [PRIO_ANCHOR]. In this case, we recommend
                                                                 [PRIO_ANCHOR] be zero. Note that there are 10 available priorities, 0
                                                                 through 9, with priority 0 being the highest and priority 9 being the
                                                                 lowest. */
        uint64_t reserved_40_63        : 24;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_topology_s cn; */
};
typedef union bdk_nixx_af_tl1x_topology bdk_nixx_af_tl1x_topology_t;

static inline uint64_t BDK_NIXX_AF_TL1X_TOPOLOGY(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_TOPOLOGY(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000c80ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_TOPOLOGY", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_TOPOLOGY(a,b) bdk_nixx_af_tl1x_topology_t
#define bustype_BDK_NIXX_AF_TL1X_TOPOLOGY(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_TOPOLOGY(a,b) "NIXX_AF_TL1X_TOPOLOGY"
#define device_bar_BDK_NIXX_AF_TL1X_TOPOLOGY(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_TOPOLOGY(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_TOPOLOGY(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_yellow
 *
 * INTERNAL: NIX Transmit Level 1 Yellow State Debug Register
 */
union bdk_nixx_af_tl1x_yellow
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_yellow_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_18_63        : 46;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_18_63        : 46;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_yellow_s cn; */
};
typedef union bdk_nixx_af_tl1x_yellow bdk_nixx_af_tl1x_yellow_t;

static inline uint64_t BDK_NIXX_AF_TL1X_YELLOW(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_YELLOW(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000ca0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_YELLOW", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_YELLOW(a,b) bdk_nixx_af_tl1x_yellow_t
#define bustype_BDK_NIXX_AF_TL1X_YELLOW(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_YELLOW(a,b) "NIXX_AF_TL1X_YELLOW"
#define device_bar_BDK_NIXX_AF_TL1X_YELLOW(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_YELLOW(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_YELLOW(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_yellow_bytes
 *
 * NIX AF Transmit Level 1 Yellow Sent Bytes Registers
 * This register has the same bit fields as NIX_AF_TL1()_GREEN_BYTES.
 */
union bdk_nixx_af_tl1x_yellow_bytes
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_yellow_bytes_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t count                 : 48; /**< [ 47:  0](R/W/H) Count. The running count of bytes. Note that this count wraps. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 48; /**< [ 47:  0](R/W/H) Count. The running count of bytes. Note that this count wraps. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_yellow_bytes_s cn; */
};
typedef union bdk_nixx_af_tl1x_yellow_bytes bdk_nixx_af_tl1x_yellow_bytes_t;

static inline uint64_t BDK_NIXX_AF_TL1X_YELLOW_BYTES(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_YELLOW_BYTES(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000d70ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_YELLOW_BYTES", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_YELLOW_BYTES(a,b) bdk_nixx_af_tl1x_yellow_bytes_t
#define bustype_BDK_NIXX_AF_TL1X_YELLOW_BYTES(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_YELLOW_BYTES(a,b) "NIXX_AF_TL1X_YELLOW_BYTES"
#define device_bar_BDK_NIXX_AF_TL1X_YELLOW_BYTES(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_YELLOW_BYTES(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_YELLOW_BYTES(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1#_yellow_packets
 *
 * NIX AF Transmit Level 1 Yellow Sent Packets Registers
 * This register has the same bit fields as NIX_AF_TL1()_GREEN_PACKETS.
 */
union bdk_nixx_af_tl1x_yellow_packets
{
    uint64_t u;
    struct bdk_nixx_af_tl1x_yellow_packets_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_40_63        : 24;
        uint64_t count                 : 40; /**< [ 39:  0](R/W/H) Count. The running count of packets. Note that this count wraps. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 40; /**< [ 39:  0](R/W/H) Count. The running count of packets. Note that this count wraps. */
        uint64_t reserved_40_63        : 24;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1x_yellow_packets_s cn; */
};
typedef union bdk_nixx_af_tl1x_yellow_packets bdk_nixx_af_tl1x_yellow_packets_t;

static inline uint64_t BDK_NIXX_AF_TL1X_YELLOW_PACKETS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1X_YELLOW_PACKETS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=27)))
        return 0x850040000d60ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1f);
    __bdk_csr_fatal("NIXX_AF_TL1X_YELLOW_PACKETS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1X_YELLOW_PACKETS(a,b) bdk_nixx_af_tl1x_yellow_packets_t
#define bustype_BDK_NIXX_AF_TL1X_YELLOW_PACKETS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1X_YELLOW_PACKETS(a,b) "NIXX_AF_TL1X_YELLOW_PACKETS"
#define device_bar_BDK_NIXX_AF_TL1X_YELLOW_PACKETS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1X_YELLOW_PACKETS(a,b) (a)
#define arguments_BDK_NIXX_AF_TL1X_YELLOW_PACKETS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1_const
 *
 * NIX AF Transmit Level 1 Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_tl1_const
{
    uint64_t u;
    struct bdk_nixx_af_tl1_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_16_63        : 48;
        uint64_t count                 : 16; /**< [ 15:  0](RO) Number of transmit level 1 shaping queues.
                                                                 Internal:
                                                                 FIXME -- Change to 0x28 for t98b. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 16; /**< [ 15:  0](RO) Number of transmit level 1 shaping queues.
                                                                 Internal:
                                                                 FIXME -- Change to 0x28 for t98b. */
        uint64_t reserved_16_63        : 48;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1_const_s cn; */
};
typedef union bdk_nixx_af_tl1_const bdk_nixx_af_tl1_const_t;

static inline uint64_t BDK_NIXX_AF_TL1_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000070ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL1_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1_CONST(a) bdk_nixx_af_tl1_const_t
#define bustype_BDK_NIXX_AF_TL1_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1_CONST(a) "NIXX_AF_TL1_CONST"
#define device_bar_BDK_NIXX_AF_TL1_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1_CONST(a) (a)
#define arguments_BDK_NIXX_AF_TL1_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1_debug_green
 *
 * INTERNAL: NIX Transmit Level 1 Green Vector Debug Register
 */
union bdk_nixx_af_tl1_debug_green
{
    uint64_t u;
    struct bdk_nixx_af_tl1_debug_green_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t green_valid           : 32; /**< [ 63: 32](RO/H) Green valid vector */
        uint64_t reserved_0_31         : 32;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_31         : 32;
        uint64_t green_valid           : 32; /**< [ 63: 32](RO/H) Green valid vector */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1_debug_green_s cn; */
};
typedef union bdk_nixx_af_tl1_debug_green bdk_nixx_af_tl1_debug_green_t;

static inline uint64_t BDK_NIXX_AF_TL1_DEBUG_GREEN(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1_DEBUG_GREEN(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000d00ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL1_DEBUG_GREEN", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1_DEBUG_GREEN(a) bdk_nixx_af_tl1_debug_green_t
#define bustype_BDK_NIXX_AF_TL1_DEBUG_GREEN(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1_DEBUG_GREEN(a) "NIXX_AF_TL1_DEBUG_GREEN"
#define device_bar_BDK_NIXX_AF_TL1_DEBUG_GREEN(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1_DEBUG_GREEN(a) (a)
#define arguments_BDK_NIXX_AF_TL1_DEBUG_GREEN(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1_debug_node
 *
 * INTERNAL: NIX Transmit Level 1 Yellow Vector Debug Register
 */
union bdk_nixx_af_tl1_debug_node
{
    uint64_t u;
    struct bdk_nixx_af_tl1_debug_node_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_28_63        : 36;
        uint64_t node_valid            : 28; /**< [ 27:  0](RO/H) node valid vector. */
#else /* Word 0 - Little Endian */
        uint64_t node_valid            : 28; /**< [ 27:  0](RO/H) node valid vector. */
        uint64_t reserved_28_63        : 36;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1_debug_node_s cn; */
};
typedef union bdk_nixx_af_tl1_debug_node bdk_nixx_af_tl1_debug_node_t;

static inline uint64_t BDK_NIXX_AF_TL1_DEBUG_NODE(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1_DEBUG_NODE(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000d10ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL1_DEBUG_NODE", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1_DEBUG_NODE(a) bdk_nixx_af_tl1_debug_node_t
#define bustype_BDK_NIXX_AF_TL1_DEBUG_NODE(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1_DEBUG_NODE(a) "NIXX_AF_TL1_DEBUG_NODE"
#define device_bar_BDK_NIXX_AF_TL1_DEBUG_NODE(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1_DEBUG_NODE(a) (a)
#define arguments_BDK_NIXX_AF_TL1_DEBUG_NODE(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1a_debug
 *
 * INTERNAL: NIX TL1A Internal Debug Register
 */
union bdk_nixx_af_tl1a_debug
{
    uint64_t u;
    struct bdk_nixx_af_tl1a_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#else /* Word 0 - Little Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1a_debug_s cn; */
};
typedef union bdk_nixx_af_tl1a_debug bdk_nixx_af_tl1a_debug_t;

static inline uint64_t BDK_NIXX_AF_TL1A_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1A_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000ce0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL1A_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1A_DEBUG(a) bdk_nixx_af_tl1a_debug_t
#define bustype_BDK_NIXX_AF_TL1A_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1A_DEBUG(a) "NIXX_AF_TL1A_DEBUG"
#define device_bar_BDK_NIXX_AF_TL1A_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1A_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_TL1A_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl1b_debug
 *
 * INTERNAL: NIX TL1B Internal Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1A_DEBUG.
 */
union bdk_nixx_af_tl1b_debug
{
    uint64_t u;
    struct bdk_nixx_af_tl1b_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#else /* Word 0 - Little Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl1b_debug_s cn; */
};
typedef union bdk_nixx_af_tl1b_debug bdk_nixx_af_tl1b_debug_t;

static inline uint64_t BDK_NIXX_AF_TL1B_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL1B_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000cf0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL1B_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL1B_DEBUG(a) bdk_nixx_af_tl1b_debug_t
#define bustype_BDK_NIXX_AF_TL1B_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL1B_DEBUG(a) "NIXX_AF_TL1B_DEBUG"
#define device_bar_BDK_NIXX_AF_TL1B_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL1B_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_TL1B_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_cir
 *
 * NIX AF Transmit Level 2 Committed Information Rate Registers
 * This register has the same bit fields as NIX_AF_TL1()_CIR.
 */
union bdk_nixx_af_tl2x_cir
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_cir_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_17_28        : 12;
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
#else /* Word 0 - Little Endian */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t reserved_17_28        : 12;
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_cir_s cn; */
};
typedef union bdk_nixx_af_tl2x_cir bdk_nixx_af_tl2x_cir_t;

static inline uint64_t BDK_NIXX_AF_TL2X_CIR(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_CIR(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e20ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_CIR", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_CIR(a,b) bdk_nixx_af_tl2x_cir_t
#define bustype_BDK_NIXX_AF_TL2X_CIR(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_CIR(a,b) "NIXX_AF_TL2X_CIR"
#define device_bar_BDK_NIXX_AF_TL2X_CIR(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_CIR(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_CIR(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_green
 *
 * INTERNAL: NIX Transmit Level 2 Green State Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1()_GREEN.
 */
union bdk_nixx_af_tl2x_green
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_green_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t rr_active             : 1;  /**< [ 40: 40](R/W/H) Round-robin red active. Set when the RED_SEND+RED_DROP DWRR child list is not empty.
                                                                 For internal use only. */
        uint64_t active_vec            : 20; /**< [ 39: 20](R/W/H) Active vector. A 20-bit vector, two bits per each of the 10 supported priorities.
                                                                 For the non-RR_PRIO priorities, the two bits encode whether the child is active
                                                                 GREEN, active YELLOW, active RED_SEND+RED_DROP, or inactive. At RR_PRIO, one
                                                                 bit is set if the GREEN DWRR child list is not empty, and the other is set if the
                                                                 YELLOW DWRR child list is not empty. For internal use only. */
        uint64_t reserved_18_19        : 2;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_18_19        : 2;
        uint64_t active_vec            : 20; /**< [ 39: 20](R/W/H) Active vector. A 20-bit vector, two bits per each of the 10 supported priorities.
                                                                 For the non-RR_PRIO priorities, the two bits encode whether the child is active
                                                                 GREEN, active YELLOW, active RED_SEND+RED_DROP, or inactive. At RR_PRIO, one
                                                                 bit is set if the GREEN DWRR child list is not empty, and the other is set if the
                                                                 YELLOW DWRR child list is not empty. For internal use only. */
        uint64_t rr_active             : 1;  /**< [ 40: 40](R/W/H) Round-robin red active. Set when the RED_SEND+RED_DROP DWRR child list is not empty.
                                                                 For internal use only. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_green_s cn; */
};
typedef union bdk_nixx_af_tl2x_green bdk_nixx_af_tl2x_green_t;

static inline uint64_t BDK_NIXX_AF_TL2X_GREEN(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_GREEN(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e90ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_GREEN", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_GREEN(a,b) bdk_nixx_af_tl2x_green_t
#define bustype_BDK_NIXX_AF_TL2X_GREEN(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_GREEN(a,b) "NIXX_AF_TL2X_GREEN"
#define device_bar_BDK_NIXX_AF_TL2X_GREEN(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_GREEN(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_GREEN(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_md_debug0
 *
 * NIX AF Transmit Level 2 Meta Descriptor Debug 0 Registers
 * See NIX_AF_TL1()_MD_DEBUG0
 */
union bdk_nixx_af_tl2x_md_debug0
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_md_debug0_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t pmd_count             : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor count, used to show the packet meta descriptor sequence. Which PMD
                                                                 is next to read and or write. */
        uint64_t reserved_62           : 1;
        uint64_t child                 : 10; /**< [ 61: 52](R/W/H) Child index, highest priority child. When [C_CON] of this result is set,
                                                                 indicating that this result is
                                                                 connected in a flow that extends through the child result, this is the index of that child
                                                                 result. */
        uint64_t reserved_50_51        : 2;
        uint64_t p_con                 : 1;  /**< [ 49: 49](R/W/H) Parent connected flag. This pick has more picks in front of it. */
        uint64_t c_con                 : 1;  /**< [ 48: 48](R/W/H) Child connected flag. This pick has more picks behind it. */
        uint64_t drain                 : 1;  /**< [ 47: 47](R/W/H) DRAIN command indicator. */
        uint64_t drain_pri             : 1;  /**< [ 46: 46](R/W/H) DRAIN priority indicator. */
        uint64_t reserved_34_45        : 12;
        uint64_t pmd1_vld              : 1;  /**< [ 33: 33](R/W/H) Packet meta-descriptor 1 valid. */
        uint64_t pmd0_vld              : 1;  /**< [ 32: 32](R/W/H) Packet meta-descriptor 0 valid. */
        uint64_t pmd1_length           : 16; /**< [ 31: 16](R/W/H) Packet meta-descriptor 1 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd0_length           : 16; /**< [ 15:  0](R/W/H) Packet meta-descriptor 0 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
#else /* Word 0 - Little Endian */
        uint64_t pmd0_length           : 16; /**< [ 15:  0](R/W/H) Packet meta-descriptor 0 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd1_length           : 16; /**< [ 31: 16](R/W/H) Packet meta-descriptor 1 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd0_vld              : 1;  /**< [ 32: 32](R/W/H) Packet meta-descriptor 0 valid. */
        uint64_t pmd1_vld              : 1;  /**< [ 33: 33](R/W/H) Packet meta-descriptor 1 valid. */
        uint64_t reserved_34_45        : 12;
        uint64_t drain_pri             : 1;  /**< [ 46: 46](R/W/H) DRAIN priority indicator. */
        uint64_t drain                 : 1;  /**< [ 47: 47](R/W/H) DRAIN command indicator. */
        uint64_t c_con                 : 1;  /**< [ 48: 48](R/W/H) Child connected flag. This pick has more picks behind it. */
        uint64_t p_con                 : 1;  /**< [ 49: 49](R/W/H) Parent connected flag. This pick has more picks in front of it. */
        uint64_t reserved_50_51        : 2;
        uint64_t child                 : 10; /**< [ 61: 52](R/W/H) Child index, highest priority child. When [C_CON] of this result is set,
                                                                 indicating that this result is
                                                                 connected in a flow that extends through the child result, this is the index of that child
                                                                 result. */
        uint64_t reserved_62           : 1;
        uint64_t pmd_count             : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor count, used to show the packet meta descriptor sequence. Which PMD
                                                                 is next to read and or write. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_md_debug0_s cn; */
};
typedef union bdk_nixx_af_tl2x_md_debug0 bdk_nixx_af_tl2x_md_debug0_t;

static inline uint64_t BDK_NIXX_AF_TL2X_MD_DEBUG0(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_MD_DEBUG0(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000ec0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_MD_DEBUG0", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_MD_DEBUG0(a,b) bdk_nixx_af_tl2x_md_debug0_t
#define bustype_BDK_NIXX_AF_TL2X_MD_DEBUG0(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_MD_DEBUG0(a,b) "NIXX_AF_TL2X_MD_DEBUG0"
#define device_bar_BDK_NIXX_AF_TL2X_MD_DEBUG0(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_MD_DEBUG0(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_MD_DEBUG0(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_md_debug1
 *
 * NIX AF Transmit Level 2 Meta Descriptor Debug 1 Registers
 * See NIX_AF_TL1()_MD_DEBUG0
 */
union bdk_nixx_af_tl2x_md_debug1
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_md_debug1_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t reserved_36           : 1;
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t reserved_0_5          : 6;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_5          : 6;
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t reserved_36           : 1;
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_md_debug1_s cn; */
};
typedef union bdk_nixx_af_tl2x_md_debug1 bdk_nixx_af_tl2x_md_debug1_t;

static inline uint64_t BDK_NIXX_AF_TL2X_MD_DEBUG1(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_MD_DEBUG1(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000ec8ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_MD_DEBUG1", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_MD_DEBUG1(a,b) bdk_nixx_af_tl2x_md_debug1_t
#define bustype_BDK_NIXX_AF_TL2X_MD_DEBUG1(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_MD_DEBUG1(a,b) "NIXX_AF_TL2X_MD_DEBUG1"
#define device_bar_BDK_NIXX_AF_TL2X_MD_DEBUG1(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_MD_DEBUG1(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_MD_DEBUG1(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_md_debug2
 *
 * NIX AF Transmit Level 2 Meta Descriptor Debug 2 Registers
 * See NIX_AF_TL1()_MD_DEBUG0
 */
union bdk_nixx_af_tl2x_md_debug2
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_md_debug2_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t reserved_36           : 1;
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE Packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t reserved_0_5          : 6;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_5          : 6;
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE Packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t reserved_36           : 1;
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_md_debug2_s cn; */
};
typedef union bdk_nixx_af_tl2x_md_debug2 bdk_nixx_af_tl2x_md_debug2_t;

static inline uint64_t BDK_NIXX_AF_TL2X_MD_DEBUG2(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_MD_DEBUG2(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000ed0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_MD_DEBUG2", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_MD_DEBUG2(a,b) bdk_nixx_af_tl2x_md_debug2_t
#define bustype_BDK_NIXX_AF_TL2X_MD_DEBUG2(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_MD_DEBUG2(a,b) "NIXX_AF_TL2X_MD_DEBUG2"
#define device_bar_BDK_NIXX_AF_TL2X_MD_DEBUG2(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_MD_DEBUG2(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_MD_DEBUG2(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_md_debug3
 *
 * NIX AF Transmit Level 2 Meta Descriptor Debug 3 Registers
 * See NIX_AF_TL1()_MD_DEBUG0
 */
union bdk_nixx_af_tl2x_md_debug3
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_md_debug3_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t reserved_0_38         : 39;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_38         : 39;
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_md_debug3_s cn; */
};
typedef union bdk_nixx_af_tl2x_md_debug3 bdk_nixx_af_tl2x_md_debug3_t;

static inline uint64_t BDK_NIXX_AF_TL2X_MD_DEBUG3(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_MD_DEBUG3(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000ed8ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_MD_DEBUG3", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_MD_DEBUG3(a,b) bdk_nixx_af_tl2x_md_debug3_t
#define bustype_BDK_NIXX_AF_TL2X_MD_DEBUG3(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_MD_DEBUG3(a,b) "NIXX_AF_TL2X_MD_DEBUG3"
#define device_bar_BDK_NIXX_AF_TL2X_MD_DEBUG3(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_MD_DEBUG3(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_MD_DEBUG3(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_parent
 *
 * NIX AF Transmit Level 2 Parent Registers
 */
union bdk_nixx_af_tl2x_parent
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_parent_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_21_63        : 43;
        uint64_t parent                : 5;  /**< [ 20: 16](R/W) Parent queue index. The index of the shaping element at the next lower hierarchical level
                                                                 that accepts this shaping element's outputs. Refer to the NIX_AF_TL*()_TOPOLOGY
                                                                 [PRIO_ANCHOR,RR_PRIO] descriptions for constraints on which child queues can attach to
                                                                 which shapers at the next lower level. When this shaper is unused, we recommend that
                                                                 [PARENT] be zero. */
        uint64_t reserved_0_15         : 16;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_15         : 16;
        uint64_t parent                : 5;  /**< [ 20: 16](R/W) Parent queue index. The index of the shaping element at the next lower hierarchical level
                                                                 that accepts this shaping element's outputs. Refer to the NIX_AF_TL*()_TOPOLOGY
                                                                 [PRIO_ANCHOR,RR_PRIO] descriptions for constraints on which child queues can attach to
                                                                 which shapers at the next lower level. When this shaper is unused, we recommend that
                                                                 [PARENT] be zero. */
        uint64_t reserved_21_63        : 43;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_parent_s cn; */
};
typedef union bdk_nixx_af_tl2x_parent bdk_nixx_af_tl2x_parent_t;

static inline uint64_t BDK_NIXX_AF_TL2X_PARENT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_PARENT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e88ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_PARENT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_PARENT(a,b) bdk_nixx_af_tl2x_parent_t
#define bustype_BDK_NIXX_AF_TL2X_PARENT(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_PARENT(a,b) "NIXX_AF_TL2X_PARENT"
#define device_bar_BDK_NIXX_AF_TL2X_PARENT(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_PARENT(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_PARENT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_pir
 *
 * NIX AF Transmit Level 2 Peak Information Rate Registers
 * This register has the same bit fields as NIX_AF_TL1()_CIR.
 */
union bdk_nixx_af_tl2x_pir
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_pir_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_17_28        : 12;
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
#else /* Word 0 - Little Endian */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t reserved_17_28        : 12;
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_pir_s cn; */
};
typedef union bdk_nixx_af_tl2x_pir bdk_nixx_af_tl2x_pir_t;

static inline uint64_t BDK_NIXX_AF_TL2X_PIR(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_PIR(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e30ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_PIR", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_PIR(a,b) bdk_nixx_af_tl2x_pir_t
#define bustype_BDK_NIXX_AF_TL2X_PIR(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_PIR(a,b) "NIXX_AF_TL2X_PIR"
#define device_bar_BDK_NIXX_AF_TL2X_PIR(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_PIR(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_PIR(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_pointers
 *
 * INTERNAL: NIX Transmit Level 2 Linked List Pointers Debug Register
 */
union bdk_nixx_af_tl2x_pointers
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_pointers_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_24_63        : 40;
        uint64_t prev                  : 8;  /**< [ 23: 16](R/W/H) Previous pointer. The linked-list previous pointer. */
        uint64_t reserved_8_15         : 8;
        uint64_t next                  : 8;  /**< [  7:  0](R/W/H) Next pointer. The linked-list next pointer. */
#else /* Word 0 - Little Endian */
        uint64_t next                  : 8;  /**< [  7:  0](R/W/H) Next pointer. The linked-list next pointer. */
        uint64_t reserved_8_15         : 8;
        uint64_t prev                  : 8;  /**< [ 23: 16](R/W/H) Previous pointer. The linked-list previous pointer. */
        uint64_t reserved_24_63        : 40;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_pointers_s cn; */
};
typedef union bdk_nixx_af_tl2x_pointers bdk_nixx_af_tl2x_pointers_t;

static inline uint64_t BDK_NIXX_AF_TL2X_POINTERS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_POINTERS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e60ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_POINTERS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_POINTERS(a,b) bdk_nixx_af_tl2x_pointers_t
#define bustype_BDK_NIXX_AF_TL2X_POINTERS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_POINTERS(a,b) "NIXX_AF_TL2X_POINTERS"
#define device_bar_BDK_NIXX_AF_TL2X_POINTERS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_POINTERS(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_POINTERS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_red
 *
 * INTERNAL: NIX Transmit Level 2 Red State Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1()_RED.
 */
union bdk_nixx_af_tl2x_red
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_red_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_18_63        : 46;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_18_63        : 46;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_red_s cn; */
};
typedef union bdk_nixx_af_tl2x_red bdk_nixx_af_tl2x_red_t;

static inline uint64_t BDK_NIXX_AF_TL2X_RED(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_RED(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000eb0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_RED", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_RED(a,b) bdk_nixx_af_tl2x_red_t
#define bustype_BDK_NIXX_AF_TL2X_RED(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_RED(a,b) "NIXX_AF_TL2X_RED"
#define device_bar_BDK_NIXX_AF_TL2X_RED(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_RED(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_RED(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_sched_state
 *
 * NIX AF Transmit Level 2 Scheduling Control State Registers
 */
union bdk_nixx_af_tl2x_sched_state
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_sched_state_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_25_63        : 39;
        uint64_t rr_count              : 25; /**< [ 24:  0](R/W/H) Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
#else /* Word 0 - Little Endian */
        uint64_t rr_count              : 25; /**< [ 24:  0](R/W/H) Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
        uint64_t reserved_25_63        : 39;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_sched_state_s cn; */
};
typedef union bdk_nixx_af_tl2x_sched_state bdk_nixx_af_tl2x_sched_state_t;

static inline uint64_t BDK_NIXX_AF_TL2X_SCHED_STATE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_SCHED_STATE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e40ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_SCHED_STATE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_SCHED_STATE(a,b) bdk_nixx_af_tl2x_sched_state_t
#define bustype_BDK_NIXX_AF_TL2X_SCHED_STATE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_SCHED_STATE(a,b) "NIXX_AF_TL2X_SCHED_STATE"
#define device_bar_BDK_NIXX_AF_TL2X_SCHED_STATE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_SCHED_STATE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_SCHED_STATE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_schedule
 *
 * NIX AF Transmit Level 2 Scheduling Control Registers
 */
union bdk_nixx_af_tl2x_schedule
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_schedule_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_28_63        : 36;
        uint64_t prio                  : 4;  /**< [ 27: 24](R/W) Priority. The priority used for this shaping queue in the (lower-level)
                                                                 parent's scheduling algorithm. When this shaping queue is not used, we
                                                                 recommend setting [PRIO] to zero. The legal [PRIO] values are zero to nine
                                                                 when the shaping queue is used. In addition to priority, [PRIO] determines
                                                                 whether the shaping queue is a static queue or not: If [PRIO] equals the
                                                                 parent's NIX_AF_TL*()_TOPOLOGY[RR_PRIO], then this is a round-robin child
                                                                 queue into the shaper at the next level. */
        uint64_t rr_quantum            : 24; /**< [ 23:  0](R/W) Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                                 integer). The packet size used in all DWRR (RR_COUNT) calculations is:

                                                                 _  (NIX_nm_SHAPE[LENGTH_DISABLE] ? 0 : (NIX_nm_MD*[LENGTH] + NIX_nm_MD*[ADJUST]))
                                                                    + NIX_nm_SHAPE[ADJUST]

                                                                 where nm corresponds to this NIX_nm_SCHEDULE CSR.

                                                                 Typically [RR_QUANTUM] should be at or near the MTU or more (to limit or prevent
                                                                 negative accumulations of the deficit count). */
#else /* Word 0 - Little Endian */
        uint64_t rr_quantum            : 24; /**< [ 23:  0](R/W) Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                                 integer). The packet size used in all DWRR (RR_COUNT) calculations is:

                                                                 _  (NIX_nm_SHAPE[LENGTH_DISABLE] ? 0 : (NIX_nm_MD*[LENGTH] + NIX_nm_MD*[ADJUST]))
                                                                    + NIX_nm_SHAPE[ADJUST]

                                                                 where nm corresponds to this NIX_nm_SCHEDULE CSR.

                                                                 Typically [RR_QUANTUM] should be at or near the MTU or more (to limit or prevent
                                                                 negative accumulations of the deficit count). */
        uint64_t prio                  : 4;  /**< [ 27: 24](R/W) Priority. The priority used for this shaping queue in the (lower-level)
                                                                 parent's scheduling algorithm. When this shaping queue is not used, we
                                                                 recommend setting [PRIO] to zero. The legal [PRIO] values are zero to nine
                                                                 when the shaping queue is used. In addition to priority, [PRIO] determines
                                                                 whether the shaping queue is a static queue or not: If [PRIO] equals the
                                                                 parent's NIX_AF_TL*()_TOPOLOGY[RR_PRIO], then this is a round-robin child
                                                                 queue into the shaper at the next level. */
        uint64_t reserved_28_63        : 36;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_schedule_s cn; */
};
typedef union bdk_nixx_af_tl2x_schedule bdk_nixx_af_tl2x_schedule_t;

static inline uint64_t BDK_NIXX_AF_TL2X_SCHEDULE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_SCHEDULE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e00ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_SCHEDULE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_SCHEDULE(a,b) bdk_nixx_af_tl2x_schedule_t
#define bustype_BDK_NIXX_AF_TL2X_SCHEDULE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_SCHEDULE(a,b) "NIXX_AF_TL2X_SCHEDULE"
#define device_bar_BDK_NIXX_AF_TL2X_SCHEDULE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_SCHEDULE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_SCHEDULE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_shape
 *
 * NIX AF Transmit Level 2 Shaping Control Registers
 */
union bdk_nixx_af_tl2x_shape
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_shape_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_27_63        : 37;
        uint64_t schedule_list         : 2;  /**< [ 26: 25](R/W) Shaper scheduling list. Restricts shaper scheduling to specific lists.
                                                                   0x0 = Normal (selected for nearly all scheduling/shaping applications).
                                                                   0x1 = Green-only.
                                                                   0x2 = Yellow-only.
                                                                   0x3 = Red-only. */
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t reserved_13_23        : 11;
        uint64_t yellow_disable        : 1;  /**< [ 12: 12](R/W) Disable yellow transitions. Disables green-to-yellow packet color marking
                                                                 transitions when set. Not used by hardware when corresponding
                                                                 NIX_AF_TL*()_CIR[ENABLE]/NIX_AF_MDQ()_CIR[ENABLE] is clear. */
        uint64_t red_disable           : 1;  /**< [ 11: 11](R/W) Disable red transitions. Disables green-to-red and yellow-to-red packet
                                                                 color marking transitions when set. Not used by hardware when
                                                                 [RED_ALGO]/NIX_SEND_EXT_S[SHP_RA]=0x2/STALLi nor when corresponding
                                                                 NIX_AF_TL*()_PIR[ENABLE]/NIX_AF_MDQ()_PIR[ENABLE] is clear. */
        uint64_t red_algo              : 2;  /**< [ 10:  9](R/W) Shaper red state algorithm when not specified by the NIX SEND. Used by hardware
                                                                 only when the shaper is in RED state. (A shaper is in RED state when
                                                                 NIX_AF_TL*()_SHAPE_STATE[PIR_ACCUM] is negative.) When NIX_SEND_EXT_S[SHP_RA]!=STD (!=0) for a
                                                                 packet, this [RED_ALGO] is not used, and NIX_SEND_EXT_S[SHP_RA] instead defines
                                                                 the shaper red state algorithm used for the packet. The
                                                                 encoding for the [RED_ALGO]/NIX_SEND_EXT_S[SHP_RA] that is used:
                                                                 0x0 = STALL. See 0x2.
                                                                 0x1 = SEND. Send packets while the shaper is in RED state. When the shaper is
                                                                       in RED state, packets that traverse the shaper will be downgraded to
                                                                       NIX_COLORRESULT_E::RED_SEND.  (if not already
                                                                       NIX_COLORRESULT_E::RED_SEND or NIX_COLORRESULT_E::RED_DROP) unless
                                                                       [RED_DISABLE] is set or NIX_SEND_EXT_S[SHP_DIS] for the packet is
                                                                       set. See also NIX_REDALG_E::SEND.
                                                                 0x2 = STALL. Stall packets while the shaper is in RED state until the shaper is
                                                                       YELLOW or GREEN state. Packets that traverse the shaper are never
                                                                       downgraded to the RED state in this mode.
                                                                       See also NIX_REDALG_E::STALL.
                                                                 0x3 = DISCARD. Continually discard packets while the shaper is in RED state.
                                                                       When the shaper is in RED state, all packets that traverse the shaper
                                                                       will be downgraded to NIX_COLORRESULT_E::RED_DROP (if not already
                                                                       NIX_COLORRESULT_E::RED_DROP), unless [RED_DISABLE] is set or
                                                                       NIX_SEND_EXT_S[SHP_DIS] for the packet is set.
                                                                       NIX_COLORRESULT_E::RED_DROP packets traverse all subsequent
                                                                       schedulers/shapers (all the way through L1), but do so as quickly as
                                                                       possible without affecting any RR_COUNT, CIR_ACCUM, or PIR_ACCUM
                                                                       state, and are then discarded by NIX. See also NIX_REDALG_E::DISCARD. */
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) Shaping and scheduling calculation adjustment. This nine-bit signed value allows
                                                                 -255 .. 255 bytes to be added to the packet length for shaping and scheduling
                                                                 calculations. [ADJUST] value 0x100 should not be used. */
#else /* Word 0 - Little Endian */
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) Shaping and scheduling calculation adjustment. This nine-bit signed value allows
                                                                 -255 .. 255 bytes to be added to the packet length for shaping and scheduling
                                                                 calculations. [ADJUST] value 0x100 should not be used. */
        uint64_t red_algo              : 2;  /**< [ 10:  9](R/W) Shaper red state algorithm when not specified by the NIX SEND. Used by hardware
                                                                 only when the shaper is in RED state. (A shaper is in RED state when
                                                                 NIX_AF_TL*()_SHAPE_STATE[PIR_ACCUM] is negative.) When NIX_SEND_EXT_S[SHP_RA]!=STD (!=0) for a
                                                                 packet, this [RED_ALGO] is not used, and NIX_SEND_EXT_S[SHP_RA] instead defines
                                                                 the shaper red state algorithm used for the packet. The
                                                                 encoding for the [RED_ALGO]/NIX_SEND_EXT_S[SHP_RA] that is used:
                                                                 0x0 = STALL. See 0x2.
                                                                 0x1 = SEND. Send packets while the shaper is in RED state. When the shaper is
                                                                       in RED state, packets that traverse the shaper will be downgraded to
                                                                       NIX_COLORRESULT_E::RED_SEND.  (if not already
                                                                       NIX_COLORRESULT_E::RED_SEND or NIX_COLORRESULT_E::RED_DROP) unless
                                                                       [RED_DISABLE] is set or NIX_SEND_EXT_S[SHP_DIS] for the packet is
                                                                       set. See also NIX_REDALG_E::SEND.
                                                                 0x2 = STALL. Stall packets while the shaper is in RED state until the shaper is
                                                                       YELLOW or GREEN state. Packets that traverse the shaper are never
                                                                       downgraded to the RED state in this mode.
                                                                       See also NIX_REDALG_E::STALL.
                                                                 0x3 = DISCARD. Continually discard packets while the shaper is in RED state.
                                                                       When the shaper is in RED state, all packets that traverse the shaper
                                                                       will be downgraded to NIX_COLORRESULT_E::RED_DROP (if not already
                                                                       NIX_COLORRESULT_E::RED_DROP), unless [RED_DISABLE] is set or
                                                                       NIX_SEND_EXT_S[SHP_DIS] for the packet is set.
                                                                       NIX_COLORRESULT_E::RED_DROP packets traverse all subsequent
                                                                       schedulers/shapers (all the way through L1), but do so as quickly as
                                                                       possible without affecting any RR_COUNT, CIR_ACCUM, or PIR_ACCUM
                                                                       state, and are then discarded by NIX. See also NIX_REDALG_E::DISCARD. */
        uint64_t red_disable           : 1;  /**< [ 11: 11](R/W) Disable red transitions. Disables green-to-red and yellow-to-red packet
                                                                 color marking transitions when set. Not used by hardware when
                                                                 [RED_ALGO]/NIX_SEND_EXT_S[SHP_RA]=0x2/STALLi nor when corresponding
                                                                 NIX_AF_TL*()_PIR[ENABLE]/NIX_AF_MDQ()_PIR[ENABLE] is clear. */
        uint64_t yellow_disable        : 1;  /**< [ 12: 12](R/W) Disable yellow transitions. Disables green-to-yellow packet color marking
                                                                 transitions when set. Not used by hardware when corresponding
                                                                 NIX_AF_TL*()_CIR[ENABLE]/NIX_AF_MDQ()_CIR[ENABLE] is clear. */
        uint64_t reserved_13_23        : 11;
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t schedule_list         : 2;  /**< [ 26: 25](R/W) Shaper scheduling list. Restricts shaper scheduling to specific lists.
                                                                   0x0 = Normal (selected for nearly all scheduling/shaping applications).
                                                                   0x1 = Green-only.
                                                                   0x2 = Yellow-only.
                                                                   0x3 = Red-only. */
        uint64_t reserved_27_63        : 37;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_shape_s cn; */
};
typedef union bdk_nixx_af_tl2x_shape bdk_nixx_af_tl2x_shape_t;

static inline uint64_t BDK_NIXX_AF_TL2X_SHAPE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_SHAPE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e10ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_SHAPE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_SHAPE(a,b) bdk_nixx_af_tl2x_shape_t
#define bustype_BDK_NIXX_AF_TL2X_SHAPE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_SHAPE(a,b) "NIXX_AF_TL2X_SHAPE"
#define device_bar_BDK_NIXX_AF_TL2X_SHAPE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_SHAPE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_SHAPE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_shape_state
 *
 * NIX AF Transmit Level 2 Shape State Registers
 * This register must not be written during normal operation.
 */
union bdk_nixx_af_tl2x_shape_state
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_shape_state_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_54_63        : 10;
        uint64_t color                 : 2;  /**< [ 53: 52](R/W/H) Shaper connection status. Debug access to the live shaper state.
                                                                 0x0 = Green - shaper is connected into the green list.
                                                                 0x1 = Yellow - shaper is connected into the yellow list.
                                                                 0x2 = Red - shaper is connected into the red list.
                                                                 0x3 = Pruned - shaper is disconnected. */
        uint64_t pir_accum             : 26; /**< [ 51: 26](R/W/H) Peak information rate accumulator. Debug access to the live PIR accumulator. */
        uint64_t cir_accum             : 26; /**< [ 25:  0](R/W/H) Committed information rate accumulator. Debug access to the live CIR accumulator. */
#else /* Word 0 - Little Endian */
        uint64_t cir_accum             : 26; /**< [ 25:  0](R/W/H) Committed information rate accumulator. Debug access to the live CIR accumulator. */
        uint64_t pir_accum             : 26; /**< [ 51: 26](R/W/H) Peak information rate accumulator. Debug access to the live PIR accumulator. */
        uint64_t color                 : 2;  /**< [ 53: 52](R/W/H) Shaper connection status. Debug access to the live shaper state.
                                                                 0x0 = Green - shaper is connected into the green list.
                                                                 0x1 = Yellow - shaper is connected into the yellow list.
                                                                 0x2 = Red - shaper is connected into the red list.
                                                                 0x3 = Pruned - shaper is disconnected. */
        uint64_t reserved_54_63        : 10;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_shape_state_s cn; */
};
typedef union bdk_nixx_af_tl2x_shape_state bdk_nixx_af_tl2x_shape_state_t;

static inline uint64_t BDK_NIXX_AF_TL2X_SHAPE_STATE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_SHAPE_STATE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e50ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_SHAPE_STATE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_SHAPE_STATE(a,b) bdk_nixx_af_tl2x_shape_state_t
#define bustype_BDK_NIXX_AF_TL2X_SHAPE_STATE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_SHAPE_STATE(a,b) "NIXX_AF_TL2X_SHAPE_STATE"
#define device_bar_BDK_NIXX_AF_TL2X_SHAPE_STATE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_SHAPE_STATE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_SHAPE_STATE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_sw_xoff
 *
 * NIX AF Transmit Level 2 Software Controlled XOFF Registers
 * This register has the same bit fields as NIX_AF_TL1()_SW_XOFF.
 */
union bdk_nixx_af_tl2x_sw_xoff
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_sw_xoff_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_4_63         : 60;
        uint64_t drain_irq             : 1;  /**< [  3:  3](WO) Drain IRQ. Enables setting of NIX_AF_GEN_INT[TL1_DRAIN] when the drain
                                                                 operation has completed.
                                                                 [DRAIN_IRQ] should be set whenever [DRAIN] is, and must not be set when [DRAIN] isn't
                                                                 set. [DRAIN_IRQ] has no effect unless [DRAIN] and [XOFF] are also set. */
        uint64_t reserved_2            : 1;
        uint64_t drain                 : 1;  /**< [  1:  1](WO) Drain. This control activates a drain path through the PSE that starts at this queue
                                                                 and ends at the TL1 level. The drain path is prioritized over other paths through PSE
                                                                 and can be used in combination with [DRAIN_IRQ]. [DRAIN] need never be set for the
                                                                 TL1 level, but is useful at all other levels, including the TL4 and MDQ levels.
                                                                 NIX_AF_GEN_INT[TL1_DRAIN] should be clear prior to initiating a [DRAIN]=1 write to
                                                                 this CSR.

                                                                 After [DRAIN] is set for a shaping queue, it should not be set again, for
                                                                 this or any other shaping queue, until NIX_AF_GEN_INT[TL1_DRAIN] is set.

                                                                 [DRAIN] must not be set for any shaping queue when an SMQ FLUSH command is
                                                                 active (any NIX_AF_SMQ()_CFG[FLUSH] is set).

                                                                 DRAIN has no effect unless XOFF is also set. Only one DRAIN command is
                                                                 allowed to be active at a time. */
        uint64_t xoff                  : 1;  /**< [  0:  0](R/W) XOFF. Stops meta flow out of the TL1 shaping queue. When [XOFF] is set, the
                                                                 corresponding NIX_AF_TL*()_MD_DEBUG* (i.e. the meta descriptor) in the TL1
                                                                 shaping queue cannot be transferred to the next level. */
#else /* Word 0 - Little Endian */
        uint64_t xoff                  : 1;  /**< [  0:  0](R/W) XOFF. Stops meta flow out of the TL1 shaping queue. When [XOFF] is set, the
                                                                 corresponding NIX_AF_TL*()_MD_DEBUG* (i.e. the meta descriptor) in the TL1
                                                                 shaping queue cannot be transferred to the next level. */
        uint64_t drain                 : 1;  /**< [  1:  1](WO) Drain. This control activates a drain path through the PSE that starts at this queue
                                                                 and ends at the TL1 level. The drain path is prioritized over other paths through PSE
                                                                 and can be used in combination with [DRAIN_IRQ]. [DRAIN] need never be set for the
                                                                 TL1 level, but is useful at all other levels, including the TL4 and MDQ levels.
                                                                 NIX_AF_GEN_INT[TL1_DRAIN] should be clear prior to initiating a [DRAIN]=1 write to
                                                                 this CSR.

                                                                 After [DRAIN] is set for a shaping queue, it should not be set again, for
                                                                 this or any other shaping queue, until NIX_AF_GEN_INT[TL1_DRAIN] is set.

                                                                 [DRAIN] must not be set for any shaping queue when an SMQ FLUSH command is
                                                                 active (any NIX_AF_SMQ()_CFG[FLUSH] is set).

                                                                 DRAIN has no effect unless XOFF is also set. Only one DRAIN command is
                                                                 allowed to be active at a time. */
        uint64_t reserved_2            : 1;
        uint64_t drain_irq             : 1;  /**< [  3:  3](WO) Drain IRQ. Enables setting of NIX_AF_GEN_INT[TL1_DRAIN] when the drain
                                                                 operation has completed.
                                                                 [DRAIN_IRQ] should be set whenever [DRAIN] is, and must not be set when [DRAIN] isn't
                                                                 set. [DRAIN_IRQ] has no effect unless [DRAIN] and [XOFF] are also set. */
        uint64_t reserved_4_63         : 60;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_sw_xoff_s cn; */
};
typedef union bdk_nixx_af_tl2x_sw_xoff bdk_nixx_af_tl2x_sw_xoff_t;

static inline uint64_t BDK_NIXX_AF_TL2X_SW_XOFF(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_SW_XOFF(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e70ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_SW_XOFF", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_SW_XOFF(a,b) bdk_nixx_af_tl2x_sw_xoff_t
#define bustype_BDK_NIXX_AF_TL2X_SW_XOFF(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_SW_XOFF(a,b) "NIXX_AF_TL2X_SW_XOFF"
#define device_bar_BDK_NIXX_AF_TL2X_SW_XOFF(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_SW_XOFF(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_SW_XOFF(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_topology
 *
 * NIX AF Transmit Level 2 Topology Registers
 */
union bdk_nixx_af_tl2x_topology
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_topology_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_40_63        : 24;
        uint64_t prio_anchor           : 8;  /**< [ 39: 32](R/W) See NIX_AF_TL1()_TOPOLOGY[PRIO_ANCHOR]. */
        uint64_t reserved_5_31         : 27;
        uint64_t rr_prio               : 4;  /**< [  4:  1](R/W) See NIX_AF_TL1()_TOPOLOGY[RR_PRIO]. */
        uint64_t reserved_0            : 1;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0            : 1;
        uint64_t rr_prio               : 4;  /**< [  4:  1](R/W) See NIX_AF_TL1()_TOPOLOGY[RR_PRIO]. */
        uint64_t reserved_5_31         : 27;
        uint64_t prio_anchor           : 8;  /**< [ 39: 32](R/W) See NIX_AF_TL1()_TOPOLOGY[PRIO_ANCHOR]. */
        uint64_t reserved_40_63        : 24;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_topology_s cn; */
};
typedef union bdk_nixx_af_tl2x_topology bdk_nixx_af_tl2x_topology_t;

static inline uint64_t BDK_NIXX_AF_TL2X_TOPOLOGY(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_TOPOLOGY(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000e80ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_TOPOLOGY", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_TOPOLOGY(a,b) bdk_nixx_af_tl2x_topology_t
#define bustype_BDK_NIXX_AF_TL2X_TOPOLOGY(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_TOPOLOGY(a,b) "NIXX_AF_TL2X_TOPOLOGY"
#define device_bar_BDK_NIXX_AF_TL2X_TOPOLOGY(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_TOPOLOGY(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_TOPOLOGY(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2#_yellow
 *
 * INTERNAL: NIX Transmit Level 2 Yellow State Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1()_YELLOW.
 */
union bdk_nixx_af_tl2x_yellow
{
    uint64_t u;
    struct bdk_nixx_af_tl2x_yellow_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_18_63        : 46;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 8;  /**< [  7:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_8_9          : 2;
        uint64_t head                  : 8;  /**< [ 17: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_18_63        : 46;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2x_yellow_s cn; */
};
typedef union bdk_nixx_af_tl2x_yellow bdk_nixx_af_tl2x_yellow_t;

static inline uint64_t BDK_NIXX_AF_TL2X_YELLOW(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2X_YELLOW(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040000ea0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL2X_YELLOW", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2X_YELLOW(a,b) bdk_nixx_af_tl2x_yellow_t
#define bustype_BDK_NIXX_AF_TL2X_YELLOW(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2X_YELLOW(a,b) "NIXX_AF_TL2X_YELLOW"
#define device_bar_BDK_NIXX_AF_TL2X_YELLOW(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2X_YELLOW(a,b) (a)
#define arguments_BDK_NIXX_AF_TL2X_YELLOW(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2_const
 *
 * NIX AF Transmit Level 2 Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_tl2_const
{
    uint64_t u;
    struct bdk_nixx_af_tl2_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_16_63        : 48;
        uint64_t count                 : 16; /**< [ 15:  0](RO) Number of transmit level 2 shaping queues. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 16; /**< [ 15:  0](RO) Number of transmit level 2 shaping queues. */
        uint64_t reserved_16_63        : 48;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2_const_s cn; */
};
typedef union bdk_nixx_af_tl2_const bdk_nixx_af_tl2_const_t;

static inline uint64_t BDK_NIXX_AF_TL2_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000078ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL2_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2_CONST(a) bdk_nixx_af_tl2_const_t
#define bustype_BDK_NIXX_AF_TL2_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2_CONST(a) "NIXX_AF_TL2_CONST"
#define device_bar_BDK_NIXX_AF_TL2_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2_CONST(a) (a)
#define arguments_BDK_NIXX_AF_TL2_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2a_debug
 *
 * INTERNAL: NIX TL2A Internal Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1A_DEBUG.
 */
union bdk_nixx_af_tl2a_debug
{
    uint64_t u;
    struct bdk_nixx_af_tl2a_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#else /* Word 0 - Little Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2a_debug_s cn; */
};
typedef union bdk_nixx_af_tl2a_debug bdk_nixx_af_tl2a_debug_t;

static inline uint64_t BDK_NIXX_AF_TL2A_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2A_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000ee0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL2A_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2A_DEBUG(a) bdk_nixx_af_tl2a_debug_t
#define bustype_BDK_NIXX_AF_TL2A_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2A_DEBUG(a) "NIXX_AF_TL2A_DEBUG"
#define device_bar_BDK_NIXX_AF_TL2A_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2A_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_TL2A_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl2b_debug
 *
 * INTERNAL: NIX TL2B Internal Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1A_DEBUG.
 */
union bdk_nixx_af_tl2b_debug
{
    uint64_t u;
    struct bdk_nixx_af_tl2b_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#else /* Word 0 - Little Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl2b_debug_s cn; */
};
typedef union bdk_nixx_af_tl2b_debug bdk_nixx_af_tl2b_debug_t;

static inline uint64_t BDK_NIXX_AF_TL2B_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL2B_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000ef0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL2B_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL2B_DEBUG(a) bdk_nixx_af_tl2b_debug_t
#define bustype_BDK_NIXX_AF_TL2B_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL2B_DEBUG(a) "NIXX_AF_TL2B_DEBUG"
#define device_bar_BDK_NIXX_AF_TL2B_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL2B_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_TL2B_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_cir
 *
 * NIX AF Transmit Level 3 Committed Information Rate Registers
 * This register has the same bit fields as NIX_AF_TL1()_CIR.
 */
union bdk_nixx_af_tl3x_cir
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_cir_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_17_28        : 12;
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
#else /* Word 0 - Little Endian */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t reserved_17_28        : 12;
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_cir_s cn; */
};
typedef union bdk_nixx_af_tl3x_cir bdk_nixx_af_tl3x_cir_t;

static inline uint64_t BDK_NIXX_AF_TL3X_CIR(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_CIR(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001020ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_CIR", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_CIR(a,b) bdk_nixx_af_tl3x_cir_t
#define bustype_BDK_NIXX_AF_TL3X_CIR(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_CIR(a,b) "NIXX_AF_TL3X_CIR"
#define device_bar_BDK_NIXX_AF_TL3X_CIR(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_CIR(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_CIR(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_green
 *
 * INTERNAL: NIX Transmit Level 3 Green State Debug Register
 */
union bdk_nixx_af_tl3x_green
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_green_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t rr_active             : 1;  /**< [ 40: 40](R/W/H) Round-robin red active. Indicates that the round-robin input is mapped to RED. */
        uint64_t active_vec            : 20; /**< [ 39: 20](R/W/H) Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to this
                                                                 scheduling queue are active. For internal use only. */
        uint64_t reserved_19           : 1;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_19           : 1;
        uint64_t active_vec            : 20; /**< [ 39: 20](R/W/H) Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to this
                                                                 scheduling queue are active. For internal use only. */
        uint64_t rr_active             : 1;  /**< [ 40: 40](R/W/H) Round-robin red active. Indicates that the round-robin input is mapped to RED. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_green_s cn; */
};
typedef union bdk_nixx_af_tl3x_green bdk_nixx_af_tl3x_green_t;

static inline uint64_t BDK_NIXX_AF_TL3X_GREEN(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_GREEN(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001090ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_GREEN", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_GREEN(a,b) bdk_nixx_af_tl3x_green_t
#define bustype_BDK_NIXX_AF_TL3X_GREEN(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_GREEN(a,b) "NIXX_AF_TL3X_GREEN"
#define device_bar_BDK_NIXX_AF_TL3X_GREEN(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_GREEN(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_GREEN(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_md_debug0
 *
 * NIX AF Transmit Level 3 Meta Descriptor Debug 0 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl3x_md_debug0
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_md_debug0_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t pmd_count             : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor count, used to show the packet meta descriptor sequence. Which PMD
                                                                 is next to read and or write. */
        uint64_t reserved_62           : 1;
        uint64_t child                 : 10; /**< [ 61: 52](R/W/H) Child index, highest priority child. When [C_CON] of this result is set,
                                                                 indicating that this result is
                                                                 connected in a flow that extends through the child result, this is the index of that child
                                                                 result. */
        uint64_t reserved_50_51        : 2;
        uint64_t p_con                 : 1;  /**< [ 49: 49](R/W/H) Parent connected flag. This pick has more picks in front of it. */
        uint64_t c_con                 : 1;  /**< [ 48: 48](R/W/H) Child connected flag. This pick has more picks behind it. */
        uint64_t drain                 : 1;  /**< [ 47: 47](R/W/H) DRAIN command indicator. */
        uint64_t drain_pri             : 1;  /**< [ 46: 46](R/W/H) DRAIN priority indicator. */
        uint64_t reserved_34_45        : 12;
        uint64_t pmd1_vld              : 1;  /**< [ 33: 33](R/W/H) Packet meta-descriptor 1 valid. */
        uint64_t pmd0_vld              : 1;  /**< [ 32: 32](R/W/H) Packet meta-descriptor 0 valid. */
        uint64_t pmd1_length           : 16; /**< [ 31: 16](R/W/H) Packet meta-descriptor 1 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd0_length           : 16; /**< [ 15:  0](R/W/H) Packet meta-descriptor 0 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
#else /* Word 0 - Little Endian */
        uint64_t pmd0_length           : 16; /**< [ 15:  0](R/W/H) Packet meta-descriptor 0 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd1_length           : 16; /**< [ 31: 16](R/W/H) Packet meta-descriptor 1 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd0_vld              : 1;  /**< [ 32: 32](R/W/H) Packet meta-descriptor 0 valid. */
        uint64_t pmd1_vld              : 1;  /**< [ 33: 33](R/W/H) Packet meta-descriptor 1 valid. */
        uint64_t reserved_34_45        : 12;
        uint64_t drain_pri             : 1;  /**< [ 46: 46](R/W/H) DRAIN priority indicator. */
        uint64_t drain                 : 1;  /**< [ 47: 47](R/W/H) DRAIN command indicator. */
        uint64_t c_con                 : 1;  /**< [ 48: 48](R/W/H) Child connected flag. This pick has more picks behind it. */
        uint64_t p_con                 : 1;  /**< [ 49: 49](R/W/H) Parent connected flag. This pick has more picks in front of it. */
        uint64_t reserved_50_51        : 2;
        uint64_t child                 : 10; /**< [ 61: 52](R/W/H) Child index, highest priority child. When [C_CON] of this result is set,
                                                                 indicating that this result is
                                                                 connected in a flow that extends through the child result, this is the index of that child
                                                                 result. */
        uint64_t reserved_62           : 1;
        uint64_t pmd_count             : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor count, used to show the packet meta descriptor sequence. Which PMD
                                                                 is next to read and or write. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_md_debug0_s cn; */
};
typedef union bdk_nixx_af_tl3x_md_debug0 bdk_nixx_af_tl3x_md_debug0_t;

static inline uint64_t BDK_NIXX_AF_TL3X_MD_DEBUG0(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_MD_DEBUG0(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x8500400010c0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_MD_DEBUG0", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_MD_DEBUG0(a,b) bdk_nixx_af_tl3x_md_debug0_t
#define bustype_BDK_NIXX_AF_TL3X_MD_DEBUG0(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_MD_DEBUG0(a,b) "NIXX_AF_TL3X_MD_DEBUG0"
#define device_bar_BDK_NIXX_AF_TL3X_MD_DEBUG0(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_MD_DEBUG0(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_MD_DEBUG0(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_md_debug1
 *
 * NIX AF Transmit Level 3 Meta Descriptor Debug 1 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl3x_md_debug1
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_md_debug1_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t reserved_36           : 1;
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t reserved_0_5          : 6;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_5          : 6;
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t reserved_36           : 1;
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_md_debug1_s cn; */
};
typedef union bdk_nixx_af_tl3x_md_debug1 bdk_nixx_af_tl3x_md_debug1_t;

static inline uint64_t BDK_NIXX_AF_TL3X_MD_DEBUG1(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_MD_DEBUG1(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x8500400010c8ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_MD_DEBUG1", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_MD_DEBUG1(a,b) bdk_nixx_af_tl3x_md_debug1_t
#define bustype_BDK_NIXX_AF_TL3X_MD_DEBUG1(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_MD_DEBUG1(a,b) "NIXX_AF_TL3X_MD_DEBUG1"
#define device_bar_BDK_NIXX_AF_TL3X_MD_DEBUG1(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_MD_DEBUG1(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_MD_DEBUG1(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_md_debug2
 *
 * NIX AF Transmit Level 3 Meta Descriptor Debug 2 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl3x_md_debug2
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_md_debug2_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t reserved_36           : 1;
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE Packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t reserved_0_5          : 6;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_5          : 6;
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE Packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t reserved_36           : 1;
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_md_debug2_s cn; */
};
typedef union bdk_nixx_af_tl3x_md_debug2 bdk_nixx_af_tl3x_md_debug2_t;

static inline uint64_t BDK_NIXX_AF_TL3X_MD_DEBUG2(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_MD_DEBUG2(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x8500400010d0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_MD_DEBUG2", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_MD_DEBUG2(a,b) bdk_nixx_af_tl3x_md_debug2_t
#define bustype_BDK_NIXX_AF_TL3X_MD_DEBUG2(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_MD_DEBUG2(a,b) "NIXX_AF_TL3X_MD_DEBUG2"
#define device_bar_BDK_NIXX_AF_TL3X_MD_DEBUG2(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_MD_DEBUG2(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_MD_DEBUG2(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_md_debug3
 *
 * NIX AF Transmit Level 3 Meta Descriptor Debug 3 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl3x_md_debug3
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_md_debug3_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t reserved_0_38         : 39;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_38         : 39;
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_md_debug3_s cn; */
};
typedef union bdk_nixx_af_tl3x_md_debug3 bdk_nixx_af_tl3x_md_debug3_t;

static inline uint64_t BDK_NIXX_AF_TL3X_MD_DEBUG3(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_MD_DEBUG3(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x8500400010d8ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_MD_DEBUG3", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_MD_DEBUG3(a,b) bdk_nixx_af_tl3x_md_debug3_t
#define bustype_BDK_NIXX_AF_TL3X_MD_DEBUG3(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_MD_DEBUG3(a,b) "NIXX_AF_TL3X_MD_DEBUG3"
#define device_bar_BDK_NIXX_AF_TL3X_MD_DEBUG3(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_MD_DEBUG3(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_MD_DEBUG3(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_parent
 *
 * NIX AF Transmit Level 3 Parent Registers
 */
union bdk_nixx_af_tl3x_parent
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_parent_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_24_63        : 40;
        uint64_t parent                : 8;  /**< [ 23: 16](R/W) See NIX_AF_TL2()_PARENT[PARENT]. */
        uint64_t reserved_0_15         : 16;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_15         : 16;
        uint64_t parent                : 8;  /**< [ 23: 16](R/W) See NIX_AF_TL2()_PARENT[PARENT]. */
        uint64_t reserved_24_63        : 40;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_parent_s cn; */
};
typedef union bdk_nixx_af_tl3x_parent bdk_nixx_af_tl3x_parent_t;

static inline uint64_t BDK_NIXX_AF_TL3X_PARENT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_PARENT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001088ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_PARENT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_PARENT(a,b) bdk_nixx_af_tl3x_parent_t
#define bustype_BDK_NIXX_AF_TL3X_PARENT(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_PARENT(a,b) "NIXX_AF_TL3X_PARENT"
#define device_bar_BDK_NIXX_AF_TL3X_PARENT(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_PARENT(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_PARENT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_pir
 *
 * NIX AF Transmit Level 3 Peak Information Rate Registers
 * This register has the same bit fields as NIX_AF_TL1()_CIR.
 */
union bdk_nixx_af_tl3x_pir
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_pir_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_17_28        : 12;
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
#else /* Word 0 - Little Endian */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t reserved_17_28        : 12;
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_pir_s cn; */
};
typedef union bdk_nixx_af_tl3x_pir bdk_nixx_af_tl3x_pir_t;

static inline uint64_t BDK_NIXX_AF_TL3X_PIR(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_PIR(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001030ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_PIR", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_PIR(a,b) bdk_nixx_af_tl3x_pir_t
#define bustype_BDK_NIXX_AF_TL3X_PIR(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_PIR(a,b) "NIXX_AF_TL3X_PIR"
#define device_bar_BDK_NIXX_AF_TL3X_PIR(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_PIR(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_PIR(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_pointers
 *
 * INTERNAL: NIX Transmit Level 3 Linked List Pointers Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL2()_POINTERS.
 */
union bdk_nixx_af_tl3x_pointers
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_pointers_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_24_63        : 40;
        uint64_t prev                  : 8;  /**< [ 23: 16](R/W/H) Previous pointer. The linked-list previous pointer. */
        uint64_t reserved_8_15         : 8;
        uint64_t next                  : 8;  /**< [  7:  0](R/W/H) Next pointer. The linked-list next pointer. */
#else /* Word 0 - Little Endian */
        uint64_t next                  : 8;  /**< [  7:  0](R/W/H) Next pointer. The linked-list next pointer. */
        uint64_t reserved_8_15         : 8;
        uint64_t prev                  : 8;  /**< [ 23: 16](R/W/H) Previous pointer. The linked-list previous pointer. */
        uint64_t reserved_24_63        : 40;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_pointers_s cn; */
};
typedef union bdk_nixx_af_tl3x_pointers bdk_nixx_af_tl3x_pointers_t;

static inline uint64_t BDK_NIXX_AF_TL3X_POINTERS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_POINTERS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001060ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_POINTERS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_POINTERS(a,b) bdk_nixx_af_tl3x_pointers_t
#define bustype_BDK_NIXX_AF_TL3X_POINTERS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_POINTERS(a,b) "NIXX_AF_TL3X_POINTERS"
#define device_bar_BDK_NIXX_AF_TL3X_POINTERS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_POINTERS(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_POINTERS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_red
 *
 * INTERNAL: NIX Transmit Level 3 Red State Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL3()_YELLOW.
 */
union bdk_nixx_af_tl3x_red
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_red_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_19_63        : 45;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_19_63        : 45;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_red_s cn; */
};
typedef union bdk_nixx_af_tl3x_red bdk_nixx_af_tl3x_red_t;

static inline uint64_t BDK_NIXX_AF_TL3X_RED(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_RED(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x8500400010b0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_RED", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_RED(a,b) bdk_nixx_af_tl3x_red_t
#define bustype_BDK_NIXX_AF_TL3X_RED(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_RED(a,b) "NIXX_AF_TL3X_RED"
#define device_bar_BDK_NIXX_AF_TL3X_RED(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_RED(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_RED(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_sched_state
 *
 * NIX AF Transmit Level 3 Scheduling Control State Registers
 * This register has the same bit fields as NIX_AF_TL2()_SCHED_STATE.
 */
union bdk_nixx_af_tl3x_sched_state
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_sched_state_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_25_63        : 39;
        uint64_t rr_count              : 25; /**< [ 24:  0](R/W/H) Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
#else /* Word 0 - Little Endian */
        uint64_t rr_count              : 25; /**< [ 24:  0](R/W/H) Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
        uint64_t reserved_25_63        : 39;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_sched_state_s cn; */
};
typedef union bdk_nixx_af_tl3x_sched_state bdk_nixx_af_tl3x_sched_state_t;

static inline uint64_t BDK_NIXX_AF_TL3X_SCHED_STATE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_SCHED_STATE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001040ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_SCHED_STATE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_SCHED_STATE(a,b) bdk_nixx_af_tl3x_sched_state_t
#define bustype_BDK_NIXX_AF_TL3X_SCHED_STATE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_SCHED_STATE(a,b) "NIXX_AF_TL3X_SCHED_STATE"
#define device_bar_BDK_NIXX_AF_TL3X_SCHED_STATE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_SCHED_STATE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_SCHED_STATE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_schedule
 *
 * NIX AF Transmit Level 3 Scheduling Control Registers
 * This register has the same bit fields as NIX_AF_TL2()_SCHEDULE.
 */
union bdk_nixx_af_tl3x_schedule
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_schedule_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_28_63        : 36;
        uint64_t prio                  : 4;  /**< [ 27: 24](R/W) Priority. The priority used for this shaping queue in the (lower-level)
                                                                 parent's scheduling algorithm. When this shaping queue is not used, we
                                                                 recommend setting [PRIO] to zero. The legal [PRIO] values are zero to nine
                                                                 when the shaping queue is used. In addition to priority, [PRIO] determines
                                                                 whether the shaping queue is a static queue or not: If [PRIO] equals the
                                                                 parent's NIX_AF_TL*()_TOPOLOGY[RR_PRIO], then this is a round-robin child
                                                                 queue into the shaper at the next level. */
        uint64_t rr_quantum            : 24; /**< [ 23:  0](R/W) Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                                 integer). The packet size used in all DWRR (RR_COUNT) calculations is:

                                                                 _  (NIX_nm_SHAPE[LENGTH_DISABLE] ? 0 : (NIX_nm_MD*[LENGTH] + NIX_nm_MD*[ADJUST]))
                                                                    + NIX_nm_SHAPE[ADJUST]

                                                                 where nm corresponds to this NIX_nm_SCHEDULE CSR.

                                                                 Typically [RR_QUANTUM] should be at or near the MTU or more (to limit or prevent
                                                                 negative accumulations of the deficit count). */
#else /* Word 0 - Little Endian */
        uint64_t rr_quantum            : 24; /**< [ 23:  0](R/W) Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                                 integer). The packet size used in all DWRR (RR_COUNT) calculations is:

                                                                 _  (NIX_nm_SHAPE[LENGTH_DISABLE] ? 0 : (NIX_nm_MD*[LENGTH] + NIX_nm_MD*[ADJUST]))
                                                                    + NIX_nm_SHAPE[ADJUST]

                                                                 where nm corresponds to this NIX_nm_SCHEDULE CSR.

                                                                 Typically [RR_QUANTUM] should be at or near the MTU or more (to limit or prevent
                                                                 negative accumulations of the deficit count). */
        uint64_t prio                  : 4;  /**< [ 27: 24](R/W) Priority. The priority used for this shaping queue in the (lower-level)
                                                                 parent's scheduling algorithm. When this shaping queue is not used, we
                                                                 recommend setting [PRIO] to zero. The legal [PRIO] values are zero to nine
                                                                 when the shaping queue is used. In addition to priority, [PRIO] determines
                                                                 whether the shaping queue is a static queue or not: If [PRIO] equals the
                                                                 parent's NIX_AF_TL*()_TOPOLOGY[RR_PRIO], then this is a round-robin child
                                                                 queue into the shaper at the next level. */
        uint64_t reserved_28_63        : 36;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_schedule_s cn; */
};
typedef union bdk_nixx_af_tl3x_schedule bdk_nixx_af_tl3x_schedule_t;

static inline uint64_t BDK_NIXX_AF_TL3X_SCHEDULE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_SCHEDULE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001000ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_SCHEDULE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_SCHEDULE(a,b) bdk_nixx_af_tl3x_schedule_t
#define bustype_BDK_NIXX_AF_TL3X_SCHEDULE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_SCHEDULE(a,b) "NIXX_AF_TL3X_SCHEDULE"
#define device_bar_BDK_NIXX_AF_TL3X_SCHEDULE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_SCHEDULE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_SCHEDULE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_shape
 *
 * NIX AF Transmit Level 3 Shaping Control Registers
 */
union bdk_nixx_af_tl3x_shape
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_shape_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_27_63        : 37;
        uint64_t schedule_list         : 2;  /**< [ 26: 25](R/W) Shaper scheduling list. Restricts shaper scheduling to specific lists.
                                                                   0x0 = Normal (selected for nearly all scheduling/shaping applications).
                                                                   0x1 = Green-only.
                                                                   0x2 = Yellow-only.
                                                                   0x3 = Red-only. */
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t reserved_13_23        : 11;
        uint64_t yellow_disable        : 1;  /**< [ 12: 12](R/W) See NIX_AF_TL2()_SHAPE[YELLOW_DISABLE]. */
        uint64_t red_disable           : 1;  /**< [ 11: 11](R/W) See NIX_AF_TL2()_SHAPE[RED_DISABLE]. */
        uint64_t red_algo              : 2;  /**< [ 10:  9](R/W) See NIX_AF_TL2()_SHAPE[RED_ALGO]. */
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) See NIX_AF_TL2()_SHAPE[ADJUST]. */
#else /* Word 0 - Little Endian */
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) See NIX_AF_TL2()_SHAPE[ADJUST]. */
        uint64_t red_algo              : 2;  /**< [ 10:  9](R/W) See NIX_AF_TL2()_SHAPE[RED_ALGO]. */
        uint64_t red_disable           : 1;  /**< [ 11: 11](R/W) See NIX_AF_TL2()_SHAPE[RED_DISABLE]. */
        uint64_t yellow_disable        : 1;  /**< [ 12: 12](R/W) See NIX_AF_TL2()_SHAPE[YELLOW_DISABLE]. */
        uint64_t reserved_13_23        : 11;
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t schedule_list         : 2;  /**< [ 26: 25](R/W) Shaper scheduling list. Restricts shaper scheduling to specific lists.
                                                                   0x0 = Normal (selected for nearly all scheduling/shaping applications).
                                                                   0x1 = Green-only.
                                                                   0x2 = Yellow-only.
                                                                   0x3 = Red-only. */
        uint64_t reserved_27_63        : 37;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_shape_s cn; */
};
typedef union bdk_nixx_af_tl3x_shape bdk_nixx_af_tl3x_shape_t;

static inline uint64_t BDK_NIXX_AF_TL3X_SHAPE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_SHAPE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001010ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_SHAPE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_SHAPE(a,b) bdk_nixx_af_tl3x_shape_t
#define bustype_BDK_NIXX_AF_TL3X_SHAPE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_SHAPE(a,b) "NIXX_AF_TL3X_SHAPE"
#define device_bar_BDK_NIXX_AF_TL3X_SHAPE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_SHAPE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_SHAPE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_shape_state
 *
 * NIX AF Transmit Level 3 Shaping State Registers
 * This register has the same bit fields as NIX_AF_TL2()_SHAPE_STATE.
 * This register must not be written during normal operation.
 */
union bdk_nixx_af_tl3x_shape_state
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_shape_state_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_54_63        : 10;
        uint64_t color                 : 2;  /**< [ 53: 52](R/W/H) Shaper connection status. Debug access to the live shaper state.
                                                                 0x0 = Green - shaper is connected into the green list.
                                                                 0x1 = Yellow - shaper is connected into the yellow list.
                                                                 0x2 = Red - shaper is connected into the red list.
                                                                 0x3 = Pruned - shaper is disconnected. */
        uint64_t pir_accum             : 26; /**< [ 51: 26](R/W/H) Peak information rate accumulator. Debug access to the live PIR accumulator. */
        uint64_t cir_accum             : 26; /**< [ 25:  0](R/W/H) Committed information rate accumulator. Debug access to the live CIR accumulator. */
#else /* Word 0 - Little Endian */
        uint64_t cir_accum             : 26; /**< [ 25:  0](R/W/H) Committed information rate accumulator. Debug access to the live CIR accumulator. */
        uint64_t pir_accum             : 26; /**< [ 51: 26](R/W/H) Peak information rate accumulator. Debug access to the live PIR accumulator. */
        uint64_t color                 : 2;  /**< [ 53: 52](R/W/H) Shaper connection status. Debug access to the live shaper state.
                                                                 0x0 = Green - shaper is connected into the green list.
                                                                 0x1 = Yellow - shaper is connected into the yellow list.
                                                                 0x2 = Red - shaper is connected into the red list.
                                                                 0x3 = Pruned - shaper is disconnected. */
        uint64_t reserved_54_63        : 10;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_shape_state_s cn; */
};
typedef union bdk_nixx_af_tl3x_shape_state bdk_nixx_af_tl3x_shape_state_t;

static inline uint64_t BDK_NIXX_AF_TL3X_SHAPE_STATE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_SHAPE_STATE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001050ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_SHAPE_STATE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_SHAPE_STATE(a,b) bdk_nixx_af_tl3x_shape_state_t
#define bustype_BDK_NIXX_AF_TL3X_SHAPE_STATE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_SHAPE_STATE(a,b) "NIXX_AF_TL3X_SHAPE_STATE"
#define device_bar_BDK_NIXX_AF_TL3X_SHAPE_STATE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_SHAPE_STATE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_SHAPE_STATE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_sw_xoff
 *
 * NIX AF Transmit Level 3 Software Controlled XOFF Registers
 * This register has the same bit fields as NIX_AF_TL1()_SW_XOFF
 */
union bdk_nixx_af_tl3x_sw_xoff
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_sw_xoff_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_4_63         : 60;
        uint64_t drain_irq             : 1;  /**< [  3:  3](WO) Drain IRQ. Enables setting of NIX_AF_GEN_INT[TL1_DRAIN] when the drain
                                                                 operation has completed.
                                                                 [DRAIN_IRQ] should be set whenever [DRAIN] is, and must not be set when [DRAIN] isn't
                                                                 set. [DRAIN_IRQ] has no effect unless [DRAIN] and [XOFF] are also set. */
        uint64_t reserved_2            : 1;
        uint64_t drain                 : 1;  /**< [  1:  1](WO) Drain. This control activates a drain path through the PSE that starts at this queue
                                                                 and ends at the TL1 level. The drain path is prioritized over other paths through PSE
                                                                 and can be used in combination with [DRAIN_IRQ]. [DRAIN] need never be set for the
                                                                 TL1 level, but is useful at all other levels, including the TL4 and MDQ levels.
                                                                 NIX_AF_GEN_INT[TL1_DRAIN] should be clear prior to initiating a [DRAIN]=1 write to
                                                                 this CSR.

                                                                 After [DRAIN] is set for a shaping queue, it should not be set again, for
                                                                 this or any other shaping queue, until NIX_AF_GEN_INT[TL1_DRAIN] is set.

                                                                 [DRAIN] must not be set for any shaping queue when an SMQ FLUSH command is
                                                                 active (any NIX_AF_SMQ()_CFG[FLUSH] is set).

                                                                 DRAIN has no effect unless XOFF is also set. Only one DRAIN command is
                                                                 allowed to be active at a time. */
        uint64_t xoff                  : 1;  /**< [  0:  0](R/W) XOFF. Stops meta flow out of the TL1 shaping queue. When [XOFF] is set, the
                                                                 corresponding NIX_AF_TL*()_MD_DEBUG* (i.e. the meta descriptor) in the TL1
                                                                 shaping queue cannot be transferred to the next level. */
#else /* Word 0 - Little Endian */
        uint64_t xoff                  : 1;  /**< [  0:  0](R/W) XOFF. Stops meta flow out of the TL1 shaping queue. When [XOFF] is set, the
                                                                 corresponding NIX_AF_TL*()_MD_DEBUG* (i.e. the meta descriptor) in the TL1
                                                                 shaping queue cannot be transferred to the next level. */
        uint64_t drain                 : 1;  /**< [  1:  1](WO) Drain. This control activates a drain path through the PSE that starts at this queue
                                                                 and ends at the TL1 level. The drain path is prioritized over other paths through PSE
                                                                 and can be used in combination with [DRAIN_IRQ]. [DRAIN] need never be set for the
                                                                 TL1 level, but is useful at all other levels, including the TL4 and MDQ levels.
                                                                 NIX_AF_GEN_INT[TL1_DRAIN] should be clear prior to initiating a [DRAIN]=1 write to
                                                                 this CSR.

                                                                 After [DRAIN] is set for a shaping queue, it should not be set again, for
                                                                 this or any other shaping queue, until NIX_AF_GEN_INT[TL1_DRAIN] is set.

                                                                 [DRAIN] must not be set for any shaping queue when an SMQ FLUSH command is
                                                                 active (any NIX_AF_SMQ()_CFG[FLUSH] is set).

                                                                 DRAIN has no effect unless XOFF is also set. Only one DRAIN command is
                                                                 allowed to be active at a time. */
        uint64_t reserved_2            : 1;
        uint64_t drain_irq             : 1;  /**< [  3:  3](WO) Drain IRQ. Enables setting of NIX_AF_GEN_INT[TL1_DRAIN] when the drain
                                                                 operation has completed.
                                                                 [DRAIN_IRQ] should be set whenever [DRAIN] is, and must not be set when [DRAIN] isn't
                                                                 set. [DRAIN_IRQ] has no effect unless [DRAIN] and [XOFF] are also set. */
        uint64_t reserved_4_63         : 60;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_sw_xoff_s cn; */
};
typedef union bdk_nixx_af_tl3x_sw_xoff bdk_nixx_af_tl3x_sw_xoff_t;

static inline uint64_t BDK_NIXX_AF_TL3X_SW_XOFF(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_SW_XOFF(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001070ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_SW_XOFF", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_SW_XOFF(a,b) bdk_nixx_af_tl3x_sw_xoff_t
#define bustype_BDK_NIXX_AF_TL3X_SW_XOFF(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_SW_XOFF(a,b) "NIXX_AF_TL3X_SW_XOFF"
#define device_bar_BDK_NIXX_AF_TL3X_SW_XOFF(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_SW_XOFF(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_SW_XOFF(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_topology
 *
 * NIX AF Transmit Level 3 Topology Registers
 */
union bdk_nixx_af_tl3x_topology
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_topology_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t prio_anchor           : 9;  /**< [ 40: 32](R/W) See NIX_AF_TL1()_TOPOLOGY[PRIO_ANCHOR]. */
        uint64_t reserved_5_31         : 27;
        uint64_t rr_prio               : 4;  /**< [  4:  1](R/W) See NIX_AF_TL1()_TOPOLOGY[RR_PRIO]. */
        uint64_t reserved_0            : 1;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0            : 1;
        uint64_t rr_prio               : 4;  /**< [  4:  1](R/W) See NIX_AF_TL1()_TOPOLOGY[RR_PRIO]. */
        uint64_t reserved_5_31         : 27;
        uint64_t prio_anchor           : 9;  /**< [ 40: 32](R/W) See NIX_AF_TL1()_TOPOLOGY[PRIO_ANCHOR]. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_topology_s cn; */
};
typedef union bdk_nixx_af_tl3x_topology bdk_nixx_af_tl3x_topology_t;

static inline uint64_t BDK_NIXX_AF_TL3X_TOPOLOGY(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_TOPOLOGY(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001080ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_TOPOLOGY", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_TOPOLOGY(a,b) bdk_nixx_af_tl3x_topology_t
#define bustype_BDK_NIXX_AF_TL3X_TOPOLOGY(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_TOPOLOGY(a,b) "NIXX_AF_TL3X_TOPOLOGY"
#define device_bar_BDK_NIXX_AF_TL3X_TOPOLOGY(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_TOPOLOGY(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_TOPOLOGY(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3#_yellow
 *
 * INTERNAL: NIX Transmit Level 3 Yellow State Debug Register
 */
union bdk_nixx_af_tl3x_yellow
{
    uint64_t u;
    struct bdk_nixx_af_tl3x_yellow_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_19_63        : 45;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_19_63        : 45;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3x_yellow_s cn; */
};
typedef union bdk_nixx_af_tl3x_yellow bdk_nixx_af_tl3x_yellow_t;

static inline uint64_t BDK_NIXX_AF_TL3X_YELLOW(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3X_YELLOW(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x8500400010a0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3X_YELLOW", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3X_YELLOW(a,b) bdk_nixx_af_tl3x_yellow_t
#define bustype_BDK_NIXX_AF_TL3X_YELLOW(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3X_YELLOW(a,b) "NIXX_AF_TL3X_YELLOW"
#define device_bar_BDK_NIXX_AF_TL3X_YELLOW(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3X_YELLOW(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3X_YELLOW(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3_const
 *
 * NIX AF Transmit Level 3 Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_tl3_const
{
    uint64_t u;
    struct bdk_nixx_af_tl3_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_16_63        : 48;
        uint64_t count                 : 16; /**< [ 15:  0](RO) Number of transmit level 3 shaping queues. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 16; /**< [ 15:  0](RO) Number of transmit level 3 shaping queues. */
        uint64_t reserved_16_63        : 48;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3_const_s cn; */
};
typedef union bdk_nixx_af_tl3_const bdk_nixx_af_tl3_const_t;

static inline uint64_t BDK_NIXX_AF_TL3_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000080ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL3_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3_CONST(a) bdk_nixx_af_tl3_const_t
#define bustype_BDK_NIXX_AF_TL3_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3_CONST(a) "NIXX_AF_TL3_CONST"
#define device_bar_BDK_NIXX_AF_TL3_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3_CONST(a) (a)
#define arguments_BDK_NIXX_AF_TL3_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3_tl2#_bp_status
 *
 * NIX AF Transmit Level 3/2 Backpressure Status Registers
 */
union bdk_nixx_af_tl3_tl2x_bp_status
{
    uint64_t u;
    struct bdk_nixx_af_tl3_tl2x_bp_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t hw_xoff               : 1;  /**< [  0:  0](RO/H) Hardware XOFF status. Set if any of the channels mapped to this level 3 or level 2 queue
                                                                 with NIX_AF_TL3_TL2()_LINK()_CFG is backpressured with XOFF, or if any associated link
                                                                 is backpressured due to lack of credits (see NIX_AF_TX_LINK()_NORM_CREDIT,
                                                                 NIX_AF_TX_LINK()_EXPR_CREDIT). */
#else /* Word 0 - Little Endian */
        uint64_t hw_xoff               : 1;  /**< [  0:  0](RO/H) Hardware XOFF status. Set if any of the channels mapped to this level 3 or level 2 queue
                                                                 with NIX_AF_TL3_TL2()_LINK()_CFG is backpressured with XOFF, or if any associated link
                                                                 is backpressured due to lack of credits (see NIX_AF_TX_LINK()_NORM_CREDIT,
                                                                 NIX_AF_TX_LINK()_EXPR_CREDIT). */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3_tl2x_bp_status_s cn; */
};
typedef union bdk_nixx_af_tl3_tl2x_bp_status bdk_nixx_af_tl3_tl2x_bp_status_t;

static inline uint64_t BDK_NIXX_AF_TL3_TL2X_BP_STATUS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3_TL2X_BP_STATUS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001610ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3_TL2X_BP_STATUS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3_TL2X_BP_STATUS(a,b) bdk_nixx_af_tl3_tl2x_bp_status_t
#define bustype_BDK_NIXX_AF_TL3_TL2X_BP_STATUS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3_TL2X_BP_STATUS(a,b) "NIXX_AF_TL3_TL2X_BP_STATUS"
#define device_bar_BDK_NIXX_AF_TL3_TL2X_BP_STATUS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3_TL2X_BP_STATUS(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3_TL2X_BP_STATUS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3_tl2#_cfg
 *
 * NIX AF Transmit Level 3/2 Configuration Registers
 */
union bdk_nixx_af_tl3_tl2x_cfg
{
    uint64_t u;
    struct bdk_nixx_af_tl3_tl2x_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t express               : 1;  /**< [  0:  0](R/W) Express.
                                                                 0 = This level 3 or level 2 shaping queue transmits normal packets
                                                                 only and use NIX_AF_TX_LINK()_NORM_CREDIT registers for link credits.
                                                                 1 = This level 3 or level 2 shaping queue transmits express packets
                                                                 only and use NIX_AF_TX_LINK()_EXPR_CREDIT registers for link credits. */
#else /* Word 0 - Little Endian */
        uint64_t express               : 1;  /**< [  0:  0](R/W) Express.
                                                                 0 = This level 3 or level 2 shaping queue transmits normal packets
                                                                 only and use NIX_AF_TX_LINK()_NORM_CREDIT registers for link credits.
                                                                 1 = This level 3 or level 2 shaping queue transmits express packets
                                                                 only and use NIX_AF_TX_LINK()_EXPR_CREDIT registers for link credits. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3_tl2x_cfg_s cn; */
};
typedef union bdk_nixx_af_tl3_tl2x_cfg bdk_nixx_af_tl3_tl2x_cfg_t;

static inline uint64_t BDK_NIXX_AF_TL3_TL2X_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3_TL2X_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255)))
        return 0x850040001600ll + 0x10000000ll * ((a) & 0x0) + 0x40000ll * ((b) & 0xff);
    __bdk_csr_fatal("NIXX_AF_TL3_TL2X_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3_TL2X_CFG(a,b) bdk_nixx_af_tl3_tl2x_cfg_t
#define bustype_BDK_NIXX_AF_TL3_TL2X_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3_TL2X_CFG(a,b) "NIXX_AF_TL3_TL2X_CFG"
#define device_bar_BDK_NIXX_AF_TL3_TL2X_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3_TL2X_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_TL3_TL2X_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3_tl2#_link#_cfg
 *
 * NIX AF Transmit Level 3/2 Link Configuration Registers
 * These registers specify the links and associated channels that a given TL3 or
 * TL2 queue (depending on NIX_AF_PSE_CHANNEL_LEVEL[BP_LEVEL]) can transmit on.
 * Each TL3/TL2 queue can be enabled to transmit on and be backpressured by one or
 * more links and associated channels.
 */
union bdk_nixx_af_tl3_tl2x_linkx_cfg
{
    uint64_t u;
    struct bdk_nixx_af_tl3_tl2x_linkx_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_14_63        : 50;
        uint64_t bp_ena                : 1;  /**< [ 13: 13](R/W) Backpressure enable. When set, the TL3/TL2 queue responds to backpressure
                                                                 from (NIX_AF_TX_LINK()_HW_XOFF/_SW_XOFF[CHAN_XOFF\<[RELCHAN]\>). */
        uint64_t ena                   : 1;  /**< [ 12: 12](R/W) Enable. When set, the TL3/TL2 queue can transmit on relative channel number
                                                                 [RELCHAN] of this link, and will respond to backpressure from link credits
                                                                 (NIX_AF_TX_LINK()_NORM_CREDIT, NIX_AF_TX_LINK()_EXPR_CREDIT). If
                                                                 [BP_ENA] is set, the queue also responds to channel backpressure. */
        uint64_t reserved_8_11         : 4;
        uint64_t relchan               : 8;  /**< [  7:  0](R/W) Relative channel number within this link. See [ENA]. */
#else /* Word 0 - Little Endian */
        uint64_t relchan               : 8;  /**< [  7:  0](R/W) Relative channel number within this link. See [ENA]. */
        uint64_t reserved_8_11         : 4;
        uint64_t ena                   : 1;  /**< [ 12: 12](R/W) Enable. When set, the TL3/TL2 queue can transmit on relative channel number
                                                                 [RELCHAN] of this link, and will respond to backpressure from link credits
                                                                 (NIX_AF_TX_LINK()_NORM_CREDIT, NIX_AF_TX_LINK()_EXPR_CREDIT). If
                                                                 [BP_ENA] is set, the queue also responds to channel backpressure. */
        uint64_t bp_ena                : 1;  /**< [ 13: 13](R/W) Backpressure enable. When set, the TL3/TL2 queue responds to backpressure
                                                                 from (NIX_AF_TX_LINK()_HW_XOFF/_SW_XOFF[CHAN_XOFF\<[RELCHAN]\>). */
        uint64_t reserved_14_63        : 50;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3_tl2x_linkx_cfg_s cn; */
};
typedef union bdk_nixx_af_tl3_tl2x_linkx_cfg bdk_nixx_af_tl3_tl2x_linkx_cfg_t;

static inline uint64_t BDK_NIXX_AF_TL3_TL2X_LINKX_CFG(unsigned long a, unsigned long b, unsigned long c) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3_TL2X_LINKX_CFG(unsigned long a, unsigned long b, unsigned long c)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=255) && (c<=12)))
        return 0x850040001700ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xff) + 8ll * ((c) & 0xf);
    __bdk_csr_fatal("NIXX_AF_TL3_TL2X_LINKX_CFG", 3, a, b, c, 0);
}

#define typedef_BDK_NIXX_AF_TL3_TL2X_LINKX_CFG(a,b,c) bdk_nixx_af_tl3_tl2x_linkx_cfg_t
#define bustype_BDK_NIXX_AF_TL3_TL2X_LINKX_CFG(a,b,c) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3_TL2X_LINKX_CFG(a,b,c) "NIXX_AF_TL3_TL2X_LINKX_CFG"
#define device_bar_BDK_NIXX_AF_TL3_TL2X_LINKX_CFG(a,b,c) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3_TL2X_LINKX_CFG(a,b,c) (a)
#define arguments_BDK_NIXX_AF_TL3_TL2X_LINKX_CFG(a,b,c) (a),(b),(c),-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3a_debug
 *
 * INTERNAL: NIX TL3A Internal Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1A_DEBUG.
 */
union bdk_nixx_af_tl3a_debug
{
    uint64_t u;
    struct bdk_nixx_af_tl3a_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#else /* Word 0 - Little Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3a_debug_s cn; */
};
typedef union bdk_nixx_af_tl3a_debug bdk_nixx_af_tl3a_debug_t;

static inline uint64_t BDK_NIXX_AF_TL3A_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3A_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400010e0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL3A_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3A_DEBUG(a) bdk_nixx_af_tl3a_debug_t
#define bustype_BDK_NIXX_AF_TL3A_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3A_DEBUG(a) "NIXX_AF_TL3A_DEBUG"
#define device_bar_BDK_NIXX_AF_TL3A_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3A_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_TL3A_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl3b_debug
 *
 * INTERNAL: NIX TL3B Internal Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1A_DEBUG.
 */
union bdk_nixx_af_tl3b_debug
{
    uint64_t u;
    struct bdk_nixx_af_tl3b_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#else /* Word 0 - Little Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl3b_debug_s cn; */
};
typedef union bdk_nixx_af_tl3b_debug bdk_nixx_af_tl3b_debug_t;

static inline uint64_t BDK_NIXX_AF_TL3B_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL3B_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400010f0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL3B_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL3B_DEBUG(a) bdk_nixx_af_tl3b_debug_t
#define bustype_BDK_NIXX_AF_TL3B_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL3B_DEBUG(a) "NIXX_AF_TL3B_DEBUG"
#define device_bar_BDK_NIXX_AF_TL3B_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL3B_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_TL3B_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_bp_status
 *
 * NIX AF Transmit Level 4 Backpressure Status Registers
 */
union bdk_nixx_af_tl4x_bp_status
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_bp_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t hw_xoff               : 1;  /**< [  0:  0](RO/H) Hardware XOFF status. Set if NIX_AF_TL4()_SDP_LINK_CFG[BP_ENA] and the SDP
                                                                 channel selected by NIX_AF_TL4()_SDP_LINK_CFG[RELCHAN] is backpressure with
                                                                 XOFF, or if the SDP link is backpressured by NIX_AF_SDP_LINK_CREDIT. */
#else /* Word 0 - Little Endian */
        uint64_t hw_xoff               : 1;  /**< [  0:  0](RO/H) Hardware XOFF status. Set if NIX_AF_TL4()_SDP_LINK_CFG[BP_ENA] and the SDP
                                                                 channel selected by NIX_AF_TL4()_SDP_LINK_CFG[RELCHAN] is backpressure with
                                                                 XOFF, or if the SDP link is backpressured by NIX_AF_SDP_LINK_CREDIT. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_bp_status_s cn; */
};
typedef union bdk_nixx_af_tl4x_bp_status bdk_nixx_af_tl4x_bp_status_t;

static inline uint64_t BDK_NIXX_AF_TL4X_BP_STATUS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_BP_STATUS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040000b00ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_BP_STATUS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_BP_STATUS(a,b) bdk_nixx_af_tl4x_bp_status_t
#define bustype_BDK_NIXX_AF_TL4X_BP_STATUS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_BP_STATUS(a,b) "NIXX_AF_TL4X_BP_STATUS"
#define device_bar_BDK_NIXX_AF_TL4X_BP_STATUS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_BP_STATUS(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_BP_STATUS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_cir
 *
 * NIX AF Transmit Level 4 Committed Information Rate Registers
 * This register has the same bit fields as NIX_AF_TL1()_CIR.
 */
union bdk_nixx_af_tl4x_cir
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_cir_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_17_28        : 12;
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
#else /* Word 0 - Little Endian */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t reserved_17_28        : 12;
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_cir_s cn; */
};
typedef union bdk_nixx_af_tl4x_cir bdk_nixx_af_tl4x_cir_t;

static inline uint64_t BDK_NIXX_AF_TL4X_CIR(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_CIR(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001220ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_CIR", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_CIR(a,b) bdk_nixx_af_tl4x_cir_t
#define bustype_BDK_NIXX_AF_TL4X_CIR(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_CIR(a,b) "NIXX_AF_TL4X_CIR"
#define device_bar_BDK_NIXX_AF_TL4X_CIR(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_CIR(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_CIR(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_green
 *
 * INTERNAL: NIX Transmit Level 4 Green State Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL3()_GREEN.
 */
union bdk_nixx_af_tl4x_green
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_green_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t rr_active             : 1;  /**< [ 40: 40](R/W/H) Round-robin red active. Indicates that the round-robin input is mapped to RED. */
        uint64_t active_vec            : 20; /**< [ 39: 20](R/W/H) Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to this
                                                                 scheduling queue are active. For internal use only. */
        uint64_t reserved_19           : 1;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_19           : 1;
        uint64_t active_vec            : 20; /**< [ 39: 20](R/W/H) Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to this
                                                                 scheduling queue are active. For internal use only. */
        uint64_t rr_active             : 1;  /**< [ 40: 40](R/W/H) Round-robin red active. Indicates that the round-robin input is mapped to RED. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_green_s cn; */
};
typedef union bdk_nixx_af_tl4x_green bdk_nixx_af_tl4x_green_t;

static inline uint64_t BDK_NIXX_AF_TL4X_GREEN(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_GREEN(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001290ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_GREEN", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_GREEN(a,b) bdk_nixx_af_tl4x_green_t
#define bustype_BDK_NIXX_AF_TL4X_GREEN(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_GREEN(a,b) "NIXX_AF_TL4X_GREEN"
#define device_bar_BDK_NIXX_AF_TL4X_GREEN(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_GREEN(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_GREEN(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_md_debug0
 *
 * NIX AF Transmit Level 4 Meta Descriptor Debug 0 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl4x_md_debug0
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_md_debug0_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t pmd_count             : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor count, used to show the packet meta descriptor sequence. Which PMD
                                                                 is next to read and or write. */
        uint64_t reserved_62           : 1;
        uint64_t child                 : 10; /**< [ 61: 52](R/W/H) Child index, highest priority child. When [C_CON] of this result is set,
                                                                 indicating that this result is
                                                                 connected in a flow that extends through the child result, this is the index of that child
                                                                 result. */
        uint64_t reserved_50_51        : 2;
        uint64_t p_con                 : 1;  /**< [ 49: 49](R/W/H) Parent connected flag. This pick has more picks in front of it. */
        uint64_t c_con                 : 1;  /**< [ 48: 48](R/W/H) Child connected flag. This pick has more picks behind it. */
        uint64_t drain                 : 1;  /**< [ 47: 47](R/W/H) DRAIN command indicator. */
        uint64_t drain_pri             : 1;  /**< [ 46: 46](R/W/H) DRAIN priority indicator. */
        uint64_t reserved_34_45        : 12;
        uint64_t pmd1_vld              : 1;  /**< [ 33: 33](R/W/H) Packet meta-descriptor 1 valid. */
        uint64_t pmd0_vld              : 1;  /**< [ 32: 32](R/W/H) Packet meta-descriptor 0 valid. */
        uint64_t pmd1_length           : 16; /**< [ 31: 16](R/W/H) Packet meta-descriptor 1 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd0_length           : 16; /**< [ 15:  0](R/W/H) Packet meta-descriptor 0 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
#else /* Word 0 - Little Endian */
        uint64_t pmd0_length           : 16; /**< [ 15:  0](R/W/H) Packet meta-descriptor 0 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd1_length           : 16; /**< [ 31: 16](R/W/H) Packet meta-descriptor 1 packet length. Generally, the size of the outgoing packet
                                                                 including pad, optional VLAN bytes inserted by NIX_SEND_EXT_S[VLAN*] and potential Vtag
                                                                 insert bytes allowed  by NIX_AF_SMQ()_CFG[MAX_VTAG_INS], but excluding FCS
                                                                 and preamble.
                                                                 See NIX_AF_SMQ()_CFG[MINLEN]. */
        uint64_t pmd0_vld              : 1;  /**< [ 32: 32](R/W/H) Packet meta-descriptor 0 valid. */
        uint64_t pmd1_vld              : 1;  /**< [ 33: 33](R/W/H) Packet meta-descriptor 1 valid. */
        uint64_t reserved_34_45        : 12;
        uint64_t drain_pri             : 1;  /**< [ 46: 46](R/W/H) DRAIN priority indicator. */
        uint64_t drain                 : 1;  /**< [ 47: 47](R/W/H) DRAIN command indicator. */
        uint64_t c_con                 : 1;  /**< [ 48: 48](R/W/H) Child connected flag. This pick has more picks behind it. */
        uint64_t p_con                 : 1;  /**< [ 49: 49](R/W/H) Parent connected flag. This pick has more picks in front of it. */
        uint64_t reserved_50_51        : 2;
        uint64_t child                 : 10; /**< [ 61: 52](R/W/H) Child index, highest priority child. When [C_CON] of this result is set,
                                                                 indicating that this result is
                                                                 connected in a flow that extends through the child result, this is the index of that child
                                                                 result. */
        uint64_t reserved_62           : 1;
        uint64_t pmd_count             : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor count, used to show the packet meta descriptor sequence. Which PMD
                                                                 is next to read and or write. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_md_debug0_s cn; */
};
typedef union bdk_nixx_af_tl4x_md_debug0 bdk_nixx_af_tl4x_md_debug0_t;

static inline uint64_t BDK_NIXX_AF_TL4X_MD_DEBUG0(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_MD_DEBUG0(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x8500400012c0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_MD_DEBUG0", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_MD_DEBUG0(a,b) bdk_nixx_af_tl4x_md_debug0_t
#define bustype_BDK_NIXX_AF_TL4X_MD_DEBUG0(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_MD_DEBUG0(a,b) "NIXX_AF_TL4X_MD_DEBUG0"
#define device_bar_BDK_NIXX_AF_TL4X_MD_DEBUG0(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_MD_DEBUG0(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_MD_DEBUG0(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_md_debug1
 *
 * NIX AF Transmit Level 4 Meta Descriptor Debug 1 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl4x_md_debug1
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_md_debug1_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t reserved_36           : 1;
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t reserved_0_5          : 6;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_5          : 6;
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t reserved_36           : 1;
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_md_debug1_s cn; */
};
typedef union bdk_nixx_af_tl4x_md_debug1 bdk_nixx_af_tl4x_md_debug1_t;

static inline uint64_t BDK_NIXX_AF_TL4X_MD_DEBUG1(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_MD_DEBUG1(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x8500400012c8ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_MD_DEBUG1", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_MD_DEBUG1(a,b) bdk_nixx_af_tl4x_md_debug1_t
#define bustype_BDK_NIXX_AF_TL4X_MD_DEBUG1(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_MD_DEBUG1(a,b) "NIXX_AF_TL4X_MD_DEBUG1"
#define device_bar_BDK_NIXX_AF_TL4X_MD_DEBUG1(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_MD_DEBUG1(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_MD_DEBUG1(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_md_debug2
 *
 * NIX AF Transmit Level 4 Meta Descriptor Debug 2 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl4x_md_debug2
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_md_debug2_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t reserved_36           : 1;
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE Packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t reserved_0_5          : 6;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_5          : 6;
        uint64_t red_algo_override     : 2;  /**< [  7:  6](R/W/H) NIX_SEND_EXT_S[SHP_RA] from the corresponding packet descriptor. [RED_ALGO_OVERRIDE]
                                                                 is used by the TL4 through TL2 shapers, but not used by the TL1 rate limiters. */
        uint64_t cir_dis               : 1;  /**< [  8:  8](R/W/H) CIR disable. Committed shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [CIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [CIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t pir_dis               : 1;  /**< [  9:  9](R/W/H) PIR disable. Peak shaper disabled. Set when NIX_SEND_EXT_S[SHP_DIS] is set
                                                                 (i.e. [PIR_DIS]=NIX_SEND_EXT_S[SHP_DIS]). [PIR_DIS] is used by
                                                                 the TL4 through TL2 shapers, but not used by the TL1 rate limiters.
                                                                 [PIR_DIS] and [CIR_DIS] will always have the same value. */
        uint64_t adjust                : 9;  /**< [ 18: 10](R/W/H) Packet meta-descriptor adjust. The NIX_SEND_EXT_S[SHP_CHG] for the packet. */
        uint64_t uid                   : 4;  /**< [ 22: 19](R/W/H) Unique ID. 4-bit unique value assigned at the TL4 level, increments for each packet. */
        uint64_t drain                 : 1;  /**< [ 23: 23](R/W/H) DRAIN command indicator. */
        uint64_t bubble                : 1;  /**< [ 24: 24](R/W/H) This MD is a fake passed forward after a prune. */
        uint64_t color                 : 2;  /**< [ 26: 25](R/W/H) See NIX_COLORRESULT_E. */
        uint64_t pse_pkt_id            : 9;  /**< [ 35: 27](R/W/H) PSE Packet ID credits vector, pointer to reserved packet link credits. */
        uint64_t reserved_36           : 1;
        uint64_t tx_pkt_p2x            : 2;  /**< [ 38: 37](R/W/H) 0 = Reserved, PMD has not cleared link credit request.
                                                                 1 = Normal packet type.
                                                                 2 = Express packet type.
                                                                 3 = SDP packet type. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_md_debug2_s cn; */
};
typedef union bdk_nixx_af_tl4x_md_debug2 bdk_nixx_af_tl4x_md_debug2_t;

static inline uint64_t BDK_NIXX_AF_TL4X_MD_DEBUG2(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_MD_DEBUG2(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x8500400012d0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_MD_DEBUG2", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_MD_DEBUG2(a,b) bdk_nixx_af_tl4x_md_debug2_t
#define bustype_BDK_NIXX_AF_TL4X_MD_DEBUG2(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_MD_DEBUG2(a,b) "NIXX_AF_TL4X_MD_DEBUG2"
#define device_bar_BDK_NIXX_AF_TL4X_MD_DEBUG2(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_MD_DEBUG2(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_MD_DEBUG2(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_md_debug3
 *
 * NIX AF Transmit Level 4 Meta Descriptor Debug 3 Registers
 * See NIX_AF_TL1()_MD_DEBUG0.
 */
union bdk_nixx_af_tl4x_md_debug3
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_md_debug3_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
        uint64_t reserved_62           : 1;
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t reserved_0_38         : 39;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_38         : 39;
        uint64_t sqm_pkt_id            : 13; /**< [ 51: 39](R/W/H) Packet ID from SQM. */
        uint64_t mdq_idx               : 10; /**< [ 61: 52](R/W/H) Meta-descriptor queue index, MDQ source of PMD if VLD field is set. */
        uint64_t reserved_62           : 1;
        uint64_t vld                   : 1;  /**< [ 63: 63](R/W/H) Packet meta descriptor 0 valid. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_md_debug3_s cn; */
};
typedef union bdk_nixx_af_tl4x_md_debug3 bdk_nixx_af_tl4x_md_debug3_t;

static inline uint64_t BDK_NIXX_AF_TL4X_MD_DEBUG3(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_MD_DEBUG3(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x8500400012d8ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_MD_DEBUG3", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_MD_DEBUG3(a,b) bdk_nixx_af_tl4x_md_debug3_t
#define bustype_BDK_NIXX_AF_TL4X_MD_DEBUG3(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_MD_DEBUG3(a,b) "NIXX_AF_TL4X_MD_DEBUG3"
#define device_bar_BDK_NIXX_AF_TL4X_MD_DEBUG3(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_MD_DEBUG3(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_MD_DEBUG3(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_parent
 *
 * NIX AF Transmit Level 4 Parent Registers
 */
union bdk_nixx_af_tl4x_parent
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_parent_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_24_63        : 40;
        uint64_t parent                : 8;  /**< [ 23: 16](R/W) See NIX_AF_TL2()_PARENT[PARENT]. */
        uint64_t reserved_0_15         : 16;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_15         : 16;
        uint64_t parent                : 8;  /**< [ 23: 16](R/W) See NIX_AF_TL2()_PARENT[PARENT]. */
        uint64_t reserved_24_63        : 40;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_parent_s cn; */
};
typedef union bdk_nixx_af_tl4x_parent bdk_nixx_af_tl4x_parent_t;

static inline uint64_t BDK_NIXX_AF_TL4X_PARENT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_PARENT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001288ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_PARENT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_PARENT(a,b) bdk_nixx_af_tl4x_parent_t
#define bustype_BDK_NIXX_AF_TL4X_PARENT(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_PARENT(a,b) "NIXX_AF_TL4X_PARENT"
#define device_bar_BDK_NIXX_AF_TL4X_PARENT(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_PARENT(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_PARENT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_pir
 *
 * NIX AF Transmit Level 4 Peak Information Rate Registers
 * This register has the same bit fields as NIX_AF_TL1()_CIR.
 */
union bdk_nixx_af_tl4x_pir
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_pir_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_17_28        : 12;
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
#else /* Word 0 - Little Endian */
        uint64_t enable                : 1;  /**< [  0:  0](R/W) Enable. Enables CIR shaping. */
        uint64_t rate_mantissa         : 8;  /**< [  8:  1](R/W) Rate mantissa. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_exponent         : 4;  /**< [ 12:  9](R/W) Rate exponent. See [RATE_DIVIDER_EXPONENT]. */
        uint64_t rate_divider_exponent : 4;  /**< [ 16: 13](R/W) Rate divider exponent. This base-2 exponent is used to divide the
                                                                 data rate by specifying the number of time-wheel turns required before the
                                                                 rate accumulator is increased.

                                                                 The supported range for [RATE_DIVIDER_EXPONENT] is 0 to 12. Programmed
                                                                 values greater than 12 are treated as 12.

                                                                 The data rate in Mbits/sec is computed as follows:
                                                                 \<pre\>
                                                                 rate_div_exp = min(12, [RATE_DIVIDER_EXPONENT]);
                                                                 data_rate = 2 Mbps * (1.[RATE_MANTISSA] \<\< [RATE_EXPONENT]) / (1 \<\< rate_div_exp);
                                                                 \</pre\>

                                                                 Internal:
                                                                 Hardware generates a rate divider tick every 400 rst__gbl_100mhz_sclk_edge
                                                                 pulses, thus 100/400 = 0.25 MBytes/sec = 2 Mbps. This corresponds to a
                                                                 minimum of 1200 SCLK cycles per tick with SCLK \>= 300 MHz. Each TL2/TL3/TL4/MDQ
                                                                 samples its rate divider every 860 SCLK cycles and each TL1 samples every
                                                                 240 SCLK cycles, so the sample rates are  fast enough to keep up with the
                                                                 divider tick.

                                                                 Max rate = 2 Mbps * ((1 + (255/256)) \<\< 15) = 130 Mbps. */
        uint64_t reserved_17_28        : 12;
        uint64_t burst_mantissa        : 8;  /**< [ 36: 29](R/W) Burst mantissa. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t burst_exponent        : 4;  /**< [ 40: 37](R/W) Burst exponent. The burst limit is 1.[BURST_MANTISSA] \<\< ([BURST_EXPONENT] + 1).
                                                                 With [BURST_EXPONENT]=0xF and [BURST_MANTISSA]=0xFF, the burst limit is the largest
                                                                 possible value, which is 130,816 (0x1FF00) bytes. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_pir_s cn; */
};
typedef union bdk_nixx_af_tl4x_pir bdk_nixx_af_tl4x_pir_t;

static inline uint64_t BDK_NIXX_AF_TL4X_PIR(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_PIR(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001230ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_PIR", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_PIR(a,b) bdk_nixx_af_tl4x_pir_t
#define bustype_BDK_NIXX_AF_TL4X_PIR(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_PIR(a,b) "NIXX_AF_TL4X_PIR"
#define device_bar_BDK_NIXX_AF_TL4X_PIR(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_PIR(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_PIR(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_pointers
 *
 * INTERNAL: NIX Transmit Level 4 Linked List Pointers Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL2()_POINTERS.
 */
union bdk_nixx_af_tl4x_pointers
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_pointers_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_25_63        : 39;
        uint64_t prev                  : 9;  /**< [ 24: 16](R/W/H) See NIX_AF_TL2()_POINTERS[PREV]. */
        uint64_t reserved_9_15         : 7;
        uint64_t next                  : 9;  /**< [  8:  0](R/W/H) See NIX_AF_TL2()_POINTERS[NEXT]. */
#else /* Word 0 - Little Endian */
        uint64_t next                  : 9;  /**< [  8:  0](R/W/H) See NIX_AF_TL2()_POINTERS[NEXT]. */
        uint64_t reserved_9_15         : 7;
        uint64_t prev                  : 9;  /**< [ 24: 16](R/W/H) See NIX_AF_TL2()_POINTERS[PREV]. */
        uint64_t reserved_25_63        : 39;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_pointers_s cn; */
};
typedef union bdk_nixx_af_tl4x_pointers bdk_nixx_af_tl4x_pointers_t;

static inline uint64_t BDK_NIXX_AF_TL4X_POINTERS(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_POINTERS(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001260ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_POINTERS", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_POINTERS(a,b) bdk_nixx_af_tl4x_pointers_t
#define bustype_BDK_NIXX_AF_TL4X_POINTERS(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_POINTERS(a,b) "NIXX_AF_TL4X_POINTERS"
#define device_bar_BDK_NIXX_AF_TL4X_POINTERS(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_POINTERS(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_POINTERS(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_red
 *
 * INTERNAL: NIX Transmit Level 4 Red State Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL3()_YELLOW.
 */
union bdk_nixx_af_tl4x_red
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_red_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_19_63        : 45;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_19_63        : 45;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_red_s cn; */
};
typedef union bdk_nixx_af_tl4x_red bdk_nixx_af_tl4x_red_t;

static inline uint64_t BDK_NIXX_AF_TL4X_RED(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_RED(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x8500400012b0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_RED", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_RED(a,b) bdk_nixx_af_tl4x_red_t
#define bustype_BDK_NIXX_AF_TL4X_RED(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_RED(a,b) "NIXX_AF_TL4X_RED"
#define device_bar_BDK_NIXX_AF_TL4X_RED(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_RED(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_RED(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_sched_state
 *
 * NIX AF Transmit Level 4 Scheduling Control State Registers
 * This register has the same bit fields as NIX_AF_TL2()_SCHED_STATE.
 */
union bdk_nixx_af_tl4x_sched_state
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_sched_state_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_25_63        : 39;
        uint64_t rr_count              : 25; /**< [ 24:  0](R/W/H) Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
#else /* Word 0 - Little Endian */
        uint64_t rr_count              : 25; /**< [ 24:  0](R/W/H) Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
        uint64_t reserved_25_63        : 39;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_sched_state_s cn; */
};
typedef union bdk_nixx_af_tl4x_sched_state bdk_nixx_af_tl4x_sched_state_t;

static inline uint64_t BDK_NIXX_AF_TL4X_SCHED_STATE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_SCHED_STATE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001240ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_SCHED_STATE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_SCHED_STATE(a,b) bdk_nixx_af_tl4x_sched_state_t
#define bustype_BDK_NIXX_AF_TL4X_SCHED_STATE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_SCHED_STATE(a,b) "NIXX_AF_TL4X_SCHED_STATE"
#define device_bar_BDK_NIXX_AF_TL4X_SCHED_STATE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_SCHED_STATE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_SCHED_STATE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_schedule
 *
 * NIX AF Transmit Level 4 Scheduling Control Registers
 * This register has the same bit fields as NIX_AF_TL2()_SCHEDULE.
 */
union bdk_nixx_af_tl4x_schedule
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_schedule_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_28_63        : 36;
        uint64_t prio                  : 4;  /**< [ 27: 24](R/W) Priority. The priority used for this shaping queue in the (lower-level)
                                                                 parent's scheduling algorithm. When this shaping queue is not used, we
                                                                 recommend setting [PRIO] to zero. The legal [PRIO] values are zero to nine
                                                                 when the shaping queue is used. In addition to priority, [PRIO] determines
                                                                 whether the shaping queue is a static queue or not: If [PRIO] equals the
                                                                 parent's NIX_AF_TL*()_TOPOLOGY[RR_PRIO], then this is a round-robin child
                                                                 queue into the shaper at the next level. */
        uint64_t rr_quantum            : 24; /**< [ 23:  0](R/W) Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                                 integer). The packet size used in all DWRR (RR_COUNT) calculations is:

                                                                 _  (NIX_nm_SHAPE[LENGTH_DISABLE] ? 0 : (NIX_nm_MD*[LENGTH] + NIX_nm_MD*[ADJUST]))
                                                                    + NIX_nm_SHAPE[ADJUST]

                                                                 where nm corresponds to this NIX_nm_SCHEDULE CSR.

                                                                 Typically [RR_QUANTUM] should be at or near the MTU or more (to limit or prevent
                                                                 negative accumulations of the deficit count). */
#else /* Word 0 - Little Endian */
        uint64_t rr_quantum            : 24; /**< [ 23:  0](R/W) Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                                 integer). The packet size used in all DWRR (RR_COUNT) calculations is:

                                                                 _  (NIX_nm_SHAPE[LENGTH_DISABLE] ? 0 : (NIX_nm_MD*[LENGTH] + NIX_nm_MD*[ADJUST]))
                                                                    + NIX_nm_SHAPE[ADJUST]

                                                                 where nm corresponds to this NIX_nm_SCHEDULE CSR.

                                                                 Typically [RR_QUANTUM] should be at or near the MTU or more (to limit or prevent
                                                                 negative accumulations of the deficit count). */
        uint64_t prio                  : 4;  /**< [ 27: 24](R/W) Priority. The priority used for this shaping queue in the (lower-level)
                                                                 parent's scheduling algorithm. When this shaping queue is not used, we
                                                                 recommend setting [PRIO] to zero. The legal [PRIO] values are zero to nine
                                                                 when the shaping queue is used. In addition to priority, [PRIO] determines
                                                                 whether the shaping queue is a static queue or not: If [PRIO] equals the
                                                                 parent's NIX_AF_TL*()_TOPOLOGY[RR_PRIO], then this is a round-robin child
                                                                 queue into the shaper at the next level. */
        uint64_t reserved_28_63        : 36;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_schedule_s cn; */
};
typedef union bdk_nixx_af_tl4x_schedule bdk_nixx_af_tl4x_schedule_t;

static inline uint64_t BDK_NIXX_AF_TL4X_SCHEDULE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_SCHEDULE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001200ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_SCHEDULE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_SCHEDULE(a,b) bdk_nixx_af_tl4x_schedule_t
#define bustype_BDK_NIXX_AF_TL4X_SCHEDULE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_SCHEDULE(a,b) "NIXX_AF_TL4X_SCHEDULE"
#define device_bar_BDK_NIXX_AF_TL4X_SCHEDULE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_SCHEDULE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_SCHEDULE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_sdp_link_cfg
 *
 * NIX AF Transmit Level 4 Link Configuration Registers
 * These registers specify the which TL4 queues transmit to and are optionally
 * backpressured by SDP.
 */
union bdk_nixx_af_tl4x_sdp_link_cfg
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_sdp_link_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_14_63        : 50;
        uint64_t bp_ena                : 1;  /**< [ 13: 13](R/W) Backpressure enable. When set, the TL4 queue responds to backpressure from
                                                                 (NIX_AF_SDP_HW_XOFF()/_SW_XOFF()[CHAN_XOFF\<[RELCHAN]\>). */
        uint64_t ena                   : 1;  /**< [ 12: 12](R/W) Enable.
                                                                 0 = The TL4 queue will not transmit to SDP and may transmit to CGX and/or LBK.
                                                                 1 = The TL4 queue may only transmit to SDP and will respond to backpressure from
                                                                 NIX_AF_SDP_LINK_CREDIT. If [BP_ENA] is set, the queue also responds to channel
                                                                 backpressure. */
        uint64_t reserved_8_11         : 4;
        uint64_t relchan               : 8;  /**< [  7:  0](R/W) Relative channel number. When [BP_ENA] is set, this is the
                                                                 NIX_CHAN_E::SDP_CH() index of the SDP channel that may backpressure the TL4
                                                                 queue. */
#else /* Word 0 - Little Endian */
        uint64_t relchan               : 8;  /**< [  7:  0](R/W) Relative channel number. When [BP_ENA] is set, this is the
                                                                 NIX_CHAN_E::SDP_CH() index of the SDP channel that may backpressure the TL4
                                                                 queue. */
        uint64_t reserved_8_11         : 4;
        uint64_t ena                   : 1;  /**< [ 12: 12](R/W) Enable.
                                                                 0 = The TL4 queue will not transmit to SDP and may transmit to CGX and/or LBK.
                                                                 1 = The TL4 queue may only transmit to SDP and will respond to backpressure from
                                                                 NIX_AF_SDP_LINK_CREDIT. If [BP_ENA] is set, the queue also responds to channel
                                                                 backpressure. */
        uint64_t bp_ena                : 1;  /**< [ 13: 13](R/W) Backpressure enable. When set, the TL4 queue responds to backpressure from
                                                                 (NIX_AF_SDP_HW_XOFF()/_SW_XOFF()[CHAN_XOFF\<[RELCHAN]\>). */
        uint64_t reserved_14_63        : 50;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_sdp_link_cfg_s cn; */
};
typedef union bdk_nixx_af_tl4x_sdp_link_cfg bdk_nixx_af_tl4x_sdp_link_cfg_t;

static inline uint64_t BDK_NIXX_AF_TL4X_SDP_LINK_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_SDP_LINK_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040000b10ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_SDP_LINK_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_SDP_LINK_CFG(a,b) bdk_nixx_af_tl4x_sdp_link_cfg_t
#define bustype_BDK_NIXX_AF_TL4X_SDP_LINK_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_SDP_LINK_CFG(a,b) "NIXX_AF_TL4X_SDP_LINK_CFG"
#define device_bar_BDK_NIXX_AF_TL4X_SDP_LINK_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_SDP_LINK_CFG(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_SDP_LINK_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_shape
 *
 * NIX AF Transmit Level 4 Shaping Control Registers
 * This register has the same bit fields as NIX_AF_TL2()_SHAPE.
 */
union bdk_nixx_af_tl4x_shape
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_shape_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_27_63        : 37;
        uint64_t schedule_list         : 2;  /**< [ 26: 25](R/W) Shaper scheduling list. Restricts shaper scheduling to specific lists.
                                                                   0x0 = Normal (selected for nearly all scheduling/shaping applications).
                                                                   0x1 = Green-only.
                                                                   0x2 = Yellow-only.
                                                                   0x3 = Red-only. */
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t reserved_13_23        : 11;
        uint64_t yellow_disable        : 1;  /**< [ 12: 12](R/W) See NIX_AF_TL2()_SHAPE[YELLOW_DISABLE]. */
        uint64_t red_disable           : 1;  /**< [ 11: 11](R/W) See NIX_AF_TL2()_SHAPE[RED_DISABLE]. */
        uint64_t red_algo              : 2;  /**< [ 10:  9](R/W) See NIX_AF_TL2()_SHAPE[RED_ALGO]. */
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) See NIX_AF_TL2()_SHAPE[ADJUST]. */
#else /* Word 0 - Little Endian */
        uint64_t adjust                : 9;  /**< [  8:  0](R/W) See NIX_AF_TL2()_SHAPE[ADJUST]. */
        uint64_t red_algo              : 2;  /**< [ 10:  9](R/W) See NIX_AF_TL2()_SHAPE[RED_ALGO]. */
        uint64_t red_disable           : 1;  /**< [ 11: 11](R/W) See NIX_AF_TL2()_SHAPE[RED_DISABLE]. */
        uint64_t yellow_disable        : 1;  /**< [ 12: 12](R/W) See NIX_AF_TL2()_SHAPE[YELLOW_DISABLE]. */
        uint64_t reserved_13_23        : 11;
        uint64_t length_disable        : 1;  /**< [ 24: 24](R/W) Length disable. Disables the use of packet lengths in DWRR scheduling
                                                                 and shaping calculations such that only the value of [ADJUST] is used. */
        uint64_t schedule_list         : 2;  /**< [ 26: 25](R/W) Shaper scheduling list. Restricts shaper scheduling to specific lists.
                                                                   0x0 = Normal (selected for nearly all scheduling/shaping applications).
                                                                   0x1 = Green-only.
                                                                   0x2 = Yellow-only.
                                                                   0x3 = Red-only. */
        uint64_t reserved_27_63        : 37;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_shape_s cn; */
};
typedef union bdk_nixx_af_tl4x_shape bdk_nixx_af_tl4x_shape_t;

static inline uint64_t BDK_NIXX_AF_TL4X_SHAPE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_SHAPE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001210ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_SHAPE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_SHAPE(a,b) bdk_nixx_af_tl4x_shape_t
#define bustype_BDK_NIXX_AF_TL4X_SHAPE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_SHAPE(a,b) "NIXX_AF_TL4X_SHAPE"
#define device_bar_BDK_NIXX_AF_TL4X_SHAPE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_SHAPE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_SHAPE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_shape_state
 *
 * NIX AF Transmit Level 4 Shaping State Registers
 * This register has the same bit fields as NIX_AF_TL2()_SHAPE_STATE.
 * This register must not be written during normal operation.
 */
union bdk_nixx_af_tl4x_shape_state
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_shape_state_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_54_63        : 10;
        uint64_t color                 : 2;  /**< [ 53: 52](R/W/H) Shaper connection status. Debug access to the live shaper state.
                                                                 0x0 = Green - shaper is connected into the green list.
                                                                 0x1 = Yellow - shaper is connected into the yellow list.
                                                                 0x2 = Red - shaper is connected into the red list.
                                                                 0x3 = Pruned - shaper is disconnected. */
        uint64_t pir_accum             : 26; /**< [ 51: 26](R/W/H) Peak information rate accumulator. Debug access to the live PIR accumulator. */
        uint64_t cir_accum             : 26; /**< [ 25:  0](R/W/H) Committed information rate accumulator. Debug access to the live CIR accumulator. */
#else /* Word 0 - Little Endian */
        uint64_t cir_accum             : 26; /**< [ 25:  0](R/W/H) Committed information rate accumulator. Debug access to the live CIR accumulator. */
        uint64_t pir_accum             : 26; /**< [ 51: 26](R/W/H) Peak information rate accumulator. Debug access to the live PIR accumulator. */
        uint64_t color                 : 2;  /**< [ 53: 52](R/W/H) Shaper connection status. Debug access to the live shaper state.
                                                                 0x0 = Green - shaper is connected into the green list.
                                                                 0x1 = Yellow - shaper is connected into the yellow list.
                                                                 0x2 = Red - shaper is connected into the red list.
                                                                 0x3 = Pruned - shaper is disconnected. */
        uint64_t reserved_54_63        : 10;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_shape_state_s cn; */
};
typedef union bdk_nixx_af_tl4x_shape_state bdk_nixx_af_tl4x_shape_state_t;

static inline uint64_t BDK_NIXX_AF_TL4X_SHAPE_STATE(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_SHAPE_STATE(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001250ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_SHAPE_STATE", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_SHAPE_STATE(a,b) bdk_nixx_af_tl4x_shape_state_t
#define bustype_BDK_NIXX_AF_TL4X_SHAPE_STATE(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_SHAPE_STATE(a,b) "NIXX_AF_TL4X_SHAPE_STATE"
#define device_bar_BDK_NIXX_AF_TL4X_SHAPE_STATE(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_SHAPE_STATE(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_SHAPE_STATE(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_sw_xoff
 *
 * NIX AF Transmit Level 4 Software Controlled XOFF Registers
 * This register has the same bit fields as NIX_AF_TL1()_SW_XOFF
 */
union bdk_nixx_af_tl4x_sw_xoff
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_sw_xoff_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_4_63         : 60;
        uint64_t drain_irq             : 1;  /**< [  3:  3](WO) Drain IRQ. Enables setting of NIX_AF_GEN_INT[TL1_DRAIN] when the drain
                                                                 operation has completed.
                                                                 [DRAIN_IRQ] should be set whenever [DRAIN] is, and must not be set when [DRAIN] isn't
                                                                 set. [DRAIN_IRQ] has no effect unless [DRAIN] and [XOFF] are also set. */
        uint64_t reserved_2            : 1;
        uint64_t drain                 : 1;  /**< [  1:  1](WO) Drain. This control activates a drain path through the PSE that starts at this queue
                                                                 and ends at the TL1 level. The drain path is prioritized over other paths through PSE
                                                                 and can be used in combination with [DRAIN_IRQ]. [DRAIN] need never be set for the
                                                                 TL1 level, but is useful at all other levels, including the TL4 and MDQ levels.
                                                                 NIX_AF_GEN_INT[TL1_DRAIN] should be clear prior to initiating a [DRAIN]=1 write to
                                                                 this CSR.

                                                                 After [DRAIN] is set for a shaping queue, it should not be set again, for
                                                                 this or any other shaping queue, until NIX_AF_GEN_INT[TL1_DRAIN] is set.

                                                                 [DRAIN] must not be set for any shaping queue when an SMQ FLUSH command is
                                                                 active (any NIX_AF_SMQ()_CFG[FLUSH] is set).

                                                                 DRAIN has no effect unless XOFF is also set. Only one DRAIN command is
                                                                 allowed to be active at a time. */
        uint64_t xoff                  : 1;  /**< [  0:  0](R/W) XOFF. Stops meta flow out of the TL1 shaping queue. When [XOFF] is set, the
                                                                 corresponding NIX_AF_TL*()_MD_DEBUG* (i.e. the meta descriptor) in the TL1
                                                                 shaping queue cannot be transferred to the next level. */
#else /* Word 0 - Little Endian */
        uint64_t xoff                  : 1;  /**< [  0:  0](R/W) XOFF. Stops meta flow out of the TL1 shaping queue. When [XOFF] is set, the
                                                                 corresponding NIX_AF_TL*()_MD_DEBUG* (i.e. the meta descriptor) in the TL1
                                                                 shaping queue cannot be transferred to the next level. */
        uint64_t drain                 : 1;  /**< [  1:  1](WO) Drain. This control activates a drain path through the PSE that starts at this queue
                                                                 and ends at the TL1 level. The drain path is prioritized over other paths through PSE
                                                                 and can be used in combination with [DRAIN_IRQ]. [DRAIN] need never be set for the
                                                                 TL1 level, but is useful at all other levels, including the TL4 and MDQ levels.
                                                                 NIX_AF_GEN_INT[TL1_DRAIN] should be clear prior to initiating a [DRAIN]=1 write to
                                                                 this CSR.

                                                                 After [DRAIN] is set for a shaping queue, it should not be set again, for
                                                                 this or any other shaping queue, until NIX_AF_GEN_INT[TL1_DRAIN] is set.

                                                                 [DRAIN] must not be set for any shaping queue when an SMQ FLUSH command is
                                                                 active (any NIX_AF_SMQ()_CFG[FLUSH] is set).

                                                                 DRAIN has no effect unless XOFF is also set. Only one DRAIN command is
                                                                 allowed to be active at a time. */
        uint64_t reserved_2            : 1;
        uint64_t drain_irq             : 1;  /**< [  3:  3](WO) Drain IRQ. Enables setting of NIX_AF_GEN_INT[TL1_DRAIN] when the drain
                                                                 operation has completed.
                                                                 [DRAIN_IRQ] should be set whenever [DRAIN] is, and must not be set when [DRAIN] isn't
                                                                 set. [DRAIN_IRQ] has no effect unless [DRAIN] and [XOFF] are also set. */
        uint64_t reserved_4_63         : 60;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_sw_xoff_s cn; */
};
typedef union bdk_nixx_af_tl4x_sw_xoff bdk_nixx_af_tl4x_sw_xoff_t;

static inline uint64_t BDK_NIXX_AF_TL4X_SW_XOFF(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_SW_XOFF(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001270ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_SW_XOFF", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_SW_XOFF(a,b) bdk_nixx_af_tl4x_sw_xoff_t
#define bustype_BDK_NIXX_AF_TL4X_SW_XOFF(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_SW_XOFF(a,b) "NIXX_AF_TL4X_SW_XOFF"
#define device_bar_BDK_NIXX_AF_TL4X_SW_XOFF(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_SW_XOFF(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_SW_XOFF(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_topology
 *
 * NIX AF Transmit Level 4 Topology Registers
 */
union bdk_nixx_af_tl4x_topology
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_topology_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_41_63        : 23;
        uint64_t prio_anchor           : 9;  /**< [ 40: 32](R/W) See NIX_AF_TL1()_TOPOLOGY[PRIO_ANCHOR]. */
        uint64_t reserved_5_31         : 27;
        uint64_t rr_prio               : 4;  /**< [  4:  1](R/W) See NIX_AF_TL1()_TOPOLOGY[RR_PRIO]. */
        uint64_t reserved_0            : 1;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0            : 1;
        uint64_t rr_prio               : 4;  /**< [  4:  1](R/W) See NIX_AF_TL1()_TOPOLOGY[RR_PRIO]. */
        uint64_t reserved_5_31         : 27;
        uint64_t prio_anchor           : 9;  /**< [ 40: 32](R/W) See NIX_AF_TL1()_TOPOLOGY[PRIO_ANCHOR]. */
        uint64_t reserved_41_63        : 23;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_topology_s cn; */
};
typedef union bdk_nixx_af_tl4x_topology bdk_nixx_af_tl4x_topology_t;

static inline uint64_t BDK_NIXX_AF_TL4X_TOPOLOGY(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_TOPOLOGY(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x850040001280ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_TOPOLOGY", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_TOPOLOGY(a,b) bdk_nixx_af_tl4x_topology_t
#define bustype_BDK_NIXX_AF_TL4X_TOPOLOGY(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_TOPOLOGY(a,b) "NIXX_AF_TL4X_TOPOLOGY"
#define device_bar_BDK_NIXX_AF_TL4X_TOPOLOGY(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_TOPOLOGY(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_TOPOLOGY(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4#_yellow
 *
 * INTERNAL: NIX Transmit Level 4 Yellow State Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL3()_YELLOW
 */
union bdk_nixx_af_tl4x_yellow
{
    uint64_t u;
    struct bdk_nixx_af_tl4x_yellow_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_19_63        : 45;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 9;  /**< [  8:  0](R/W/H) Tail pointer. The index of round-robin linked-list tail. For internal use only. */
        uint64_t reserved_9            : 1;
        uint64_t head                  : 9;  /**< [ 18: 10](R/W/H) Head pointer. The index of round-robin linked-list head. For internal use only. */
        uint64_t reserved_19_63        : 45;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4x_yellow_s cn; */
};
typedef union bdk_nixx_af_tl4x_yellow bdk_nixx_af_tl4x_yellow_t;

static inline uint64_t BDK_NIXX_AF_TL4X_YELLOW(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4X_YELLOW(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=511)))
        return 0x8500400012a0ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x1ff);
    __bdk_csr_fatal("NIXX_AF_TL4X_YELLOW", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4X_YELLOW(a,b) bdk_nixx_af_tl4x_yellow_t
#define bustype_BDK_NIXX_AF_TL4X_YELLOW(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4X_YELLOW(a,b) "NIXX_AF_TL4X_YELLOW"
#define device_bar_BDK_NIXX_AF_TL4X_YELLOW(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4X_YELLOW(a,b) (a)
#define arguments_BDK_NIXX_AF_TL4X_YELLOW(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4_const
 *
 * NIX AF Transmit Level 4 Constants Register
 * This register contains constants for software discovery.
 */
union bdk_nixx_af_tl4_const
{
    uint64_t u;
    struct bdk_nixx_af_tl4_const_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_16_63        : 48;
        uint64_t count                 : 16; /**< [ 15:  0](RO) Number of transmit level 4 shaping queues. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 16; /**< [ 15:  0](RO) Number of transmit level 4 shaping queues. */
        uint64_t reserved_16_63        : 48;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4_const_s cn; */
};
typedef union bdk_nixx_af_tl4_const bdk_nixx_af_tl4_const_t;

static inline uint64_t BDK_NIXX_AF_TL4_CONST(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4_CONST(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000088ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL4_CONST", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4_CONST(a) bdk_nixx_af_tl4_const_t
#define bustype_BDK_NIXX_AF_TL4_CONST(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4_CONST(a) "NIXX_AF_TL4_CONST"
#define device_bar_BDK_NIXX_AF_TL4_CONST(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4_CONST(a) (a)
#define arguments_BDK_NIXX_AF_TL4_CONST(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4a_debug
 *
 * INTERNAL: NIX TL4A Internal Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1A_DEBUG.
 */
union bdk_nixx_af_tl4a_debug
{
    uint64_t u;
    struct bdk_nixx_af_tl4a_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#else /* Word 0 - Little Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4a_debug_s cn; */
};
typedef union bdk_nixx_af_tl4a_debug bdk_nixx_af_tl4a_debug_t;

static inline uint64_t BDK_NIXX_AF_TL4A_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4A_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400012e0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL4A_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4A_DEBUG(a) bdk_nixx_af_tl4a_debug_t
#define bustype_BDK_NIXX_AF_TL4A_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4A_DEBUG(a) "NIXX_AF_TL4A_DEBUG"
#define device_bar_BDK_NIXX_AF_TL4A_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4A_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_TL4A_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tl4b_debug
 *
 * INTERNAL: NIX TL4B Internal Debug Register
 *
 * This register has the same bit fields as NIX_AF_TL1A_DEBUG.
 */
union bdk_nixx_af_tl4b_debug
{
    uint64_t u;
    struct bdk_nixx_af_tl4b_debug_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#else /* Word 0 - Little Endian */
        uint64_t dbg_vec               : 64; /**< [ 63:  0](RO/H) Debug vector. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tl4b_debug_s cn; */
};
typedef union bdk_nixx_af_tl4b_debug bdk_nixx_af_tl4b_debug_t;

static inline uint64_t BDK_NIXX_AF_TL4B_DEBUG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TL4B_DEBUG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400012f0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TL4B_DEBUG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TL4B_DEBUG(a) bdk_nixx_af_tl4b_debug_t
#define bustype_BDK_NIXX_AF_TL4B_DEBUG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TL4B_DEBUG(a) "NIXX_AF_TL4B_DEBUG"
#define device_bar_BDK_NIXX_AF_TL4B_DEBUG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TL4B_DEBUG(a) (a)
#define arguments_BDK_NIXX_AF_TL4B_DEBUG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tx_expr_credit
 *
 * NIX AF Transmit Express Credit Register
 * This register tracks internal credits for express packets sent to CGX and LBK
 * that may potentially preempt normal packets.
 */
union bdk_nixx_af_tx_expr_credit
{
    uint64_t u;
    struct bdk_nixx_af_tx_expr_credit_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_32_63        : 32;
        uint64_t cc_unit_cnt           : 20; /**< [ 31: 12](R/W) Credit unit count. This value, plus 1 MTU, represents the maximum
                                                                 outstanding aggregate credit units for all packets associated with this
                                                                 register (normal or express packets to CGX and LBK). A credit unit is 16
                                                                 bytes. Note that this 20-bit field represents a signed value that
                                                                 decrements towards zero as credits are used. Packets are not allowed to
                                                                 flow when the count is less than zero. As such, the most significant bit
                                                                 should normally be programmed as zero (positive count). This gives a
                                                                 maximum value for this field of 2^19 - 1.

                                                                 The recommended value is
                                                                 [CC_UNIT_CNT] = (10 * Max_aggregate_CGX_plus_LBK_Data_Rate),
                                                                 e.g. [CC_UNIT_CNT] = 1000 for 100 Gbps data rate.

                                                                 Internal:
                                                                 Recommended value is sized for specified data rate with 1200ns round trip
                                                                 latency. See LBK example in NIX_AF_TX_LINK()_NORM_CREDIT[CC_UNIT_CNT]. */
        uint64_t cc_packet_cnt         : 10; /**< [ 11:  2](R/W) Credit packet count. This value, plus 1, represents the maximum outstanding
                                                                 aggregate packet count for packets associated with this register (normal or
                                                                 express packets to CGX and LBK).
                                                                 Note that this 10-bit field represents a signed value that decrements
                                                                 towards zero as credits are used. Packets are not allowed to flow when the
                                                                 count is less than zero. As such the most significant bit should be
                                                                 programmed as zero (positive count).

                                                                 The recommended value is 255. Must satisfy:
                                                                 _  (NIX_AF_TX_NORM_CREDIT[CC_PACKET_COUNT] +
                                                                 NIX_AF_TX_EXPR_CREDIT[CC_PACKET_COUNT]) \< 511

                                                                 Internal:
                                                                 Limited by the 512 non-SDP PSE packet IDs. */
        uint64_t reserved_0_1          : 2;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_1          : 2;
        uint64_t cc_packet_cnt         : 10; /**< [ 11:  2](R/W) Credit packet count. This value, plus 1, represents the maximum outstanding
                                                                 aggregate packet count for packets associated with this register (normal or
                                                                 express packets to CGX and LBK).
                                                                 Note that this 10-bit field represents a signed value that decrements
                                                                 towards zero as credits are used. Packets are not allowed to flow when the
                                                                 count is less than zero. As such the most significant bit should be
                                                                 programmed as zero (positive count).

                                                                 The recommended value is 255. Must satisfy:
                                                                 _  (NIX_AF_TX_NORM_CREDIT[CC_PACKET_COUNT] +
                                                                 NIX_AF_TX_EXPR_CREDIT[CC_PACKET_COUNT]) \< 511

                                                                 Internal:
                                                                 Limited by the 512 non-SDP PSE packet IDs. */
        uint64_t cc_unit_cnt           : 20; /**< [ 31: 12](R/W) Credit unit count. This value, plus 1 MTU, represents the maximum
                                                                 outstanding aggregate credit units for all packets associated with this
                                                                 register (normal or express packets to CGX and LBK). A credit unit is 16
                                                                 bytes. Note that this 20-bit field represents a signed value that
                                                                 decrements towards zero as credits are used. Packets are not allowed to
                                                                 flow when the count is less than zero. As such, the most significant bit
                                                                 should normally be programmed as zero (positive count). This gives a
                                                                 maximum value for this field of 2^19 - 1.

                                                                 The recommended value is
                                                                 [CC_UNIT_CNT] = (10 * Max_aggregate_CGX_plus_LBK_Data_Rate),
                                                                 e.g. [CC_UNIT_CNT] = 1000 for 100 Gbps data rate.

                                                                 Internal:
                                                                 Recommended value is sized for specified data rate with 1200ns round trip
                                                                 latency. See LBK example in NIX_AF_TX_LINK()_NORM_CREDIT[CC_UNIT_CNT]. */
        uint64_t reserved_32_63        : 32;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tx_expr_credit_s cn; */
};
typedef union bdk_nixx_af_tx_expr_credit bdk_nixx_af_tx_expr_credit_t;

static inline uint64_t BDK_NIXX_AF_TX_EXPR_CREDIT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TX_EXPR_CREDIT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000830ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TX_EXPR_CREDIT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TX_EXPR_CREDIT(a) bdk_nixx_af_tx_expr_credit_t
#define bustype_BDK_NIXX_AF_TX_EXPR_CREDIT(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TX_EXPR_CREDIT(a) "NIXX_AF_TX_EXPR_CREDIT"
#define device_bar_BDK_NIXX_AF_TX_EXPR_CREDIT(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TX_EXPR_CREDIT(a) (a)
#define arguments_BDK_NIXX_AF_TX_EXPR_CREDIT(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tx_link#_expr_credit
 *
 * NIX AF Transmit Link Express Credit Registers
 * These registers track credits per link for express packets that may potentially
 * preempt normal packets. Link index enumerated by NIX_LINK_E.
 */
union bdk_nixx_af_tx_linkx_expr_credit
{
    uint64_t u;
    struct bdk_nixx_af_tx_linkx_expr_credit_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_32_63        : 32;
        uint64_t cc_unit_cnt           : 20; /**< [ 31: 12](R/W/H) Link-credit unit count. This value, plus 1 MTU, represents the maximum outstanding
                                                                 credit units for this link. A credit unit is 16 bytes. Note that this
                                                                 20-bit field represents a signed value that decrements towards zero as credits are used.
                                                                 Packets are not allowed to flow when the count is less than zero. As such, the most
                                                                 significant bit should normally be programmed as zero (positive count). This gives a
                                                                 maximum value for this field of 2^19 - 1.

                                                                 In order to prevent blocking between CGX LMACs, [CC_ENABLE] should be set to 1 and
                                                                 [CC_UNIT_CNT] should be less than

                                                                 _     ((LMAC TX buffer size in CGX) - (MTU excluding FCS))/16

                                                                 The LMAC TX buffer size is defined by CGX()_CMR_TX_LMACS[LMACS]. For example, if
                                                                 CGX()_CMR_TX_LMACS[LMACS]=0x4 (16 KB per LMAC) and the LMAC's MTU excluding FCS
                                                                 is 9212 bytes (9216 minus 4 byte FCS), then [CC_UNIT_CNT] should be \< (16384 - 9212)/16 =
                                                                 448.

                                                                 The recommended configuration for LBK is [CC_ENABLE] = 1 and
                                                                 [CC_UNIT_CNT] = (10 * Max_LBK_Data_Rate),
                                                                 e.g. [CC_UNIT_CNT] = 1000 for 100 Gbps max LBK data rate.

                                                                 Internal:
                                                                 LBK value is sized for specified data rate with 1200ns round trip latency,
                                                                 e.g. for 100 Gbps:

                                                                 _ Minimum LBK in-flight data = 100*1200/128b = 938 credit units.

                                                                 Note: maximum LBK in-fligh data = initial_value + MTU. */
        uint64_t cc_packet_cnt         : 10; /**< [ 11:  2](R/W/H) Link-credit packet count. This value, plus 1, represents the maximum outstanding
                                                                 packet count for this link. Note that this 10-bit field represents a signed
                                                                 value that decrements towards zero as credits are used. Packets are not allowed to flow
                                                                 when the count is less than zero. As such the most significant bit should normally be
                                                                 programmed as zero (positive count). This gives a maximum value for this field of 2^9 - 1. */
        uint64_t cc_enable             : 1;  /**< [  1:  1](R/W) Credit enable. Enables [CC_UNIT_CNT] and [CC_PACKET_CNT] link credit processing. */
        uint64_t reserved_0            : 1;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0            : 1;
        uint64_t cc_enable             : 1;  /**< [  1:  1](R/W) Credit enable. Enables [CC_UNIT_CNT] and [CC_PACKET_CNT] link credit processing. */
        uint64_t cc_packet_cnt         : 10; /**< [ 11:  2](R/W/H) Link-credit packet count. This value, plus 1, represents the maximum outstanding
                                                                 packet count for this link. Note that this 10-bit field represents a signed
                                                                 value that decrements towards zero as credits are used. Packets are not allowed to flow
                                                                 when the count is less than zero. As such the most significant bit should normally be
                                                                 programmed as zero (positive count). This gives a maximum value for this field of 2^9 - 1. */
        uint64_t cc_unit_cnt           : 20; /**< [ 31: 12](R/W/H) Link-credit unit count. This value, plus 1 MTU, represents the maximum outstanding
                                                                 credit units for this link. A credit unit is 16 bytes. Note that this
                                                                 20-bit field represents a signed value that decrements towards zero as credits are used.
                                                                 Packets are not allowed to flow when the count is less than zero. As such, the most
                                                                 significant bit should normally be programmed as zero (positive count). This gives a
                                                                 maximum value for this field of 2^19 - 1.

                                                                 In order to prevent blocking between CGX LMACs, [CC_ENABLE] should be set to 1 and
                                                                 [CC_UNIT_CNT] should be less than

                                                                 _     ((LMAC TX buffer size in CGX) - (MTU excluding FCS))/16

                                                                 The LMAC TX buffer size is defined by CGX()_CMR_TX_LMACS[LMACS]. For example, if
                                                                 CGX()_CMR_TX_LMACS[LMACS]=0x4 (16 KB per LMAC) and the LMAC's MTU excluding FCS
                                                                 is 9212 bytes (9216 minus 4 byte FCS), then [CC_UNIT_CNT] should be \< (16384 - 9212)/16 =
                                                                 448.

                                                                 The recommended configuration for LBK is [CC_ENABLE] = 1 and
                                                                 [CC_UNIT_CNT] = (10 * Max_LBK_Data_Rate),
                                                                 e.g. [CC_UNIT_CNT] = 1000 for 100 Gbps max LBK data rate.

                                                                 Internal:
                                                                 LBK value is sized for specified data rate with 1200ns round trip latency,
                                                                 e.g. for 100 Gbps:

                                                                 _ Minimum LBK in-flight data = 100*1200/128b = 938 credit units.

                                                                 Note: maximum LBK in-fligh data = initial_value + MTU. */
        uint64_t reserved_32_63        : 32;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tx_linkx_expr_credit_s cn; */
};
typedef union bdk_nixx_af_tx_linkx_expr_credit bdk_nixx_af_tx_linkx_expr_credit_t;

static inline uint64_t BDK_NIXX_AF_TX_LINKX_EXPR_CREDIT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TX_LINKX_EXPR_CREDIT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=12)))
        return 0x850040000a10ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xf);
    __bdk_csr_fatal("NIXX_AF_TX_LINKX_EXPR_CREDIT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TX_LINKX_EXPR_CREDIT(a,b) bdk_nixx_af_tx_linkx_expr_credit_t
#define bustype_BDK_NIXX_AF_TX_LINKX_EXPR_CREDIT(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TX_LINKX_EXPR_CREDIT(a,b) "NIXX_AF_TX_LINKX_EXPR_CREDIT"
#define device_bar_BDK_NIXX_AF_TX_LINKX_EXPR_CREDIT(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TX_LINKX_EXPR_CREDIT(a,b) (a)
#define arguments_BDK_NIXX_AF_TX_LINKX_EXPR_CREDIT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tx_link#_hw_xoff
 *
 * NIX AF Transmit Link Hardware Controlled XOFF Registers
 * Link index enumerated by NIX_LINK_E.
 */
union bdk_nixx_af_tx_linkx_hw_xoff
{
    uint64_t u;
    struct bdk_nixx_af_tx_linkx_hw_xoff_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t chan_xoff             : 64; /**< [ 63:  0](RO/H) Channel hardware XOFF status. One bit per channel on this link. When a bit is set,
                                                                 indicates that the transmit channel is being backpressured (XOFF) by the link. */
#else /* Word 0 - Little Endian */
        uint64_t chan_xoff             : 64; /**< [ 63:  0](RO/H) Channel hardware XOFF status. One bit per channel on this link. When a bit is set,
                                                                 indicates that the transmit channel is being backpressured (XOFF) by the link. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tx_linkx_hw_xoff_s cn; */
};
typedef union bdk_nixx_af_tx_linkx_hw_xoff bdk_nixx_af_tx_linkx_hw_xoff_t;

static inline uint64_t BDK_NIXX_AF_TX_LINKX_HW_XOFF(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TX_LINKX_HW_XOFF(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=12)))
        return 0x850040000a30ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xf);
    __bdk_csr_fatal("NIXX_AF_TX_LINKX_HW_XOFF", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TX_LINKX_HW_XOFF(a,b) bdk_nixx_af_tx_linkx_hw_xoff_t
#define bustype_BDK_NIXX_AF_TX_LINKX_HW_XOFF(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TX_LINKX_HW_XOFF(a,b) "NIXX_AF_TX_LINKX_HW_XOFF"
#define device_bar_BDK_NIXX_AF_TX_LINKX_HW_XOFF(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TX_LINKX_HW_XOFF(a,b) (a)
#define arguments_BDK_NIXX_AF_TX_LINKX_HW_XOFF(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tx_link#_norm_credit
 *
 * NIX AF Transmit Link Normal Credit Registers
 * These registers track credits per link for normal (potentially preemptable)
 * packets sent to CGX and LBK. Link index enumerated by NIX_LINK_E.
 */
union bdk_nixx_af_tx_linkx_norm_credit
{
    uint64_t u;
    struct bdk_nixx_af_tx_linkx_norm_credit_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_32_63        : 32;
        uint64_t cc_unit_cnt           : 20; /**< [ 31: 12](R/W/H) Link-credit unit count. This value, plus 1 MTU, represents the maximum outstanding
                                                                 credit units for this link. A credit unit is 16 bytes. Note that this
                                                                 20-bit field represents a signed value that decrements towards zero as credits are used.
                                                                 Packets are not allowed to flow when the count is less than zero. As such, the most
                                                                 significant bit should normally be programmed as zero (positive count). This gives a
                                                                 maximum value for this field of 2^19 - 1.

                                                                 In order to prevent blocking between CGX LMACs, [CC_ENABLE] should be set to 1 and
                                                                 [CC_UNIT_CNT] should be less than

                                                                 _     ((LMAC TX buffer size in CGX) - (MTU excluding FCS))/16

                                                                 The LMAC TX buffer size is defined by CGX()_CMR_TX_LMACS[LMACS]. For example, if
                                                                 CGX()_CMR_TX_LMACS[LMACS]=0x4 (16 KB per LMAC) and the LMAC's MTU excluding FCS
                                                                 is 9212 bytes (9216 minus 4 byte FCS), then [CC_UNIT_CNT] should be \< (16384 - 9212)/16 =
                                                                 448.

                                                                 The recommended configuration for LBK is [CC_ENABLE] = 1 and
                                                                 [CC_UNIT_CNT] = (10 * Max_LBK_Data_Rate),
                                                                 e.g. [CC_UNIT_CNT] = 1000 for 100 Gbps max LBK data rate.

                                                                 Internal:
                                                                 LBK value is sized for specified data rate with 1200ns round trip latency,
                                                                 e.g. for 100 Gbps:

                                                                 _ Minimum LBK in-flight data = 100*1200/128b = 938 credit units.

                                                                 Note: maximum LBK in-fligh data = initial_value + MTU. */
        uint64_t cc_packet_cnt         : 10; /**< [ 11:  2](R/W/H) Link-credit packet count. This value, plus 1, represents the maximum outstanding
                                                                 packet count for this link. Note that this 10-bit field represents a signed
                                                                 value that decrements towards zero as credits are used. Packets are not allowed to flow
                                                                 when the count is less than zero. As such the most significant bit should normally be
                                                                 programmed as zero (positive count). This gives a maximum value for this field of 2^9 - 1. */
        uint64_t cc_enable             : 1;  /**< [  1:  1](R/W) Credit enable. Enables [CC_UNIT_CNT] and [CC_PACKET_CNT] link credit processing. */
        uint64_t reserved_0            : 1;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0            : 1;
        uint64_t cc_enable             : 1;  /**< [  1:  1](R/W) Credit enable. Enables [CC_UNIT_CNT] and [CC_PACKET_CNT] link credit processing. */
        uint64_t cc_packet_cnt         : 10; /**< [ 11:  2](R/W/H) Link-credit packet count. This value, plus 1, represents the maximum outstanding
                                                                 packet count for this link. Note that this 10-bit field represents a signed
                                                                 value that decrements towards zero as credits are used. Packets are not allowed to flow
                                                                 when the count is less than zero. As such the most significant bit should normally be
                                                                 programmed as zero (positive count). This gives a maximum value for this field of 2^9 - 1. */
        uint64_t cc_unit_cnt           : 20; /**< [ 31: 12](R/W/H) Link-credit unit count. This value, plus 1 MTU, represents the maximum outstanding
                                                                 credit units for this link. A credit unit is 16 bytes. Note that this
                                                                 20-bit field represents a signed value that decrements towards zero as credits are used.
                                                                 Packets are not allowed to flow when the count is less than zero. As such, the most
                                                                 significant bit should normally be programmed as zero (positive count). This gives a
                                                                 maximum value for this field of 2^19 - 1.

                                                                 In order to prevent blocking between CGX LMACs, [CC_ENABLE] should be set to 1 and
                                                                 [CC_UNIT_CNT] should be less than

                                                                 _     ((LMAC TX buffer size in CGX) - (MTU excluding FCS))/16

                                                                 The LMAC TX buffer size is defined by CGX()_CMR_TX_LMACS[LMACS]. For example, if
                                                                 CGX()_CMR_TX_LMACS[LMACS]=0x4 (16 KB per LMAC) and the LMAC's MTU excluding FCS
                                                                 is 9212 bytes (9216 minus 4 byte FCS), then [CC_UNIT_CNT] should be \< (16384 - 9212)/16 =
                                                                 448.

                                                                 The recommended configuration for LBK is [CC_ENABLE] = 1 and
                                                                 [CC_UNIT_CNT] = (10 * Max_LBK_Data_Rate),
                                                                 e.g. [CC_UNIT_CNT] = 1000 for 100 Gbps max LBK data rate.

                                                                 Internal:
                                                                 LBK value is sized for specified data rate with 1200ns round trip latency,
                                                                 e.g. for 100 Gbps:

                                                                 _ Minimum LBK in-flight data = 100*1200/128b = 938 credit units.

                                                                 Note: maximum LBK in-fligh data = initial_value + MTU. */
        uint64_t reserved_32_63        : 32;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tx_linkx_norm_credit_s cn; */
};
typedef union bdk_nixx_af_tx_linkx_norm_credit bdk_nixx_af_tx_linkx_norm_credit_t;

static inline uint64_t BDK_NIXX_AF_TX_LINKX_NORM_CREDIT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TX_LINKX_NORM_CREDIT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=12)))
        return 0x850040000a00ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xf);
    __bdk_csr_fatal("NIXX_AF_TX_LINKX_NORM_CREDIT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TX_LINKX_NORM_CREDIT(a,b) bdk_nixx_af_tx_linkx_norm_credit_t
#define bustype_BDK_NIXX_AF_TX_LINKX_NORM_CREDIT(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TX_LINKX_NORM_CREDIT(a,b) "NIXX_AF_TX_LINKX_NORM_CREDIT"
#define device_bar_BDK_NIXX_AF_TX_LINKX_NORM_CREDIT(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TX_LINKX_NORM_CREDIT(a,b) (a)
#define arguments_BDK_NIXX_AF_TX_LINKX_NORM_CREDIT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tx_link#_sw_xoff
 *
 * NIX AF Transmit Link Software Controlled XOFF Registers
 * Link index enumerated by NIX_LINK_E.
 */
union bdk_nixx_af_tx_linkx_sw_xoff
{
    uint64_t u;
    struct bdk_nixx_af_tx_linkx_sw_xoff_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t chan_xoff             : 64; /**< [ 63:  0](R/W) Channel software controlled XOFF. One bit per channel on this link. When a bit is set,
                                                                 packets will not be transmitted on the associated channel. */
#else /* Word 0 - Little Endian */
        uint64_t chan_xoff             : 64; /**< [ 63:  0](R/W) Channel software controlled XOFF. One bit per channel on this link. When a bit is set,
                                                                 packets will not be transmitted on the associated channel. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tx_linkx_sw_xoff_s cn; */
};
typedef union bdk_nixx_af_tx_linkx_sw_xoff bdk_nixx_af_tx_linkx_sw_xoff_t;

static inline uint64_t BDK_NIXX_AF_TX_LINKX_SW_XOFF(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TX_LINKX_SW_XOFF(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=12)))
        return 0x850040000a20ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0xf);
    __bdk_csr_fatal("NIXX_AF_TX_LINKX_SW_XOFF", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TX_LINKX_SW_XOFF(a,b) bdk_nixx_af_tx_linkx_sw_xoff_t
#define bustype_BDK_NIXX_AF_TX_LINKX_SW_XOFF(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TX_LINKX_SW_XOFF(a,b) "NIXX_AF_TX_LINKX_SW_XOFF"
#define device_bar_BDK_NIXX_AF_TX_LINKX_SW_XOFF(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TX_LINKX_SW_XOFF(a,b) (a)
#define arguments_BDK_NIXX_AF_TX_LINKX_SW_XOFF(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tx_mcast#
 *
 * NIX AF Transmit Multicast Registers
 * These registers access transmit multicast table entries used to specify multicast replication
 * lists. Each list consists of linked entries with [EOL] = 1 in the last entry.
 *
 * A transmit packet is multicast when the action returned by NPC has NIX_TX_ACTION_S[OP] =
 * NIX_TX_ACTIONOP_E::MCAST. NIX_TX_ACTION_S[INDEX] points to the start of the multicast
 * replication list, and [EOL] = 1 indicates the end of list.
 */
union bdk_nixx_af_tx_mcastx
{
    uint64_t u;
    struct bdk_nixx_af_tx_mcastx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_32_63        : 32;
        uint64_t next                  : 16; /**< [ 31: 16](R/W) Pointer to next NIX_AF_TX_MCAST() register in the multicast replication
                                                                 list. Valid when [EOL] is clear. */
        uint64_t reserved_13_15        : 3;
        uint64_t eol                   : 1;  /**< [ 12: 12](R/W) End of multicast replication list. */
        uint64_t channel               : 12; /**< [ 11:  0](R/W) Transmit channel ID enumerated by NIX_CHAN_E. */
#else /* Word 0 - Little Endian */
        uint64_t channel               : 12; /**< [ 11:  0](R/W) Transmit channel ID enumerated by NIX_CHAN_E. */
        uint64_t eol                   : 1;  /**< [ 12: 12](R/W) End of multicast replication list. */
        uint64_t reserved_13_15        : 3;
        uint64_t next                  : 16; /**< [ 31: 16](R/W) Pointer to next NIX_AF_TX_MCAST() register in the multicast replication
                                                                 list. Valid when [EOL] is clear. */
        uint64_t reserved_32_63        : 32;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tx_mcastx_s cn; */
};
typedef union bdk_nixx_af_tx_mcastx bdk_nixx_af_tx_mcastx_t;

static inline uint64_t BDK_NIXX_AF_TX_MCASTX(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TX_MCASTX(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=2047)))
        return 0x850040001900ll + 0x10000000ll * ((a) & 0x0) + 0x8000ll * ((b) & 0x7ff);
    __bdk_csr_fatal("NIXX_AF_TX_MCASTX", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TX_MCASTX(a,b) bdk_nixx_af_tx_mcastx_t
#define bustype_BDK_NIXX_AF_TX_MCASTX(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TX_MCASTX(a,b) "NIXX_AF_TX_MCASTX"
#define device_bar_BDK_NIXX_AF_TX_MCASTX(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TX_MCASTX(a,b) (a)
#define arguments_BDK_NIXX_AF_TX_MCASTX(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tx_norm_credit
 *
 * NIX AF Transmit Normal Credit Register
 * This register tracks internal credits for normal (potentially preemptable)
 * packets sent to CGX and LBK.
 */
union bdk_nixx_af_tx_norm_credit
{
    uint64_t u;
    struct bdk_nixx_af_tx_norm_credit_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_32_63        : 32;
        uint64_t cc_unit_cnt           : 20; /**< [ 31: 12](R/W) Credit unit count. This value, plus 1 MTU, represents the maximum
                                                                 outstanding aggregate credit units for all packets associated with this
                                                                 register (normal or express packets to CGX and LBK). A credit unit is 16
                                                                 bytes. Note that this 20-bit field represents a signed value that
                                                                 decrements towards zero as credits are used. Packets are not allowed to
                                                                 flow when the count is less than zero. As such, the most significant bit
                                                                 should normally be programmed as zero (positive count). This gives a
                                                                 maximum value for this field of 2^19 - 1.

                                                                 The recommended value is
                                                                 [CC_UNIT_CNT] = (10 * Max_aggregate_CGX_plus_LBK_Data_Rate),
                                                                 e.g. [CC_UNIT_CNT] = 1000 for 100 Gbps data rate.

                                                                 Internal:
                                                                 Recommended value is sized for specified data rate with 1200ns round trip
                                                                 latency. See LBK example in NIX_AF_TX_LINK()_NORM_CREDIT[CC_UNIT_CNT]. */
        uint64_t cc_packet_cnt         : 10; /**< [ 11:  2](R/W) Credit packet count. This value, plus 1, represents the maximum outstanding
                                                                 aggregate packet count for packets associated with this register (normal or
                                                                 express packets to CGX and LBK).
                                                                 Note that this 10-bit field represents a signed value that decrements
                                                                 towards zero as credits are used. Packets are not allowed to flow when the
                                                                 count is less than zero. As such the most significant bit should be
                                                                 programmed as zero (positive count).

                                                                 The recommended value is 255. Must satisfy:
                                                                 _  (NIX_AF_TX_NORM_CREDIT[CC_PACKET_COUNT] +
                                                                 NIX_AF_TX_EXPR_CREDIT[CC_PACKET_COUNT]) \< 511

                                                                 Internal:
                                                                 Limited by the 512 non-SDP PSE packet IDs. */
        uint64_t reserved_0_1          : 2;
#else /* Word 0 - Little Endian */
        uint64_t reserved_0_1          : 2;
        uint64_t cc_packet_cnt         : 10; /**< [ 11:  2](R/W) Credit packet count. This value, plus 1, represents the maximum outstanding
                                                                 aggregate packet count for packets associated with this register (normal or
                                                                 express packets to CGX and LBK).
                                                                 Note that this 10-bit field represents a signed value that decrements
                                                                 towards zero as credits are used. Packets are not allowed to flow when the
                                                                 count is less than zero. As such the most significant bit should be
                                                                 programmed as zero (positive count).

                                                                 The recommended value is 255. Must satisfy:
                                                                 _  (NIX_AF_TX_NORM_CREDIT[CC_PACKET_COUNT] +
                                                                 NIX_AF_TX_EXPR_CREDIT[CC_PACKET_COUNT]) \< 511

                                                                 Internal:
                                                                 Limited by the 512 non-SDP PSE packet IDs. */
        uint64_t cc_unit_cnt           : 20; /**< [ 31: 12](R/W) Credit unit count. This value, plus 1 MTU, represents the maximum
                                                                 outstanding aggregate credit units for all packets associated with this
                                                                 register (normal or express packets to CGX and LBK). A credit unit is 16
                                                                 bytes. Note that this 20-bit field represents a signed value that
                                                                 decrements towards zero as credits are used. Packets are not allowed to
                                                                 flow when the count is less than zero. As such, the most significant bit
                                                                 should normally be programmed as zero (positive count). This gives a
                                                                 maximum value for this field of 2^19 - 1.

                                                                 The recommended value is
                                                                 [CC_UNIT_CNT] = (10 * Max_aggregate_CGX_plus_LBK_Data_Rate),
                                                                 e.g. [CC_UNIT_CNT] = 1000 for 100 Gbps data rate.

                                                                 Internal:
                                                                 Recommended value is sized for specified data rate with 1200ns round trip
                                                                 latency. See LBK example in NIX_AF_TX_LINK()_NORM_CREDIT[CC_UNIT_CNT]. */
        uint64_t reserved_32_63        : 32;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tx_norm_credit_s cn; */
};
typedef union bdk_nixx_af_tx_norm_credit bdk_nixx_af_tx_norm_credit_t;

static inline uint64_t BDK_NIXX_AF_TX_NORM_CREDIT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TX_NORM_CREDIT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850040000820ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TX_NORM_CREDIT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TX_NORM_CREDIT(a) bdk_nixx_af_tx_norm_credit_t
#define bustype_BDK_NIXX_AF_TX_NORM_CREDIT(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TX_NORM_CREDIT(a) "NIXX_AF_TX_NORM_CREDIT"
#define device_bar_BDK_NIXX_AF_TX_NORM_CREDIT(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TX_NORM_CREDIT(a) (a)
#define arguments_BDK_NIXX_AF_TX_NORM_CREDIT(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tx_tstmp_cfg
 *
 * NIX AF Transmit Timestamp Configuration Register
 */
union bdk_nixx_af_tx_tstmp_cfg
{
    uint64_t u;
    struct bdk_nixx_af_tx_tstmp_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_4_63         : 60;
        uint64_t tstmp_wd_period       : 4;  /**< [  3:  0](R/W) Watchdog timeout count for send timestamp capture. See
                                                                 NIX_SEND_EXT_S[TSTMP].

                                                                 The timeout period is 4*(2^[TSTMP_WD_PERIOD]) timer ticks, where each tick
                                                                 is 128 cycles of the 100 MHz reference clock: 0 = 4 ticks, 1 = 8 tick, ...
                                                                 15 = 131072 ticks. */
#else /* Word 0 - Little Endian */
        uint64_t tstmp_wd_period       : 4;  /**< [  3:  0](R/W) Watchdog timeout count for send timestamp capture. See
                                                                 NIX_SEND_EXT_S[TSTMP].

                                                                 The timeout period is 4*(2^[TSTMP_WD_PERIOD]) timer ticks, where each tick
                                                                 is 128 cycles of the 100 MHz reference clock: 0 = 4 ticks, 1 = 8 tick, ...
                                                                 15 = 131072 ticks. */
        uint64_t reserved_4_63         : 60;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tx_tstmp_cfg_s cn; */
};
typedef union bdk_nixx_af_tx_tstmp_cfg bdk_nixx_af_tx_tstmp_cfg_t;

static inline uint64_t BDK_NIXX_AF_TX_TSTMP_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TX_TSTMP_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x8500400000c0ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_AF_TX_TSTMP_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_AF_TX_TSTMP_CFG(a) bdk_nixx_af_tx_tstmp_cfg_t
#define bustype_BDK_NIXX_AF_TX_TSTMP_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TX_TSTMP_CFG(a) "NIXX_AF_TX_TSTMP_CFG"
#define device_bar_BDK_NIXX_AF_TX_TSTMP_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TX_TSTMP_CFG(a) (a)
#define arguments_BDK_NIXX_AF_TX_TSTMP_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tx_vtag_def#_ctl
 *
 * NIX AF Transmit Vtag Definition Control Registers
 * The transmit Vtag definition table specifies Vtag layers (e.g. VLAN, E-TAG) to
 * optionally insert or replace in the TX packet header. Indexed by
 * NIX_TX_VTAG_ACTION_S[VTAG*_DEF].
 */
union bdk_nixx_af_tx_vtag_defx_ctl
{
    uint64_t u;
    struct bdk_nixx_af_tx_vtag_defx_ctl_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_3_63         : 61;
        uint64_t size                  : 3;  /**< [  2:  0](R/W) Vtag size enumerated by NIX_VTAGSIZE_E. */
#else /* Word 0 - Little Endian */
        uint64_t size                  : 3;  /**< [  2:  0](R/W) Vtag size enumerated by NIX_VTAGSIZE_E. */
        uint64_t reserved_3_63         : 61;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tx_vtag_defx_ctl_s cn; */
};
typedef union bdk_nixx_af_tx_vtag_defx_ctl bdk_nixx_af_tx_vtag_defx_ctl_t;

static inline uint64_t BDK_NIXX_AF_TX_VTAG_DEFX_CTL(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TX_VTAG_DEFX_CTL(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=1023)))
        return 0x850040001a00ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x3ff);
    __bdk_csr_fatal("NIXX_AF_TX_VTAG_DEFX_CTL", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TX_VTAG_DEFX_CTL(a,b) bdk_nixx_af_tx_vtag_defx_ctl_t
#define bustype_BDK_NIXX_AF_TX_VTAG_DEFX_CTL(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TX_VTAG_DEFX_CTL(a,b) "NIXX_AF_TX_VTAG_DEFX_CTL"
#define device_bar_BDK_NIXX_AF_TX_VTAG_DEFX_CTL(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TX_VTAG_DEFX_CTL(a,b) (a)
#define arguments_BDK_NIXX_AF_TX_VTAG_DEFX_CTL(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_af_tx_vtag_def#_data
 *
 * NIX AF Transmit Vtag Definition Data Registers
 * See NIX_AF_TX_VTAG_DEF()_CTL.
 */
union bdk_nixx_af_tx_vtag_defx_data
{
    uint64_t u;
    struct bdk_nixx_af_tx_vtag_defx_data_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t data                  : 64; /**< [ 63:  0](R/W) Vtag data formatted as follows:
                                                                 * If NIX_AF_TX_VTAG_DEF()_CTL[SIZE] = NIX_VTAGSIZE_E::T4: \<31:16\> Ethertype, \<15:0\> TCI.
                                                                 * If NIX_AF_TX_VTAG_DEF()_CTL[SIZE] = NIX_VTAGSIZE_E::T8: \<63:48\> Ethertype, \<47:0\> TCI. */
#else /* Word 0 - Little Endian */
        uint64_t data                  : 64; /**< [ 63:  0](R/W) Vtag data formatted as follows:
                                                                 * If NIX_AF_TX_VTAG_DEF()_CTL[SIZE] = NIX_VTAGSIZE_E::T4: \<31:16\> Ethertype, \<15:0\> TCI.
                                                                 * If NIX_AF_TX_VTAG_DEF()_CTL[SIZE] = NIX_VTAGSIZE_E::T8: \<63:48\> Ethertype, \<47:0\> TCI. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_af_tx_vtag_defx_data_s cn; */
};
typedef union bdk_nixx_af_tx_vtag_defx_data bdk_nixx_af_tx_vtag_defx_data_t;

static inline uint64_t BDK_NIXX_AF_TX_VTAG_DEFX_DATA(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_AF_TX_VTAG_DEFX_DATA(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=1023)))
        return 0x850040001a10ll + 0x10000000ll * ((a) & 0x0) + 0x10000ll * ((b) & 0x3ff);
    __bdk_csr_fatal("NIXX_AF_TX_VTAG_DEFX_DATA", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_AF_TX_VTAG_DEFX_DATA(a,b) bdk_nixx_af_tx_vtag_defx_data_t
#define bustype_BDK_NIXX_AF_TX_VTAG_DEFX_DATA(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_AF_TX_VTAG_DEFX_DATA(a,b) "NIXX_AF_TX_VTAG_DEFX_DATA"
#define device_bar_BDK_NIXX_AF_TX_VTAG_DEFX_DATA(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_AF_TX_VTAG_DEFX_DATA(a,b) (a)
#define arguments_BDK_NIXX_AF_TX_VTAG_DEFX_DATA(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_cfg
 *
 * NIX LF Configuration Register
 */
union bdk_nixx_lf_cfg
{
    uint64_t u;
    struct bdk_nixx_lf_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t tcp_timer_int_ena     : 1;  /**< [  0:  0](R/W) TCP timer interrupt enable. When set along with NIX_AF_TCP_TIMER[ENA],
                                                                 NIX_LF_GINT[TCP_TIMER] is set every
                                                                 NIX_AF_TCP_TIMER[DURATION]*NIX_AF_CONST2[LFS]*100 nanoseconds. */
#else /* Word 0 - Little Endian */
        uint64_t tcp_timer_int_ena     : 1;  /**< [  0:  0](R/W) TCP timer interrupt enable. When set along with NIX_AF_TCP_TIMER[ENA],
                                                                 NIX_LF_GINT[TCP_TIMER] is set every
                                                                 NIX_AF_TCP_TIMER[DURATION]*NIX_AF_CONST2[LFS]*100 nanoseconds. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_cfg_s cn; */
};
typedef union bdk_nixx_lf_cfg bdk_nixx_lf_cfg_t;

static inline uint64_t BDK_NIXX_LF_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400100ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_CFG(a) bdk_nixx_lf_cfg_t
#define bustype_BDK_NIXX_LF_CFG(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_CFG(a) "NIXX_LF_CFG"
#define device_bar_BDK_NIXX_LF_CFG(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_CFG(a) (a)
#define arguments_BDK_NIXX_LF_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_cint#_cnt
 *
 * NIX LF Completion Interrupt Count Registers
 */
union bdk_nixx_lf_cintx_cnt
{
    uint64_t u;
    struct bdk_nixx_lf_cintx_cnt_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t qcount                : 16; /**< [ 47: 32](R/W/H) Active queue count. Number of CQs feeding this CINT that a not empty.
                                                                 Writes to this field are for diagnostic use only. */
        uint64_t ecount                : 32; /**< [ 31:  0](R/W/H) Entry count. Number of posted CQEs across all CQs feeding this CINT.
                                                                 Hardware increments the counter when it adds a CQE and subtracts the
                                                                 number of dequeued CQEs when software writes to
                                                                 NIX_LF_CQ_OP_DOOR.
                                                                 Writes to this field are for diagnostic use only. */
#else /* Word 0 - Little Endian */
        uint64_t ecount                : 32; /**< [ 31:  0](R/W/H) Entry count. Number of posted CQEs across all CQs feeding this CINT.
                                                                 Hardware increments the counter when it adds a CQE and subtracts the
                                                                 number of dequeued CQEs when software writes to
                                                                 NIX_LF_CQ_OP_DOOR.
                                                                 Writes to this field are for diagnostic use only. */
        uint64_t qcount                : 16; /**< [ 47: 32](R/W/H) Active queue count. Number of CQs feeding this CINT that a not empty.
                                                                 Writes to this field are for diagnostic use only. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_cintx_cnt_s cn; */
};
typedef union bdk_nixx_lf_cintx_cnt bdk_nixx_lf_cintx_cnt_t;

static inline uint64_t BDK_NIXX_LF_CINTX_CNT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_CINTX_CNT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400d00ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_CINTX_CNT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_CINTX_CNT(a,b) bdk_nixx_lf_cintx_cnt_t
#define bustype_BDK_NIXX_LF_CINTX_CNT(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_CINTX_CNT(a,b) "NIXX_LF_CINTX_CNT"
#define device_bar_BDK_NIXX_LF_CINTX_CNT(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_CINTX_CNT(a,b) (a)
#define arguments_BDK_NIXX_LF_CINTX_CNT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_cint#_ena_w1c
 *
 * NIX LF Completion Interrupt Enable Clear Registers
 * This register clears interrupt enable bits.
 */
union bdk_nixx_lf_cintx_ena_w1c
{
    uint64_t u;
    struct bdk_nixx_lf_cintx_ena_w1c_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_LF_CINT(0..63)_INT[INTR]. */
#else /* Word 0 - Little Endian */
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_LF_CINT(0..63)_INT[INTR]. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_cintx_ena_w1c_s cn; */
};
typedef union bdk_nixx_lf_cintx_ena_w1c bdk_nixx_lf_cintx_ena_w1c_t;

static inline uint64_t BDK_NIXX_LF_CINTX_ENA_W1C(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_CINTX_ENA_W1C(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400d50ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_CINTX_ENA_W1C", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_CINTX_ENA_W1C(a,b) bdk_nixx_lf_cintx_ena_w1c_t
#define bustype_BDK_NIXX_LF_CINTX_ENA_W1C(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_CINTX_ENA_W1C(a,b) "NIXX_LF_CINTX_ENA_W1C"
#define device_bar_BDK_NIXX_LF_CINTX_ENA_W1C(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_CINTX_ENA_W1C(a,b) (a)
#define arguments_BDK_NIXX_LF_CINTX_ENA_W1C(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_cint#_ena_w1s
 *
 * NIX LF Completion Interrupt Enable Set Registers
 * This register sets interrupt enable bits.
 */
union bdk_nixx_lf_cintx_ena_w1s
{
    uint64_t u;
    struct bdk_nixx_lf_cintx_ena_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_LF_CINT(0..63)_INT[INTR]. */
#else /* Word 0 - Little Endian */
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_LF_CINT(0..63)_INT[INTR]. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_cintx_ena_w1s_s cn; */
};
typedef union bdk_nixx_lf_cintx_ena_w1s bdk_nixx_lf_cintx_ena_w1s_t;

static inline uint64_t BDK_NIXX_LF_CINTX_ENA_W1S(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_CINTX_ENA_W1S(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400d40ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_CINTX_ENA_W1S", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_CINTX_ENA_W1S(a,b) bdk_nixx_lf_cintx_ena_w1s_t
#define bustype_BDK_NIXX_LF_CINTX_ENA_W1S(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_CINTX_ENA_W1S(a,b) "NIXX_LF_CINTX_ENA_W1S"
#define device_bar_BDK_NIXX_LF_CINTX_ENA_W1S(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_CINTX_ENA_W1S(a,b) (a)
#define arguments_BDK_NIXX_LF_CINTX_ENA_W1S(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_cint#_int
 *
 * NIX LF Completion Interrupt Registers
 */
union bdk_nixx_lf_cintx_int
{
    uint64_t u;
    struct bdk_nixx_lf_cintx_int_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1C/H) Interrupt status. Hardware sets this bit when any of the following occurs:
                                                                 * ECOUNT hold-off value is reached:
                                                                 NIX_LF_CINT()_CNT[ECOUNT] = NIX_LF_CINT()_WAIT[ECOUNT_WAIT]
                                                                 after hardware adds a CQE.
                                                                 * QCOUNT hold-off value is reached:
                                                                 NIX_LF_CINT()_CNT[QCOUNT] = NIX_LF_CINT()_WAIT[QCOUNT_WAIT]
                                                                 after hardware adds a CQE.
                                                                 * Associated CINT timer expires, or the timer is removed/deactivated
                                                                 when NIX_LF_CINT()_CNT[ECOUNT] is nonzero. See
                                                                 NIX_AF_CINT_TIMER()[ACTIVE], NIX_AF_CINT_TIMER()[EXPIR_TIME].
                                                                 * Software writes a one to clear this bit, and
                                                                 NIX_LF_CINT()_CNT[ECOUNT] \> NIX_LF_CINT()_WAIT[ECOUNT_WAIT] or
                                                                 NIX_LF_CINT()_CNT[QCOUNT] \> NIX_LF_CINT()_WAIT[QCOUNT_WAIT]. */
#else /* Word 0 - Little Endian */
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1C/H) Interrupt status. Hardware sets this bit when any of the following occurs:
                                                                 * ECOUNT hold-off value is reached:
                                                                 NIX_LF_CINT()_CNT[ECOUNT] = NIX_LF_CINT()_WAIT[ECOUNT_WAIT]
                                                                 after hardware adds a CQE.
                                                                 * QCOUNT hold-off value is reached:
                                                                 NIX_LF_CINT()_CNT[QCOUNT] = NIX_LF_CINT()_WAIT[QCOUNT_WAIT]
                                                                 after hardware adds a CQE.
                                                                 * Associated CINT timer expires, or the timer is removed/deactivated
                                                                 when NIX_LF_CINT()_CNT[ECOUNT] is nonzero. See
                                                                 NIX_AF_CINT_TIMER()[ACTIVE], NIX_AF_CINT_TIMER()[EXPIR_TIME].
                                                                 * Software writes a one to clear this bit, and
                                                                 NIX_LF_CINT()_CNT[ECOUNT] \> NIX_LF_CINT()_WAIT[ECOUNT_WAIT] or
                                                                 NIX_LF_CINT()_CNT[QCOUNT] \> NIX_LF_CINT()_WAIT[QCOUNT_WAIT]. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_cintx_int_s cn; */
};
typedef union bdk_nixx_lf_cintx_int bdk_nixx_lf_cintx_int_t;

static inline uint64_t BDK_NIXX_LF_CINTX_INT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_CINTX_INT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400d20ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_CINTX_INT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_CINTX_INT(a,b) bdk_nixx_lf_cintx_int_t
#define bustype_BDK_NIXX_LF_CINTX_INT(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_CINTX_INT(a,b) "NIXX_LF_CINTX_INT"
#define device_bar_BDK_NIXX_LF_CINTX_INT(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_CINTX_INT(a,b) (a)
#define arguments_BDK_NIXX_LF_CINTX_INT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_cint#_int_w1s
 *
 * NIX LF Completion Interrupt Set Registers
 * This register sets interrupt bits.
 */
union bdk_nixx_lf_cintx_int_w1s
{
    uint64_t u;
    struct bdk_nixx_lf_cintx_int_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_LF_CINT(0..63)_INT[INTR]. */
#else /* Word 0 - Little Endian */
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_LF_CINT(0..63)_INT[INTR]. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_cintx_int_w1s_s cn; */
};
typedef union bdk_nixx_lf_cintx_int_w1s bdk_nixx_lf_cintx_int_w1s_t;

static inline uint64_t BDK_NIXX_LF_CINTX_INT_W1S(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_CINTX_INT_W1S(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400d30ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_CINTX_INT_W1S", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_CINTX_INT_W1S(a,b) bdk_nixx_lf_cintx_int_w1s_t
#define bustype_BDK_NIXX_LF_CINTX_INT_W1S(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_CINTX_INT_W1S(a,b) "NIXX_LF_CINTX_INT_W1S"
#define device_bar_BDK_NIXX_LF_CINTX_INT_W1S(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_CINTX_INT_W1S(a,b) (a)
#define arguments_BDK_NIXX_LF_CINTX_INT_W1S(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_cint#_wait
 *
 * NIX LF Completion Interrupt Count Registers
 */
union bdk_nixx_lf_cintx_wait
{
    uint64_t u;
    struct bdk_nixx_lf_cintx_wait_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_56_63        : 8;
        uint64_t time_wait             : 8;  /**< [ 55: 48](R/W/H) Time hold-off as a multiple of
                                                                 (NIX_AF_CINT_DELAY[CINT_DLY]+1)*100 nanoseconds. Time to wait
                                                                 when NIX_LF_CINT()_CNT[ECOUNT] is nonzero before setting
                                                                 NIX_LF_CINT()_W1C[INTR]. */
        uint64_t qcount_wait           : 16; /**< [ 47: 32](R/W/H) Active queue count hold-off. See NIX_LF_CINT()_INT[INTR]. */
        uint64_t ecount_wait           : 32; /**< [ 31:  0](R/W/H) Entry count hold-off. See NIX_LF_CINT()_INT[INTR]. */
#else /* Word 0 - Little Endian */
        uint64_t ecount_wait           : 32; /**< [ 31:  0](R/W/H) Entry count hold-off. See NIX_LF_CINT()_INT[INTR]. */
        uint64_t qcount_wait           : 16; /**< [ 47: 32](R/W/H) Active queue count hold-off. See NIX_LF_CINT()_INT[INTR]. */
        uint64_t time_wait             : 8;  /**< [ 55: 48](R/W/H) Time hold-off as a multiple of
                                                                 (NIX_AF_CINT_DELAY[CINT_DLY]+1)*100 nanoseconds. Time to wait
                                                                 when NIX_LF_CINT()_CNT[ECOUNT] is nonzero before setting
                                                                 NIX_LF_CINT()_W1C[INTR]. */
        uint64_t reserved_56_63        : 8;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_cintx_wait_s cn; */
};
typedef union bdk_nixx_lf_cintx_wait bdk_nixx_lf_cintx_wait_t;

static inline uint64_t BDK_NIXX_LF_CINTX_WAIT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_CINTX_WAIT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400d10ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_CINTX_WAIT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_CINTX_WAIT(a,b) bdk_nixx_lf_cintx_wait_t
#define bustype_BDK_NIXX_LF_CINTX_WAIT(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_CINTX_WAIT(a,b) "NIXX_LF_CINTX_WAIT"
#define device_bar_BDK_NIXX_LF_CINTX_WAIT(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_CINTX_WAIT(a,b) (a)
#define arguments_BDK_NIXX_LF_CINTX_WAIT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_cq_op_door
 *
 * NIX LF CQ Doorbell Operation Register
 * A write to this register dequeues CQEs from a CQ ring within the LF.
 */
union bdk_nixx_lf_cq_op_door
{
    uint64_t u;
    struct bdk_nixx_lf_cq_op_door_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_52_63        : 12;
        uint64_t cq                    : 20; /**< [ 51: 32](WO) CQ within the VF. */
        uint64_t reserved_16_31        : 16;
        uint64_t count                 : 16; /**< [ 15:  0](WO) Number of dequeued CQEs. Hardware advances NIX_CQ_CTX_S[HEAD] by this value
                                                                 if the CQ is enabled. The size of each dequeued CQE is selected by
                                                                 NIX_AF_LF()_CFG[XQE_SIZE].

                                                                 A doorbell write that would underflow the ring is suppressed and sets
                                                                 NIX_CQ_CTX_S[CQ_ERR_INT]\<NIX_CQERRINT_E::DOOR_ERR\>. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 16; /**< [ 15:  0](WO) Number of dequeued CQEs. Hardware advances NIX_CQ_CTX_S[HEAD] by this value
                                                                 if the CQ is enabled. The size of each dequeued CQE is selected by
                                                                 NIX_AF_LF()_CFG[XQE_SIZE].

                                                                 A doorbell write that would underflow the ring is suppressed and sets
                                                                 NIX_CQ_CTX_S[CQ_ERR_INT]\<NIX_CQERRINT_E::DOOR_ERR\>. */
        uint64_t reserved_16_31        : 16;
        uint64_t cq                    : 20; /**< [ 51: 32](WO) CQ within the VF. */
        uint64_t reserved_52_63        : 12;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_cq_op_door_s cn; */
};
typedef union bdk_nixx_lf_cq_op_door bdk_nixx_lf_cq_op_door_t;

static inline uint64_t BDK_NIXX_LF_CQ_OP_DOOR(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_CQ_OP_DOOR(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400b30ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_CQ_OP_DOOR", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_CQ_OP_DOOR(a) bdk_nixx_lf_cq_op_door_t
#define bustype_BDK_NIXX_LF_CQ_OP_DOOR(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_CQ_OP_DOOR(a) "NIXX_LF_CQ_OP_DOOR"
#define device_bar_BDK_NIXX_LF_CQ_OP_DOOR(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_CQ_OP_DOOR(a) (a)
#define arguments_BDK_NIXX_LF_CQ_OP_DOOR(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_cq_op_int
 *
 * NIX LF Completion Queue Interrupt Operation Register
 * A 64-bit atomic load-and-add to this register reads CQ interrupts and
 * interrupt enables.
 * A write optionally sets or clears interrupts and interrupt enables.
 */
union bdk_nixx_lf_cq_op_int
{
    uint64_t u;
    struct bdk_nixx_lf_cq_op_int_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t cq                    : 20; /**< [ 63: 44](WO) CQ within LF. This field is present on a write, or in the write data
                                                                 of an atomic load-and-add. */
        uint64_t setop                 : 1;  /**< [ 43: 43](WO) Set operation. Valid on a write. Indicates write-one-to-set when set,
                                                                 write-one-to-clear when clear. */
        uint64_t op_err                : 1;  /**< [ 42: 42](RO/H) Operation error. Remaining read data fields are not valid. One of the
                                                                 following (may not be exhaustive):
                                                                 * Memory fault on NIX_CQ_CTX_S read; also sets NIX_LF_ERR_INT[CQ_CTX_FAULT].
                                                                 * Poisoned data returned on NIX_CQ_CTX_S read; also sets
                                                                 NIX_LF_RAS[CQ_CTX_POISON] and NIX_AF_RAS[CQ_CTX_POISON] .
                                                                 * Access to out-of-range CQ ([CQ] \> NIX_AF_LF()_CQS_CFG[MAX_QUEUESM1]);
                                                                 also sets NIX_LF_ERR_INT[CQ_OOR].
                                                                 * Disabled CQ (NIX_CQ_CTX_S[ENA] = 0); also sets
                                                                 NIX_LF_ERR_INT[CQ_DISABLED]. */
        uint64_t reserved_16_41        : 26;
        uint64_t cq_err_int_ena        : 8;  /**< [ 15:  8](R/W/H) Returns NIX_CQ_CTX_S[CQ_ERR_INT_ENA] on an atomic load-and-add. On a
                                                                 write, write-one-to-set NIX_CQ_CTX_S[CQ_ERR_INT_ENA] if [SETOP] is
                                                                 set, write-one-to-clear otherwise. */
        uint64_t cq_err_int            : 8;  /**< [  7:  0](R/W/H) Returns NIX_CQ_CTX_S[CQ_ERR_INT] on an atomic load-and-add. On a
                                                                 write, write-one-to-set NIX_CQ_CTX_S[CQ_ERR_INT] if [SETOP] is set,
                                                                 write-one-to-clear otherwise. */
#else /* Word 0 - Little Endian */
        uint64_t cq_err_int            : 8;  /**< [  7:  0](R/W/H) Returns NIX_CQ_CTX_S[CQ_ERR_INT] on an atomic load-and-add. On a
                                                                 write, write-one-to-set NIX_CQ_CTX_S[CQ_ERR_INT] if [SETOP] is set,
                                                                 write-one-to-clear otherwise. */
        uint64_t cq_err_int_ena        : 8;  /**< [ 15:  8](R/W/H) Returns NIX_CQ_CTX_S[CQ_ERR_INT_ENA] on an atomic load-and-add. On a
                                                                 write, write-one-to-set NIX_CQ_CTX_S[CQ_ERR_INT_ENA] if [SETOP] is
                                                                 set, write-one-to-clear otherwise. */
        uint64_t reserved_16_41        : 26;
        uint64_t op_err                : 1;  /**< [ 42: 42](RO/H) Operation error. Remaining read data fields are not valid. One of the
                                                                 following (may not be exhaustive):
                                                                 * Memory fault on NIX_CQ_CTX_S read; also sets NIX_LF_ERR_INT[CQ_CTX_FAULT].
                                                                 * Poisoned data returned on NIX_CQ_CTX_S read; also sets
                                                                 NIX_LF_RAS[CQ_CTX_POISON] and NIX_AF_RAS[CQ_CTX_POISON] .
                                                                 * Access to out-of-range CQ ([CQ] \> NIX_AF_LF()_CQS_CFG[MAX_QUEUESM1]);
                                                                 also sets NIX_LF_ERR_INT[CQ_OOR].
                                                                 * Disabled CQ (NIX_CQ_CTX_S[ENA] = 0); also sets
                                                                 NIX_LF_ERR_INT[CQ_DISABLED]. */
        uint64_t setop                 : 1;  /**< [ 43: 43](WO) Set operation. Valid on a write. Indicates write-one-to-set when set,
                                                                 write-one-to-clear when clear. */
        uint64_t cq                    : 20; /**< [ 63: 44](WO) CQ within LF. This field is present on a write, or in the write data
                                                                 of an atomic load-and-add. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_cq_op_int_s cn; */
};
typedef union bdk_nixx_lf_cq_op_int bdk_nixx_lf_cq_op_int_t;

static inline uint64_t BDK_NIXX_LF_CQ_OP_INT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_CQ_OP_INT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400b00ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_CQ_OP_INT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_CQ_OP_INT(a) bdk_nixx_lf_cq_op_int_t
#define bustype_BDK_NIXX_LF_CQ_OP_INT(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_CQ_OP_INT(a) "NIXX_LF_CQ_OP_INT"
#define device_bar_BDK_NIXX_LF_CQ_OP_INT(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_CQ_OP_INT(a) (a)
#define arguments_BDK_NIXX_LF_CQ_OP_INT(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_cq_op_status
 *
 * NIX LF Completion Queue Status Operation Register
 * A 64-bit atomic load-and-add to this register reads NIX_CQ_CTX_S[HEAD,TAIL].
 * The atomic write data has format NIX_OP_Q_WDATA_S and selects the CQ within LF.
 *
 * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
 */
union bdk_nixx_lf_cq_op_status
{
    uint64_t u;
    struct bdk_nixx_lf_cq_op_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_RQ_OP_INT[OP_ERR]. */
        uint64_t reserved_47_62        : 16;
        uint64_t cq_err                : 1;  /**< [ 46: 46](RO/H) See NIX_CQ_CTX_S[CQ_ERR]. */
        uint64_t state                 : 6;  /**< [ 45: 40](RO/H) See NIX_CQ_CTX_S[STATE]. */
        uint64_t head                  : 20; /**< [ 39: 20](RO/H) See NIX_CQ_CTX_S[HEAD]. The number of entries in the CQ is:
                                                                 _ ([TAIL] - [HEAD]) % (1 \<\< (2 * (NIX_CQ_CTX_S[QSIZE] + 2)) */
        uint64_t tail                  : 20; /**< [ 19:  0](RO/H) See NIX_CQ_CTX_S[TAIL]. */
#else /* Word 0 - Little Endian */
        uint64_t tail                  : 20; /**< [ 19:  0](RO/H) See NIX_CQ_CTX_S[TAIL]. */
        uint64_t head                  : 20; /**< [ 39: 20](RO/H) See NIX_CQ_CTX_S[HEAD]. The number of entries in the CQ is:
                                                                 _ ([TAIL] - [HEAD]) % (1 \<\< (2 * (NIX_CQ_CTX_S[QSIZE] + 2)) */
        uint64_t state                 : 6;  /**< [ 45: 40](RO/H) See NIX_CQ_CTX_S[STATE]. */
        uint64_t cq_err                : 1;  /**< [ 46: 46](RO/H) See NIX_CQ_CTX_S[CQ_ERR]. */
        uint64_t reserved_47_62        : 16;
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_RQ_OP_INT[OP_ERR]. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_cq_op_status_s cn; */
};
typedef union bdk_nixx_lf_cq_op_status bdk_nixx_lf_cq_op_status_t;

static inline uint64_t BDK_NIXX_LF_CQ_OP_STATUS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_CQ_OP_STATUS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400b40ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_CQ_OP_STATUS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_CQ_OP_STATUS(a) bdk_nixx_lf_cq_op_status_t
#define bustype_BDK_NIXX_LF_CQ_OP_STATUS(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_CQ_OP_STATUS(a) "NIXX_LF_CQ_OP_STATUS"
#define device_bar_BDK_NIXX_LF_CQ_OP_STATUS(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_CQ_OP_STATUS(a) (a)
#define arguments_BDK_NIXX_LF_CQ_OP_STATUS(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_err_int
 *
 * NIX LF Error Interrupt Register
 */
union bdk_nixx_lf_err_int
{
    uint64_t u;
    struct bdk_nixx_lf_err_int_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_30_63        : 34;
        uint64_t cint_fault            : 1;  /**< [ 29: 29](R/W1C/H) Memory fault on NIX_CINT_HW_S read or write. */
        uint64_t qint_fault            : 1;  /**< [ 28: 28](R/W1C/H) Memory fault on NIX_QINT_HW_S read or write. */
        uint64_t reserved_26_27        : 2;
        uint64_t cq_oor                : 1;  /**< [ 25: 25](R/W1C/H) CQ out of range. The CQ index of a send/receive CQE write or
                                                                 NIX_LF_CQ_OP_* access was greater than
                                                                 NIX_AF_LF()_CQS_CFG[MAX_QUEUESM1]. */
        uint64_t cq_disabled           : 1;  /**< [ 24: 24](R/W1C/H) CQ disabled. NIX_CQ_CTX_S[ENA] was clear for the CQ of a send/receive CQE
                                                                 write or NIX_LF_CQ_OP_* access. */
        uint64_t reserved_21_23        : 3;
        uint64_t dyno_err              : 1;  /**< [ 20: 20](R/W1C/H) NIX_LF_OP_IPSEC_DYNO_CNT[COUNT] underflow or overflow. */
        uint64_t reserved_14_19        : 6;
        uint64_t rx_wqe_fault          : 1;  /**< [ 13: 13](R/W1C/H) Memory fault on receive packet WQE write to LLC/DRAM. */
        uint64_t rq_oor                : 1;  /**< [ 12: 12](R/W1C/H) Packet receive or NIX_LF_RQ_OP_* access to out-of-range RQ. The
                                                                 packet's RQ index was greater than NIX_AF_LF()_RQS_CFG[MAX_QUEUESM1]. The RQ
                                                                 index for a received packet is obtained as follows:
                                                                 * When NIX_RX_ACTION_S[OP] = NIX_RX_ACTIONOP_E::UCAST or
                                                                 NIX_RX_ACTIONOP_E::UCAST_IPSEC, RQ = NIX_RX_ACTION_S[INDEX].
                                                                 * For RSS, NIX_RX_SSE_S[RQ].
                                                                 * For multicast or mirror packet with NIX_RX_MCE_S[OP] = NIX_RX_MCOP_E::RQ,
                                                                 RQ = NIX_RX_MCE_S[INDEX]. */
        uint64_t rq_disabled           : 1;  /**< [ 11: 11](R/W1C/H) Packet receive or NIX_LF_RQ_OP_* access to a disabled RQ;
                                                                 NIX_RQ_CTX_S[ENA] was clear for selected RQ. See [RQ_OOR] for RQ
                                                                 selection. */
        uint64_t send_sg_fault         : 1;  /**< [ 10: 10](R/W1C/H) Memory fault on packet data read for NIX_SEND_SG_S. */
        uint64_t send_jump_fault       : 1;  /**< [  9:  9](R/W1C/H) Memory fault on send descriptor read at or beyond NIX_SEND_JUMP_S[ADDR]. */
        uint64_t sq_oor                : 1;  /**< [  8:  8](R/W1C/H) LMT store or NIX_LF_SQ_OP_* access to out-of-range SQ. The SQ index was
                                                                 greater than NIX_AF_LF()_SQS_CFG[MAX_QUEUESM1]. */
        uint64_t sq_disabled           : 1;  /**< [  7:  7](R/W1C/H) LMT store or NIX_LF_SQ_OP_* access to a disabled SQ. NIX_SQ_CTX_S[ENA]
                                                                 was clear for the selected SQ. */
        uint64_t ipsec_dyno_fault      : 1;  /**< [  6:  6](R/W1C/H) Memory fault on IPSEC dynamic ordering counter read. See
                                                                 NIX_AF_LF()_RX_IPSEC_CFG. */
        uint64_t rsse_fault            : 1;  /**< [  5:  5](R/W1C/H) Memory fault on NIX_RSSE_S read. */
        uint64_t reserved_4            : 1;
        uint64_t cq_ctx_fault          : 1;  /**< [  3:  3](R/W1C/H) Memory fault on NIX_CQ_CTX_S read. */
        uint64_t rq_ctx_fault          : 1;  /**< [  2:  2](R/W1C/H) Memory fault on NIX_RQ_CTX_S read on packet receive or NIX_LF_RQ_OP_* access. */
        uint64_t sq_ctx_fault          : 1;  /**< [  1:  1](R/W1C/H) Memory fault on NIX_SQ_CTX_HW_S read. */
        uint64_t sqb_fault             : 1;  /**< [  0:  0](R/W1C/H) Memory fault on SQB read or write. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_fault             : 1;  /**< [  0:  0](R/W1C/H) Memory fault on SQB read or write. */
        uint64_t sq_ctx_fault          : 1;  /**< [  1:  1](R/W1C/H) Memory fault on NIX_SQ_CTX_HW_S read. */
        uint64_t rq_ctx_fault          : 1;  /**< [  2:  2](R/W1C/H) Memory fault on NIX_RQ_CTX_S read on packet receive or NIX_LF_RQ_OP_* access. */
        uint64_t cq_ctx_fault          : 1;  /**< [  3:  3](R/W1C/H) Memory fault on NIX_CQ_CTX_S read. */
        uint64_t reserved_4            : 1;
        uint64_t rsse_fault            : 1;  /**< [  5:  5](R/W1C/H) Memory fault on NIX_RSSE_S read. */
        uint64_t ipsec_dyno_fault      : 1;  /**< [  6:  6](R/W1C/H) Memory fault on IPSEC dynamic ordering counter read. See
                                                                 NIX_AF_LF()_RX_IPSEC_CFG. */
        uint64_t sq_disabled           : 1;  /**< [  7:  7](R/W1C/H) LMT store or NIX_LF_SQ_OP_* access to a disabled SQ. NIX_SQ_CTX_S[ENA]
                                                                 was clear for the selected SQ. */
        uint64_t sq_oor                : 1;  /**< [  8:  8](R/W1C/H) LMT store or NIX_LF_SQ_OP_* access to out-of-range SQ. The SQ index was
                                                                 greater than NIX_AF_LF()_SQS_CFG[MAX_QUEUESM1]. */
        uint64_t send_jump_fault       : 1;  /**< [  9:  9](R/W1C/H) Memory fault on send descriptor read at or beyond NIX_SEND_JUMP_S[ADDR]. */
        uint64_t send_sg_fault         : 1;  /**< [ 10: 10](R/W1C/H) Memory fault on packet data read for NIX_SEND_SG_S. */
        uint64_t rq_disabled           : 1;  /**< [ 11: 11](R/W1C/H) Packet receive or NIX_LF_RQ_OP_* access to a disabled RQ;
                                                                 NIX_RQ_CTX_S[ENA] was clear for selected RQ. See [RQ_OOR] for RQ
                                                                 selection. */
        uint64_t rq_oor                : 1;  /**< [ 12: 12](R/W1C/H) Packet receive or NIX_LF_RQ_OP_* access to out-of-range RQ. The
                                                                 packet's RQ index was greater than NIX_AF_LF()_RQS_CFG[MAX_QUEUESM1]. The RQ
                                                                 index for a received packet is obtained as follows:
                                                                 * When NIX_RX_ACTION_S[OP] = NIX_RX_ACTIONOP_E::UCAST or
                                                                 NIX_RX_ACTIONOP_E::UCAST_IPSEC, RQ = NIX_RX_ACTION_S[INDEX].
                                                                 * For RSS, NIX_RX_SSE_S[RQ].
                                                                 * For multicast or mirror packet with NIX_RX_MCE_S[OP] = NIX_RX_MCOP_E::RQ,
                                                                 RQ = NIX_RX_MCE_S[INDEX]. */
        uint64_t rx_wqe_fault          : 1;  /**< [ 13: 13](R/W1C/H) Memory fault on receive packet WQE write to LLC/DRAM. */
        uint64_t reserved_14_19        : 6;
        uint64_t dyno_err              : 1;  /**< [ 20: 20](R/W1C/H) NIX_LF_OP_IPSEC_DYNO_CNT[COUNT] underflow or overflow. */
        uint64_t reserved_21_23        : 3;
        uint64_t cq_disabled           : 1;  /**< [ 24: 24](R/W1C/H) CQ disabled. NIX_CQ_CTX_S[ENA] was clear for the CQ of a send/receive CQE
                                                                 write or NIX_LF_CQ_OP_* access. */
        uint64_t cq_oor                : 1;  /**< [ 25: 25](R/W1C/H) CQ out of range. The CQ index of a send/receive CQE write or
                                                                 NIX_LF_CQ_OP_* access was greater than
                                                                 NIX_AF_LF()_CQS_CFG[MAX_QUEUESM1]. */
        uint64_t reserved_26_27        : 2;
        uint64_t qint_fault            : 1;  /**< [ 28: 28](R/W1C/H) Memory fault on NIX_QINT_HW_S read or write. */
        uint64_t cint_fault            : 1;  /**< [ 29: 29](R/W1C/H) Memory fault on NIX_CINT_HW_S read or write. */
        uint64_t reserved_30_63        : 34;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_err_int_s cn; */
};
typedef union bdk_nixx_lf_err_int bdk_nixx_lf_err_int_t;

static inline uint64_t BDK_NIXX_LF_ERR_INT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_ERR_INT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400220ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_ERR_INT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_ERR_INT(a) bdk_nixx_lf_err_int_t
#define bustype_BDK_NIXX_LF_ERR_INT(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_ERR_INT(a) "NIXX_LF_ERR_INT"
#define device_bar_BDK_NIXX_LF_ERR_INT(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_ERR_INT(a) (a)
#define arguments_BDK_NIXX_LF_ERR_INT(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_err_int_ena_w1c
 *
 * NIX LF Error Interrupt Enable Clear Register
 * This register clears interrupt enable bits.
 */
union bdk_nixx_lf_err_int_ena_w1c
{
    uint64_t u;
    struct bdk_nixx_lf_err_int_ena_w1c_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_30_63        : 34;
        uint64_t cint_fault            : 1;  /**< [ 29: 29](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[CINT_FAULT]. */
        uint64_t qint_fault            : 1;  /**< [ 28: 28](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[QINT_FAULT]. */
        uint64_t reserved_26_27        : 2;
        uint64_t cq_oor                : 1;  /**< [ 25: 25](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[CQ_OOR]. */
        uint64_t cq_disabled           : 1;  /**< [ 24: 24](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[CQ_DISABLED]. */
        uint64_t reserved_21_23        : 3;
        uint64_t dyno_err              : 1;  /**< [ 20: 20](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[DYNO_ERR]. */
        uint64_t reserved_14_19        : 6;
        uint64_t rx_wqe_fault          : 1;  /**< [ 13: 13](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[RX_WQE_FAULT]. */
        uint64_t rq_oor                : 1;  /**< [ 12: 12](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[RQ_OOR]. */
        uint64_t rq_disabled           : 1;  /**< [ 11: 11](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[RQ_DISABLED]. */
        uint64_t send_sg_fault         : 1;  /**< [ 10: 10](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SEND_SG_FAULT]. */
        uint64_t send_jump_fault       : 1;  /**< [  9:  9](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SEND_JUMP_FAULT]. */
        uint64_t sq_oor                : 1;  /**< [  8:  8](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SQ_OOR]. */
        uint64_t sq_disabled           : 1;  /**< [  7:  7](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SQ_DISABLED]. */
        uint64_t ipsec_dyno_fault      : 1;  /**< [  6:  6](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[IPSEC_DYNO_FAULT]. */
        uint64_t rsse_fault            : 1;  /**< [  5:  5](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[RSSE_FAULT]. */
        uint64_t reserved_4            : 1;
        uint64_t cq_ctx_fault          : 1;  /**< [  3:  3](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[CQ_CTX_FAULT]. */
        uint64_t rq_ctx_fault          : 1;  /**< [  2:  2](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[RQ_CTX_FAULT]. */
        uint64_t sq_ctx_fault          : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SQ_CTX_FAULT]. */
        uint64_t sqb_fault             : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SQB_FAULT]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_fault             : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SQB_FAULT]. */
        uint64_t sq_ctx_fault          : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SQ_CTX_FAULT]. */
        uint64_t rq_ctx_fault          : 1;  /**< [  2:  2](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[RQ_CTX_FAULT]. */
        uint64_t cq_ctx_fault          : 1;  /**< [  3:  3](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[CQ_CTX_FAULT]. */
        uint64_t reserved_4            : 1;
        uint64_t rsse_fault            : 1;  /**< [  5:  5](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[RSSE_FAULT]. */
        uint64_t ipsec_dyno_fault      : 1;  /**< [  6:  6](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[IPSEC_DYNO_FAULT]. */
        uint64_t sq_disabled           : 1;  /**< [  7:  7](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SQ_DISABLED]. */
        uint64_t sq_oor                : 1;  /**< [  8:  8](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SQ_OOR]. */
        uint64_t send_jump_fault       : 1;  /**< [  9:  9](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SEND_JUMP_FAULT]. */
        uint64_t send_sg_fault         : 1;  /**< [ 10: 10](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[SEND_SG_FAULT]. */
        uint64_t rq_disabled           : 1;  /**< [ 11: 11](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[RQ_DISABLED]. */
        uint64_t rq_oor                : 1;  /**< [ 12: 12](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[RQ_OOR]. */
        uint64_t rx_wqe_fault          : 1;  /**< [ 13: 13](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[RX_WQE_FAULT]. */
        uint64_t reserved_14_19        : 6;
        uint64_t dyno_err              : 1;  /**< [ 20: 20](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[DYNO_ERR]. */
        uint64_t reserved_21_23        : 3;
        uint64_t cq_disabled           : 1;  /**< [ 24: 24](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[CQ_DISABLED]. */
        uint64_t cq_oor                : 1;  /**< [ 25: 25](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[CQ_OOR]. */
        uint64_t reserved_26_27        : 2;
        uint64_t qint_fault            : 1;  /**< [ 28: 28](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[QINT_FAULT]. */
        uint64_t cint_fault            : 1;  /**< [ 29: 29](R/W1C/H) Reads or clears enable for NIX_LF_ERR_INT[CINT_FAULT]. */
        uint64_t reserved_30_63        : 34;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_err_int_ena_w1c_s cn; */
};
typedef union bdk_nixx_lf_err_int_ena_w1c bdk_nixx_lf_err_int_ena_w1c_t;

static inline uint64_t BDK_NIXX_LF_ERR_INT_ENA_W1C(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_ERR_INT_ENA_W1C(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400230ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_ERR_INT_ENA_W1C", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_ERR_INT_ENA_W1C(a) bdk_nixx_lf_err_int_ena_w1c_t
#define bustype_BDK_NIXX_LF_ERR_INT_ENA_W1C(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_ERR_INT_ENA_W1C(a) "NIXX_LF_ERR_INT_ENA_W1C"
#define device_bar_BDK_NIXX_LF_ERR_INT_ENA_W1C(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_ERR_INT_ENA_W1C(a) (a)
#define arguments_BDK_NIXX_LF_ERR_INT_ENA_W1C(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_err_int_ena_w1s
 *
 * NIX LF Error Interrupt Enable Set Register
 * This register sets interrupt enable bits.
 */
union bdk_nixx_lf_err_int_ena_w1s
{
    uint64_t u;
    struct bdk_nixx_lf_err_int_ena_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_30_63        : 34;
        uint64_t cint_fault            : 1;  /**< [ 29: 29](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[CINT_FAULT]. */
        uint64_t qint_fault            : 1;  /**< [ 28: 28](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[QINT_FAULT]. */
        uint64_t reserved_26_27        : 2;
        uint64_t cq_oor                : 1;  /**< [ 25: 25](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[CQ_OOR]. */
        uint64_t cq_disabled           : 1;  /**< [ 24: 24](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[CQ_DISABLED]. */
        uint64_t reserved_21_23        : 3;
        uint64_t dyno_err              : 1;  /**< [ 20: 20](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[DYNO_ERR]. */
        uint64_t reserved_14_19        : 6;
        uint64_t rx_wqe_fault          : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[RX_WQE_FAULT]. */
        uint64_t rq_oor                : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[RQ_OOR]. */
        uint64_t rq_disabled           : 1;  /**< [ 11: 11](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[RQ_DISABLED]. */
        uint64_t send_sg_fault         : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SEND_SG_FAULT]. */
        uint64_t send_jump_fault       : 1;  /**< [  9:  9](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SEND_JUMP_FAULT]. */
        uint64_t sq_oor                : 1;  /**< [  8:  8](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SQ_OOR]. */
        uint64_t sq_disabled           : 1;  /**< [  7:  7](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SQ_DISABLED]. */
        uint64_t ipsec_dyno_fault      : 1;  /**< [  6:  6](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[IPSEC_DYNO_FAULT]. */
        uint64_t rsse_fault            : 1;  /**< [  5:  5](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[RSSE_FAULT]. */
        uint64_t reserved_4            : 1;
        uint64_t cq_ctx_fault          : 1;  /**< [  3:  3](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[CQ_CTX_FAULT]. */
        uint64_t rq_ctx_fault          : 1;  /**< [  2:  2](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[RQ_CTX_FAULT]. */
        uint64_t sq_ctx_fault          : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SQ_CTX_FAULT]. */
        uint64_t sqb_fault             : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SQB_FAULT]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_fault             : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SQB_FAULT]. */
        uint64_t sq_ctx_fault          : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SQ_CTX_FAULT]. */
        uint64_t rq_ctx_fault          : 1;  /**< [  2:  2](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[RQ_CTX_FAULT]. */
        uint64_t cq_ctx_fault          : 1;  /**< [  3:  3](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[CQ_CTX_FAULT]. */
        uint64_t reserved_4            : 1;
        uint64_t rsse_fault            : 1;  /**< [  5:  5](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[RSSE_FAULT]. */
        uint64_t ipsec_dyno_fault      : 1;  /**< [  6:  6](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[IPSEC_DYNO_FAULT]. */
        uint64_t sq_disabled           : 1;  /**< [  7:  7](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SQ_DISABLED]. */
        uint64_t sq_oor                : 1;  /**< [  8:  8](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SQ_OOR]. */
        uint64_t send_jump_fault       : 1;  /**< [  9:  9](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SEND_JUMP_FAULT]. */
        uint64_t send_sg_fault         : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[SEND_SG_FAULT]. */
        uint64_t rq_disabled           : 1;  /**< [ 11: 11](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[RQ_DISABLED]. */
        uint64_t rq_oor                : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[RQ_OOR]. */
        uint64_t rx_wqe_fault          : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[RX_WQE_FAULT]. */
        uint64_t reserved_14_19        : 6;
        uint64_t dyno_err              : 1;  /**< [ 20: 20](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[DYNO_ERR]. */
        uint64_t reserved_21_23        : 3;
        uint64_t cq_disabled           : 1;  /**< [ 24: 24](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[CQ_DISABLED]. */
        uint64_t cq_oor                : 1;  /**< [ 25: 25](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[CQ_OOR]. */
        uint64_t reserved_26_27        : 2;
        uint64_t qint_fault            : 1;  /**< [ 28: 28](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[QINT_FAULT]. */
        uint64_t cint_fault            : 1;  /**< [ 29: 29](R/W1S/H) Reads or sets enable for NIX_LF_ERR_INT[CINT_FAULT]. */
        uint64_t reserved_30_63        : 34;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_err_int_ena_w1s_s cn; */
};
typedef union bdk_nixx_lf_err_int_ena_w1s bdk_nixx_lf_err_int_ena_w1s_t;

static inline uint64_t BDK_NIXX_LF_ERR_INT_ENA_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_ERR_INT_ENA_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400238ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_ERR_INT_ENA_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_ERR_INT_ENA_W1S(a) bdk_nixx_lf_err_int_ena_w1s_t
#define bustype_BDK_NIXX_LF_ERR_INT_ENA_W1S(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_ERR_INT_ENA_W1S(a) "NIXX_LF_ERR_INT_ENA_W1S"
#define device_bar_BDK_NIXX_LF_ERR_INT_ENA_W1S(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_ERR_INT_ENA_W1S(a) (a)
#define arguments_BDK_NIXX_LF_ERR_INT_ENA_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_err_int_w1s
 *
 * NIX LF Error Interrupt Set Register
 * This register sets interrupt bits.
 */
union bdk_nixx_lf_err_int_w1s
{
    uint64_t u;
    struct bdk_nixx_lf_err_int_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_30_63        : 34;
        uint64_t cint_fault            : 1;  /**< [ 29: 29](R/W1S/H) Reads or sets NIX_LF_ERR_INT[CINT_FAULT]. */
        uint64_t qint_fault            : 1;  /**< [ 28: 28](R/W1S/H) Reads or sets NIX_LF_ERR_INT[QINT_FAULT]. */
        uint64_t reserved_26_27        : 2;
        uint64_t cq_oor                : 1;  /**< [ 25: 25](R/W1S/H) Reads or sets NIX_LF_ERR_INT[CQ_OOR]. */
        uint64_t cq_disabled           : 1;  /**< [ 24: 24](R/W1S/H) Reads or sets NIX_LF_ERR_INT[CQ_DISABLED]. */
        uint64_t reserved_21_23        : 3;
        uint64_t dyno_err              : 1;  /**< [ 20: 20](R/W1S/H) Reads or sets NIX_LF_ERR_INT[DYNO_ERR]. */
        uint64_t reserved_14_19        : 6;
        uint64_t rx_wqe_fault          : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets NIX_LF_ERR_INT[RX_WQE_FAULT]. */
        uint64_t rq_oor                : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets NIX_LF_ERR_INT[RQ_OOR]. */
        uint64_t rq_disabled           : 1;  /**< [ 11: 11](R/W1S/H) Reads or sets NIX_LF_ERR_INT[RQ_DISABLED]. */
        uint64_t send_sg_fault         : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SEND_SG_FAULT]. */
        uint64_t send_jump_fault       : 1;  /**< [  9:  9](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SEND_JUMP_FAULT]. */
        uint64_t sq_oor                : 1;  /**< [  8:  8](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SQ_OOR]. */
        uint64_t sq_disabled           : 1;  /**< [  7:  7](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SQ_DISABLED]. */
        uint64_t ipsec_dyno_fault      : 1;  /**< [  6:  6](R/W1S/H) Reads or sets NIX_LF_ERR_INT[IPSEC_DYNO_FAULT]. */
        uint64_t rsse_fault            : 1;  /**< [  5:  5](R/W1S/H) Reads or sets NIX_LF_ERR_INT[RSSE_FAULT]. */
        uint64_t reserved_4            : 1;
        uint64_t cq_ctx_fault          : 1;  /**< [  3:  3](R/W1S/H) Reads or sets NIX_LF_ERR_INT[CQ_CTX_FAULT]. */
        uint64_t rq_ctx_fault          : 1;  /**< [  2:  2](R/W1S/H) Reads or sets NIX_LF_ERR_INT[RQ_CTX_FAULT]. */
        uint64_t sq_ctx_fault          : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SQ_CTX_FAULT]. */
        uint64_t sqb_fault             : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SQB_FAULT]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_fault             : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SQB_FAULT]. */
        uint64_t sq_ctx_fault          : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SQ_CTX_FAULT]. */
        uint64_t rq_ctx_fault          : 1;  /**< [  2:  2](R/W1S/H) Reads or sets NIX_LF_ERR_INT[RQ_CTX_FAULT]. */
        uint64_t cq_ctx_fault          : 1;  /**< [  3:  3](R/W1S/H) Reads or sets NIX_LF_ERR_INT[CQ_CTX_FAULT]. */
        uint64_t reserved_4            : 1;
        uint64_t rsse_fault            : 1;  /**< [  5:  5](R/W1S/H) Reads or sets NIX_LF_ERR_INT[RSSE_FAULT]. */
        uint64_t ipsec_dyno_fault      : 1;  /**< [  6:  6](R/W1S/H) Reads or sets NIX_LF_ERR_INT[IPSEC_DYNO_FAULT]. */
        uint64_t sq_disabled           : 1;  /**< [  7:  7](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SQ_DISABLED]. */
        uint64_t sq_oor                : 1;  /**< [  8:  8](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SQ_OOR]. */
        uint64_t send_jump_fault       : 1;  /**< [  9:  9](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SEND_JUMP_FAULT]. */
        uint64_t send_sg_fault         : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets NIX_LF_ERR_INT[SEND_SG_FAULT]. */
        uint64_t rq_disabled           : 1;  /**< [ 11: 11](R/W1S/H) Reads or sets NIX_LF_ERR_INT[RQ_DISABLED]. */
        uint64_t rq_oor                : 1;  /**< [ 12: 12](R/W1S/H) Reads or sets NIX_LF_ERR_INT[RQ_OOR]. */
        uint64_t rx_wqe_fault          : 1;  /**< [ 13: 13](R/W1S/H) Reads or sets NIX_LF_ERR_INT[RX_WQE_FAULT]. */
        uint64_t reserved_14_19        : 6;
        uint64_t dyno_err              : 1;  /**< [ 20: 20](R/W1S/H) Reads or sets NIX_LF_ERR_INT[DYNO_ERR]. */
        uint64_t reserved_21_23        : 3;
        uint64_t cq_disabled           : 1;  /**< [ 24: 24](R/W1S/H) Reads or sets NIX_LF_ERR_INT[CQ_DISABLED]. */
        uint64_t cq_oor                : 1;  /**< [ 25: 25](R/W1S/H) Reads or sets NIX_LF_ERR_INT[CQ_OOR]. */
        uint64_t reserved_26_27        : 2;
        uint64_t qint_fault            : 1;  /**< [ 28: 28](R/W1S/H) Reads or sets NIX_LF_ERR_INT[QINT_FAULT]. */
        uint64_t cint_fault            : 1;  /**< [ 29: 29](R/W1S/H) Reads or sets NIX_LF_ERR_INT[CINT_FAULT]. */
        uint64_t reserved_30_63        : 34;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_err_int_w1s_s cn; */
};
typedef union bdk_nixx_lf_err_int_w1s bdk_nixx_lf_err_int_w1s_t;

static inline uint64_t BDK_NIXX_LF_ERR_INT_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_ERR_INT_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400228ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_ERR_INT_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_ERR_INT_W1S(a) bdk_nixx_lf_err_int_w1s_t
#define bustype_BDK_NIXX_LF_ERR_INT_W1S(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_ERR_INT_W1S(a) "NIXX_LF_ERR_INT_W1S"
#define device_bar_BDK_NIXX_LF_ERR_INT_W1S(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_ERR_INT_W1S(a) (a)
#define arguments_BDK_NIXX_LF_ERR_INT_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_gint
 *
 * NIX LF General Interrupt Register
 */
union bdk_nixx_lf_gint
{
    uint64_t u;
    struct bdk_nixx_lf_gint_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_2_63         : 62;
        uint64_t tcp_timer             : 1;  /**< [  1:  1](R/W1C/H) TCP timer interrupt. Enabled when NIX_AF_TCP_TIMER[ENA] and
                                                                 NIX_LF_CFG[TCP_TIMER_INT_ENA] are both set. Set every
                                                                 NIX_AF_TCP_TIMER[DURATION]*256*128 coprocessor cycles when enabled. */
        uint64_t drop                  : 1;  /**< [  0:  0](R/W1C/H) Packet dropped interrupt. Set when any packet has been dropped. This is intended for
                                                                 diagnostic use; typical production software will want this interrupt disabled. */
#else /* Word 0 - Little Endian */
        uint64_t drop                  : 1;  /**< [  0:  0](R/W1C/H) Packet dropped interrupt. Set when any packet has been dropped. This is intended for
                                                                 diagnostic use; typical production software will want this interrupt disabled. */
        uint64_t tcp_timer             : 1;  /**< [  1:  1](R/W1C/H) TCP timer interrupt. Enabled when NIX_AF_TCP_TIMER[ENA] and
                                                                 NIX_LF_CFG[TCP_TIMER_INT_ENA] are both set. Set every
                                                                 NIX_AF_TCP_TIMER[DURATION]*256*128 coprocessor cycles when enabled. */
        uint64_t reserved_2_63         : 62;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_gint_s cn; */
};
typedef union bdk_nixx_lf_gint bdk_nixx_lf_gint_t;

static inline uint64_t BDK_NIXX_LF_GINT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_GINT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400200ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_GINT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_GINT(a) bdk_nixx_lf_gint_t
#define bustype_BDK_NIXX_LF_GINT(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_GINT(a) "NIXX_LF_GINT"
#define device_bar_BDK_NIXX_LF_GINT(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_GINT(a) (a)
#define arguments_BDK_NIXX_LF_GINT(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_gint_ena_w1c
 *
 * NIX LF General Interrupt Enable Clear Register
 * This register clears interrupt enable bits.
 */
union bdk_nixx_lf_gint_ena_w1c
{
    uint64_t u;
    struct bdk_nixx_lf_gint_ena_w1c_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_2_63         : 62;
        uint64_t tcp_timer             : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_LF_GINT[TCP_TIMER]. */
        uint64_t drop                  : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_LF_GINT[DROP]. */
#else /* Word 0 - Little Endian */
        uint64_t drop                  : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_LF_GINT[DROP]. */
        uint64_t tcp_timer             : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_LF_GINT[TCP_TIMER]. */
        uint64_t reserved_2_63         : 62;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_gint_ena_w1c_s cn; */
};
typedef union bdk_nixx_lf_gint_ena_w1c bdk_nixx_lf_gint_ena_w1c_t;

static inline uint64_t BDK_NIXX_LF_GINT_ENA_W1C(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_GINT_ENA_W1C(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400210ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_GINT_ENA_W1C", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_GINT_ENA_W1C(a) bdk_nixx_lf_gint_ena_w1c_t
#define bustype_BDK_NIXX_LF_GINT_ENA_W1C(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_GINT_ENA_W1C(a) "NIXX_LF_GINT_ENA_W1C"
#define device_bar_BDK_NIXX_LF_GINT_ENA_W1C(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_GINT_ENA_W1C(a) (a)
#define arguments_BDK_NIXX_LF_GINT_ENA_W1C(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_gint_ena_w1s
 *
 * NIX LF General Interrupt Enable Set Register
 * This register sets interrupt enable bits.
 */
union bdk_nixx_lf_gint_ena_w1s
{
    uint64_t u;
    struct bdk_nixx_lf_gint_ena_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_2_63         : 62;
        uint64_t tcp_timer             : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_LF_GINT[TCP_TIMER]. */
        uint64_t drop                  : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_LF_GINT[DROP]. */
#else /* Word 0 - Little Endian */
        uint64_t drop                  : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_LF_GINT[DROP]. */
        uint64_t tcp_timer             : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_LF_GINT[TCP_TIMER]. */
        uint64_t reserved_2_63         : 62;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_gint_ena_w1s_s cn; */
};
typedef union bdk_nixx_lf_gint_ena_w1s bdk_nixx_lf_gint_ena_w1s_t;

static inline uint64_t BDK_NIXX_LF_GINT_ENA_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_GINT_ENA_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400218ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_GINT_ENA_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_GINT_ENA_W1S(a) bdk_nixx_lf_gint_ena_w1s_t
#define bustype_BDK_NIXX_LF_GINT_ENA_W1S(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_GINT_ENA_W1S(a) "NIXX_LF_GINT_ENA_W1S"
#define device_bar_BDK_NIXX_LF_GINT_ENA_W1S(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_GINT_ENA_W1S(a) (a)
#define arguments_BDK_NIXX_LF_GINT_ENA_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_gint_w1s
 *
 * NIX LF General Interrupt Set Register
 * This register sets interrupt bits.
 */
union bdk_nixx_lf_gint_w1s
{
    uint64_t u;
    struct bdk_nixx_lf_gint_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_2_63         : 62;
        uint64_t tcp_timer             : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_LF_GINT[TCP_TIMER]. */
        uint64_t drop                  : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_LF_GINT[DROP]. */
#else /* Word 0 - Little Endian */
        uint64_t drop                  : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_LF_GINT[DROP]. */
        uint64_t tcp_timer             : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_LF_GINT[TCP_TIMER]. */
        uint64_t reserved_2_63         : 62;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_gint_w1s_s cn; */
};
typedef union bdk_nixx_lf_gint_w1s bdk_nixx_lf_gint_w1s_t;

static inline uint64_t BDK_NIXX_LF_GINT_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_GINT_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400208ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_GINT_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_GINT_W1S(a) bdk_nixx_lf_gint_w1s_t
#define bustype_BDK_NIXX_LF_GINT_W1S(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_GINT_W1S(a) "NIXX_LF_GINT_W1S"
#define device_bar_BDK_NIXX_LF_GINT_W1S(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_GINT_W1S(a) (a)
#define arguments_BDK_NIXX_LF_GINT_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_lmt_err_dbg
 *
 * NIX LF LMT Store Error Debug Register
 * This register captures debug info for an error detected on LMT store to
 * NIX_LF_OP_SEND().
 * Hardware sets [VALID] when the debug info is captured, and subsequent errors
 * are not captured until software clears [VALID] by writing a one to it.
 */
union bdk_nixx_lf_lmt_err_dbg
{
    uint64_t u;
    struct bdk_nixx_lf_lmt_err_dbg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_45_63        : 19;
        uint64_t valid                 : 1;  /**< [ 44: 44](R/W1C/H) Debug info valid. When set, remaining fields in this register are valid. */
        uint64_t sqe_id                : 16; /**< [ 43: 28](RO/H) SQE identifier from NIX_SEND_HDR_S[SQE_ID]. */
        uint64_t sq                    : 20; /**< [ 27:  8](RO/H) SQ within LF. */
        uint64_t errcode               : 8;  /**< [  7:  0](RO/H) Error code enumerated by NIX_SQOPERR_E. */
#else /* Word 0 - Little Endian */
        uint64_t errcode               : 8;  /**< [  7:  0](RO/H) Error code enumerated by NIX_SQOPERR_E. */
        uint64_t sq                    : 20; /**< [ 27:  8](RO/H) SQ within LF. */
        uint64_t sqe_id                : 16; /**< [ 43: 28](RO/H) SQE identifier from NIX_SEND_HDR_S[SQE_ID]. */
        uint64_t valid                 : 1;  /**< [ 44: 44](R/W1C/H) Debug info valid. When set, remaining fields in this register are valid. */
        uint64_t reserved_45_63        : 19;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_lmt_err_dbg_s cn; */
};
typedef union bdk_nixx_lf_lmt_err_dbg bdk_nixx_lf_lmt_err_dbg_t;

static inline uint64_t BDK_NIXX_LF_LMT_ERR_DBG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_LMT_ERR_DBG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400260ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_LMT_ERR_DBG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_LMT_ERR_DBG(a) bdk_nixx_lf_lmt_err_dbg_t
#define bustype_BDK_NIXX_LF_LMT_ERR_DBG(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_LMT_ERR_DBG(a) "NIXX_LF_LMT_ERR_DBG"
#define device_bar_BDK_NIXX_LF_LMT_ERR_DBG(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_LMT_ERR_DBG(a) (a)
#define arguments_BDK_NIXX_LF_LMT_ERR_DBG(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_mnq_err_dbg
 *
 * NIX LF Meta-descriptor Enqueue Error Debug Register
 * This register captures debug info for an error detected during send
 * meta-descriptor enqueue from an SQ to an SMQ.
 * Hardware sets [VALID] when the debug info is captured, and subsequent errors
 * are not captured until software clears [VALID] by writing a one to it.
 */
union bdk_nixx_lf_mnq_err_dbg
{
    uint64_t u;
    struct bdk_nixx_lf_mnq_err_dbg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_45_63        : 19;
        uint64_t valid                 : 1;  /**< [ 44: 44](R/W1C/H) Debug info valid. When set, remaining fields in this register are valid. */
        uint64_t sqe_id                : 16; /**< [ 43: 28](RO/H) SQE identifier from NIX_SEND_HDR_S[SQE_ID]. */
        uint64_t sq                    : 20; /**< [ 27:  8](RO/H) SQ within LF. */
        uint64_t errcode               : 8;  /**< [  7:  0](RO/H) Error code enumerated by NIX_MNQERR_E. */
#else /* Word 0 - Little Endian */
        uint64_t errcode               : 8;  /**< [  7:  0](RO/H) Error code enumerated by NIX_MNQERR_E. */
        uint64_t sq                    : 20; /**< [ 27:  8](RO/H) SQ within LF. */
        uint64_t sqe_id                : 16; /**< [ 43: 28](RO/H) SQE identifier from NIX_SEND_HDR_S[SQE_ID]. */
        uint64_t valid                 : 1;  /**< [ 44: 44](R/W1C/H) Debug info valid. When set, remaining fields in this register are valid. */
        uint64_t reserved_45_63        : 19;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_mnq_err_dbg_s cn; */
};
typedef union bdk_nixx_lf_mnq_err_dbg bdk_nixx_lf_mnq_err_dbg_t;

static inline uint64_t BDK_NIXX_LF_MNQ_ERR_DBG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_MNQ_ERR_DBG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400270ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_MNQ_ERR_DBG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_MNQ_ERR_DBG(a) bdk_nixx_lf_mnq_err_dbg_t
#define bustype_BDK_NIXX_LF_MNQ_ERR_DBG(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_MNQ_ERR_DBG(a) "NIXX_LF_MNQ_ERR_DBG"
#define device_bar_BDK_NIXX_LF_MNQ_ERR_DBG(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_MNQ_ERR_DBG(a) (a)
#define arguments_BDK_NIXX_LF_MNQ_ERR_DBG(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_op_ipsec_dyno_cnt
 *
 * NIX LF IPSEC Dynamic Ordering Counter Operation Register
 * A 64-bit atomic load-and-add to this register reads an IPSEC dynamic ordering
 * (DYNO) counter. A write decrements a DYNO counter.
 * See NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_ENA].
 */
union bdk_nixx_lf_op_ipsec_dyno_cnt
{
    uint64_t u;
    struct bdk_nixx_lf_op_ipsec_dyno_cnt_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_63           : 1;
        uint64_t dyno_sel              : 15; /**< [ 62: 48](WO) Selects DYNO counter within LF. This field is present on a write, or in the
                                                                 write data of an atomic load-and-add. Hardware uses the lower
                                                                 NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_IDX_W] bits of this field and ignores
                                                                 upper bits, if any. */
        uint64_t storeop               : 1;  /**< [ 47: 47](WO) Store operation. Valid on a write to this register:
                                                                 0 = Decrement [COUNT] from the DYNO counter.
                                                                 1 = Store [COUNT] to the DYNO counter. */
        uint64_t reserved_32_46        : 15;
        uint64_t count                 : 32; /**< [ 31:  0](R/W/H) Returns the DYNO counter value on an atomic load-and-add. On a write, value
                                                                 to decrement from the DYNO counter when [STOREOP] is clear, else value
                                                                 written to the DYNO counter.

                                                                 Hardware writes a DYNO counter value of all ones (0xFFFFFFFF) to indicate a
                                                                 counter error due to one of the following:
                                                                 * A decrement (write to this register with [STOREOP] = 0) that would
                                                                 underflow the counter. This also sets NIX_LF_ERR_INT[DYNO_ERR].
                                                                 * A hardware increment to all ones for a dynamically prevented IPSEC
                                                                 packet (counter overflow). This also sets NIX_LF_ERR_INT[DYNO_ERR].
                                                                 * Counter read poison error. This also sets NIX_LF_RAS[IPSEC_DYNO_POISON]
                                                                 and NIX_AF_RAS[IPSEC_DYNO_POISON].

                                                                 Hardware does the following when a DYNO counter is all ones:
                                                                 * Disable IPSEC hardware fast-path for associated flows. Packets that would
                                                                 otherwise use the hardware fast-path are steered to the dynamically
                                                                 prevented path with NIX_WQE_HDR_S[WQE_TYPE] = NIX_XQE_TYPE_E::RX_IPSECD.
                                                                 * Suppress counter decrement by software and increment by hardware.

                                                                 Software store a value other than all ones to a DYNO counter to clear the
                                                                 error state. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 32; /**< [ 31:  0](R/W/H) Returns the DYNO counter value on an atomic load-and-add. On a write, value
                                                                 to decrement from the DYNO counter when [STOREOP] is clear, else value
                                                                 written to the DYNO counter.

                                                                 Hardware writes a DYNO counter value of all ones (0xFFFFFFFF) to indicate a
                                                                 counter error due to one of the following:
                                                                 * A decrement (write to this register with [STOREOP] = 0) that would
                                                                 underflow the counter. This also sets NIX_LF_ERR_INT[DYNO_ERR].
                                                                 * A hardware increment to all ones for a dynamically prevented IPSEC
                                                                 packet (counter overflow). This also sets NIX_LF_ERR_INT[DYNO_ERR].
                                                                 * Counter read poison error. This also sets NIX_LF_RAS[IPSEC_DYNO_POISON]
                                                                 and NIX_AF_RAS[IPSEC_DYNO_POISON].

                                                                 Hardware does the following when a DYNO counter is all ones:
                                                                 * Disable IPSEC hardware fast-path for associated flows. Packets that would
                                                                 otherwise use the hardware fast-path are steered to the dynamically
                                                                 prevented path with NIX_WQE_HDR_S[WQE_TYPE] = NIX_XQE_TYPE_E::RX_IPSECD.
                                                                 * Suppress counter decrement by software and increment by hardware.

                                                                 Software store a value other than all ones to a DYNO counter to clear the
                                                                 error state. */
        uint64_t reserved_32_46        : 15;
        uint64_t storeop               : 1;  /**< [ 47: 47](WO) Store operation. Valid on a write to this register:
                                                                 0 = Decrement [COUNT] from the DYNO counter.
                                                                 1 = Store [COUNT] to the DYNO counter. */
        uint64_t dyno_sel              : 15; /**< [ 62: 48](WO) Selects DYNO counter within LF. This field is present on a write, or in the
                                                                 write data of an atomic load-and-add. Hardware uses the lower
                                                                 NIX_AF_LF()_RX_IPSEC_DYNO_CFG[DYNO_IDX_W] bits of this field and ignores
                                                                 upper bits, if any. */
        uint64_t reserved_63           : 1;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_op_ipsec_dyno_cnt_s cn; */
};
typedef union bdk_nixx_lf_op_ipsec_dyno_cnt bdk_nixx_lf_op_ipsec_dyno_cnt_t;

static inline uint64_t BDK_NIXX_LF_OP_IPSEC_DYNO_CNT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_OP_IPSEC_DYNO_CNT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400980ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_OP_IPSEC_DYNO_CNT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_OP_IPSEC_DYNO_CNT(a) bdk_nixx_lf_op_ipsec_dyno_cnt_t
#define bustype_BDK_NIXX_LF_OP_IPSEC_DYNO_CNT(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_OP_IPSEC_DYNO_CNT(a) "NIXX_LF_OP_IPSEC_DYNO_CNT"
#define device_bar_BDK_NIXX_LF_OP_IPSEC_DYNO_CNT(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_OP_IPSEC_DYNO_CNT(a) (a)
#define arguments_BDK_NIXX_LF_OP_IPSEC_DYNO_CNT(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_op_send#
 *
 * NIX LF Send Operation Registers
 * An atomic store or LMTST to this address enqueues one or more SQEs to a send
 * queue. NIX_SEND_HDR_S[SQ] in the first SQE selects the send queue.The maximum
 * size of each SQE is specified by NIX_SQ_CTX_S[MAX_SQE_SIZE].
 *
 * The endianness of the instruction write data is controlled by NIX_AF_LF()_CFG[BE].
 *
 * When a NIX_SEND_JUMP_S is not present in the SQE, the SQE consists of the
 * entire send descriptor.
 *
 * When a NIX_SEND_JUMP_S is present in the SQE, the SQE must contain exactly the
 * portion of the send descriptor up to and including the NIX_SEND_JUMP_S, and the
 * remainder of the send descriptor must be at IOVA NIX_SEND_JUMP_S[ADDR] in
 * LLC/DRAM.
 *
 * Software must ensure that all LLC/DRAM locations that will be referenced by NIX while
 * processing this descriptor, including all packet data and post-jump subdescriptors
 * contain the latest updates before issuing the LMTST. A DMB instruction may be required prior
 * to the LMTST to ensure this. A DMB following the LMTST may be useful if SQ descriptor ordering
 * matters and more than one CPU core is simultaneously enqueueing to the same SQ. For more
 * information on ordering, refer to the HRM "Core Memory Reference Ordering" section in the CPU
 * cores chapter.
 */
union bdk_nixx_lf_op_sendx
{
    uint64_t u;
    struct bdk_nixx_lf_op_sendx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t data                  : 64; /**< [ 63:  0](WO) Data that forms the send descriptor; see register description. */
#else /* Word 0 - Little Endian */
        uint64_t data                  : 64; /**< [ 63:  0](WO) Data that forms the send descriptor; see register description. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_op_sendx_s cn; */
};
typedef union bdk_nixx_lf_op_sendx bdk_nixx_lf_op_sendx_t;

static inline uint64_t BDK_NIXX_LF_OP_SENDX(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_OP_SENDX(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=15)))
        return 0x850200400800ll + 0x100000ll * ((a) & 0x0) + 8ll * ((b) & 0xf);
    __bdk_csr_fatal("NIXX_LF_OP_SENDX", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_OP_SENDX(a,b) bdk_nixx_lf_op_sendx_t
#define bustype_BDK_NIXX_LF_OP_SENDX(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_OP_SENDX(a,b) "NIXX_LF_OP_SENDX"
#define device_bar_BDK_NIXX_LF_OP_SENDX(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_OP_SENDX(a,b) (a)
#define arguments_BDK_NIXX_LF_OP_SENDX(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_qint#_cnt
 *
 * NIX LF Queue Interrupt Count Registers
 */
union bdk_nixx_lf_qintx_cnt
{
    uint64_t u;
    struct bdk_nixx_lf_qintx_cnt_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_22_63        : 42;
        uint64_t count                 : 22; /**< [ 21:  0](R/W/H) Interrupt count. Value in NIX_QINT_HW_S[COUNT]. Number of pending
                                                                 interrupts to this QINT from SQs/RQs/CQs in the LF. Hardware increments
                                                                 the counter when an interrupt is set and decrements it when an interrupt is
                                                                 cleared.
                                                                 Writes to this field are for diagnostic use only. */
#else /* Word 0 - Little Endian */
        uint64_t count                 : 22; /**< [ 21:  0](R/W/H) Interrupt count. Value in NIX_QINT_HW_S[COUNT]. Number of pending
                                                                 interrupts to this QINT from SQs/RQs/CQs in the LF. Hardware increments
                                                                 the counter when an interrupt is set and decrements it when an interrupt is
                                                                 cleared.
                                                                 Writes to this field are for diagnostic use only. */
        uint64_t reserved_22_63        : 42;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_qintx_cnt_s cn; */
};
typedef union bdk_nixx_lf_qintx_cnt bdk_nixx_lf_qintx_cnt_t;

static inline uint64_t BDK_NIXX_LF_QINTX_CNT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_QINTX_CNT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400c00ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_QINTX_CNT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_QINTX_CNT(a,b) bdk_nixx_lf_qintx_cnt_t
#define bustype_BDK_NIXX_LF_QINTX_CNT(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_QINTX_CNT(a,b) "NIXX_LF_QINTX_CNT"
#define device_bar_BDK_NIXX_LF_QINTX_CNT(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_QINTX_CNT(a,b) (a)
#define arguments_BDK_NIXX_LF_QINTX_CNT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_qint#_ena_w1c
 *
 * NIX LF Queue Interrupt Enable Clear Registers
 */
union bdk_nixx_lf_qintx_ena_w1c
{
    uint64_t u;
    struct bdk_nixx_lf_qintx_ena_w1c_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1C) Interrupt enable. Writing a one will clear NIX_QINT_HW_S[ENA].
                                                                 Writing a zero has no effect. A read will return the QINT enable bit. */
#else /* Word 0 - Little Endian */
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1C) Interrupt enable. Writing a one will clear NIX_QINT_HW_S[ENA].
                                                                 Writing a zero has no effect. A read will return the QINT enable bit. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_qintx_ena_w1c_s cn; */
};
typedef union bdk_nixx_lf_qintx_ena_w1c bdk_nixx_lf_qintx_ena_w1c_t;

static inline uint64_t BDK_NIXX_LF_QINTX_ENA_W1C(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_QINTX_ENA_W1C(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400c30ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_QINTX_ENA_W1C", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_QINTX_ENA_W1C(a,b) bdk_nixx_lf_qintx_ena_w1c_t
#define bustype_BDK_NIXX_LF_QINTX_ENA_W1C(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_QINTX_ENA_W1C(a,b) "NIXX_LF_QINTX_ENA_W1C"
#define device_bar_BDK_NIXX_LF_QINTX_ENA_W1C(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_QINTX_ENA_W1C(a,b) (a)
#define arguments_BDK_NIXX_LF_QINTX_ENA_W1C(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_qint#_ena_w1s
 *
 * NIX LF Queue Interrupt Enable Set Registers
 */
union bdk_nixx_lf_qintx_ena_w1s
{
    uint64_t u;
    struct bdk_nixx_lf_qintx_ena_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1S) Interrupt enable. Writing a one will set NIX_QINT_HW_S[ENA].
                                                                 Writing a zero has no effect. A read will return the QINT enable bit. */
#else /* Word 0 - Little Endian */
        uint64_t intr                  : 1;  /**< [  0:  0](R/W1S) Interrupt enable. Writing a one will set NIX_QINT_HW_S[ENA].
                                                                 Writing a zero has no effect. A read will return the QINT enable bit. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_qintx_ena_w1s_s cn; */
};
typedef union bdk_nixx_lf_qintx_ena_w1s bdk_nixx_lf_qintx_ena_w1s_t;

static inline uint64_t BDK_NIXX_LF_QINTX_ENA_W1S(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_QINTX_ENA_W1S(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400c20ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_QINTX_ENA_W1S", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_QINTX_ENA_W1S(a,b) bdk_nixx_lf_qintx_ena_w1s_t
#define bustype_BDK_NIXX_LF_QINTX_ENA_W1S(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_QINTX_ENA_W1S(a,b) "NIXX_LF_QINTX_ENA_W1S"
#define device_bar_BDK_NIXX_LF_QINTX_ENA_W1S(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_QINTX_ENA_W1S(a,b) (a)
#define arguments_BDK_NIXX_LF_QINTX_ENA_W1S(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_qint#_int
 *
 * NIX LF Queue Interrupt Registers
 */
union bdk_nixx_lf_qintx_int
{
    uint64_t u;
    struct bdk_nixx_lf_qintx_int_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t intr                  : 1;  /**< [  0:  0](RO/H) Interrupt pending. Set when NIX_LF_QINT()_CNT[COUNT] is non-zero. */
#else /* Word 0 - Little Endian */
        uint64_t intr                  : 1;  /**< [  0:  0](RO/H) Interrupt pending. Set when NIX_LF_QINT()_CNT[COUNT] is non-zero. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_qintx_int_s cn; */
};
typedef union bdk_nixx_lf_qintx_int bdk_nixx_lf_qintx_int_t;

static inline uint64_t BDK_NIXX_LF_QINTX_INT(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_QINTX_INT(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400c10ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_QINTX_INT", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_QINTX_INT(a,b) bdk_nixx_lf_qintx_int_t
#define bustype_BDK_NIXX_LF_QINTX_INT(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_QINTX_INT(a,b) "NIXX_LF_QINTX_INT"
#define device_bar_BDK_NIXX_LF_QINTX_INT(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_QINTX_INT(a,b) (a)
#define arguments_BDK_NIXX_LF_QINTX_INT(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_qint#_int_w1s
 *
 * INTERNAL: NIX LF Queue Interrupt Set Registers
 */
union bdk_nixx_lf_qintx_int_w1s
{
    uint64_t u;
    struct bdk_nixx_lf_qintx_int_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_1_63         : 63;
        uint64_t intr                  : 1;  /**< [  0:  0](RO/H) Interrupt pending. Set when NIX_LF_QINT()_CNT[COUNT] is non-zero. */
#else /* Word 0 - Little Endian */
        uint64_t intr                  : 1;  /**< [  0:  0](RO/H) Interrupt pending. Set when NIX_LF_QINT()_CNT[COUNT] is non-zero. */
        uint64_t reserved_1_63         : 63;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_qintx_int_w1s_s cn; */
};
typedef union bdk_nixx_lf_qintx_int_w1s bdk_nixx_lf_qintx_int_w1s_t;

static inline uint64_t BDK_NIXX_LF_QINTX_INT_W1S(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_QINTX_INT_W1S(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=63)))
        return 0x850200400c18ll + 0x100000ll * ((a) & 0x0) + 0x1000ll * ((b) & 0x3f);
    __bdk_csr_fatal("NIXX_LF_QINTX_INT_W1S", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_QINTX_INT_W1S(a,b) bdk_nixx_lf_qintx_int_w1s_t
#define bustype_BDK_NIXX_LF_QINTX_INT_W1S(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_QINTX_INT_W1S(a,b) "NIXX_LF_QINTX_INT_W1S"
#define device_bar_BDK_NIXX_LF_QINTX_INT_W1S(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_QINTX_INT_W1S(a,b) (a)
#define arguments_BDK_NIXX_LF_QINTX_INT_W1S(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_ras
 *
 * NIX LF RAS Interrupt Register
 */
union bdk_nixx_lf_ras
{
    uint64_t u;
    struct bdk_nixx_lf_ras_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t cint_poison           : 1;  /**< [ 10: 10](R/W1C/H) Poisoned data returned on NIX_CINT_HW_S read. Hardware also sets
                                                                 NIX_AF_RAS[CINT_POISON]. */
        uint64_t qint_poison           : 1;  /**< [  9:  9](R/W1C/H) Poisoned data returned on NIX_QINT_HW_S read. Hardware also sets
                                                                 NIX_AF_RAS[QINT_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1C/H) Poisoned data returned on packet data read for NIX_SEND_SG_S. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1C/H) Poisoned data returned on send descriptor read at or beyond
                                                                 NIX_SEND_JUMP_S[ADDR]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1C/H) Poisoned data returned on IPSEC dynamic ordering counter read. See
                                                                 NIX_AF_LF()_RX_IPSEC_CFG. Hardware also sets NIX_AF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1C/H) Poisoned data returned on NIX_RSSE_S read. Hardware also sets
                                                                 NIX_AF_RAS[RSSE_POISON]. */
        uint64_t reserved_4            : 1;
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1C/H) Poisoned data returned on NIX_CQ_CTX_S read. Hardware also sets
                                                                 NIX_AF_RAS[CQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1C/H) Poisoned data returned on NIX_RQ_CTX_S read. Hardware also sets
                                                                 NIX_AF_RAS[RQ_CTX_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1C/H) Poisoned data returned on NIX_SQ_CTX_HW_S read. Hardware also sets
                                                                 NIX_AF_RAS[SQ_CTX_POISON]. */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1C/H) Poisoned data returned on SQB read. Hardware also sets NIX_AF_RAS[SQB_POISON]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1C/H) Poisoned data returned on SQB read. Hardware also sets NIX_AF_RAS[SQB_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1C/H) Poisoned data returned on NIX_SQ_CTX_HW_S read. Hardware also sets
                                                                 NIX_AF_RAS[SQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1C/H) Poisoned data returned on NIX_RQ_CTX_S read. Hardware also sets
                                                                 NIX_AF_RAS[RQ_CTX_POISON]. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1C/H) Poisoned data returned on NIX_CQ_CTX_S read. Hardware also sets
                                                                 NIX_AF_RAS[CQ_CTX_POISON]. */
        uint64_t reserved_4            : 1;
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1C/H) Poisoned data returned on NIX_RSSE_S read. Hardware also sets
                                                                 NIX_AF_RAS[RSSE_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1C/H) Poisoned data returned on IPSEC dynamic ordering counter read. See
                                                                 NIX_AF_LF()_RX_IPSEC_CFG. Hardware also sets NIX_AF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1C/H) Poisoned data returned on send descriptor read at or beyond
                                                                 NIX_SEND_JUMP_S[ADDR]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1C/H) Poisoned data returned on packet data read for NIX_SEND_SG_S. */
        uint64_t qint_poison           : 1;  /**< [  9:  9](R/W1C/H) Poisoned data returned on NIX_QINT_HW_S read. Hardware also sets
                                                                 NIX_AF_RAS[QINT_POISON]. */
        uint64_t cint_poison           : 1;  /**< [ 10: 10](R/W1C/H) Poisoned data returned on NIX_CINT_HW_S read. Hardware also sets
                                                                 NIX_AF_RAS[CINT_POISON]. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_ras_s cn; */
};
typedef union bdk_nixx_lf_ras bdk_nixx_lf_ras_t;

static inline uint64_t BDK_NIXX_LF_RAS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_RAS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400240ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_RAS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_RAS(a) bdk_nixx_lf_ras_t
#define bustype_BDK_NIXX_LF_RAS(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_RAS(a) "NIXX_LF_RAS"
#define device_bar_BDK_NIXX_LF_RAS(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_RAS(a) (a)
#define arguments_BDK_NIXX_LF_RAS(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_ras_ena_w1c
 *
 * NIX LF RAS Interrupt Enable Clear Register
 * This register clears interrupt enable bits.
 */
union bdk_nixx_lf_ras_ena_w1c
{
    uint64_t u;
    struct bdk_nixx_lf_ras_ena_w1c_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t cint_poison           : 1;  /**< [ 10: 10](R/W1C/H) Reads or clears enable for NIX_LF_RAS[CINT_POISON]. */
        uint64_t qint_poison           : 1;  /**< [  9:  9](R/W1C/H) Reads or clears enable for NIX_LF_RAS[QINT_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1C/H) Reads or clears enable for NIX_LF_RAS[SEND_SG_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1C/H) Reads or clears enable for NIX_LF_RAS[SEND_JUMP_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1C/H) Reads or clears enable for NIX_LF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1C/H) Reads or clears enable for NIX_LF_RAS[RSSE_POISON]. */
        uint64_t reserved_4            : 1;
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1C/H) Reads or clears enable for NIX_LF_RAS[CQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1C/H) Reads or clears enable for NIX_LF_RAS[RQ_CTX_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_LF_RAS[SQ_CTX_POISON]. */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_LF_RAS[SQB_POISON]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1C/H) Reads or clears enable for NIX_LF_RAS[SQB_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1C/H) Reads or clears enable for NIX_LF_RAS[SQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1C/H) Reads or clears enable for NIX_LF_RAS[RQ_CTX_POISON]. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1C/H) Reads or clears enable for NIX_LF_RAS[CQ_CTX_POISON]. */
        uint64_t reserved_4            : 1;
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1C/H) Reads or clears enable for NIX_LF_RAS[RSSE_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1C/H) Reads or clears enable for NIX_LF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1C/H) Reads or clears enable for NIX_LF_RAS[SEND_JUMP_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1C/H) Reads or clears enable for NIX_LF_RAS[SEND_SG_POISON]. */
        uint64_t qint_poison           : 1;  /**< [  9:  9](R/W1C/H) Reads or clears enable for NIX_LF_RAS[QINT_POISON]. */
        uint64_t cint_poison           : 1;  /**< [ 10: 10](R/W1C/H) Reads or clears enable for NIX_LF_RAS[CINT_POISON]. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_ras_ena_w1c_s cn; */
};
typedef union bdk_nixx_lf_ras_ena_w1c bdk_nixx_lf_ras_ena_w1c_t;

static inline uint64_t BDK_NIXX_LF_RAS_ENA_W1C(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_RAS_ENA_W1C(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400250ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_RAS_ENA_W1C", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_RAS_ENA_W1C(a) bdk_nixx_lf_ras_ena_w1c_t
#define bustype_BDK_NIXX_LF_RAS_ENA_W1C(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_RAS_ENA_W1C(a) "NIXX_LF_RAS_ENA_W1C"
#define device_bar_BDK_NIXX_LF_RAS_ENA_W1C(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_RAS_ENA_W1C(a) (a)
#define arguments_BDK_NIXX_LF_RAS_ENA_W1C(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_ras_ena_w1s
 *
 * NIX LF RAS Interrupt Enable Set Register
 * This register sets interrupt enable bits.
 */
union bdk_nixx_lf_ras_ena_w1s
{
    uint64_t u;
    struct bdk_nixx_lf_ras_ena_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t cint_poison           : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets enable for NIX_LF_RAS[CINT_POISON]. */
        uint64_t qint_poison           : 1;  /**< [  9:  9](R/W1S/H) Reads or sets enable for NIX_LF_RAS[QINT_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1S/H) Reads or sets enable for NIX_LF_RAS[SEND_SG_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1S/H) Reads or sets enable for NIX_LF_RAS[SEND_JUMP_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1S/H) Reads or sets enable for NIX_LF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1S/H) Reads or sets enable for NIX_LF_RAS[RSSE_POISON]. */
        uint64_t reserved_4            : 1;
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1S/H) Reads or sets enable for NIX_LF_RAS[CQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1S/H) Reads or sets enable for NIX_LF_RAS[RQ_CTX_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_LF_RAS[SQ_CTX_POISON]. */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_LF_RAS[SQB_POISON]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1S/H) Reads or sets enable for NIX_LF_RAS[SQB_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1S/H) Reads or sets enable for NIX_LF_RAS[SQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1S/H) Reads or sets enable for NIX_LF_RAS[RQ_CTX_POISON]. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1S/H) Reads or sets enable for NIX_LF_RAS[CQ_CTX_POISON]. */
        uint64_t reserved_4            : 1;
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1S/H) Reads or sets enable for NIX_LF_RAS[RSSE_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1S/H) Reads or sets enable for NIX_LF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1S/H) Reads or sets enable for NIX_LF_RAS[SEND_JUMP_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1S/H) Reads or sets enable for NIX_LF_RAS[SEND_SG_POISON]. */
        uint64_t qint_poison           : 1;  /**< [  9:  9](R/W1S/H) Reads or sets enable for NIX_LF_RAS[QINT_POISON]. */
        uint64_t cint_poison           : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets enable for NIX_LF_RAS[CINT_POISON]. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_ras_ena_w1s_s cn; */
};
typedef union bdk_nixx_lf_ras_ena_w1s bdk_nixx_lf_ras_ena_w1s_t;

static inline uint64_t BDK_NIXX_LF_RAS_ENA_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_RAS_ENA_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400258ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_RAS_ENA_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_RAS_ENA_W1S(a) bdk_nixx_lf_ras_ena_w1s_t
#define bustype_BDK_NIXX_LF_RAS_ENA_W1S(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_RAS_ENA_W1S(a) "NIXX_LF_RAS_ENA_W1S"
#define device_bar_BDK_NIXX_LF_RAS_ENA_W1S(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_RAS_ENA_W1S(a) (a)
#define arguments_BDK_NIXX_LF_RAS_ENA_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_ras_w1s
 *
 * NIX LF RAS Interrupt Set Register
 * This register sets interrupt bits.
 */
union bdk_nixx_lf_ras_w1s
{
    uint64_t u;
    struct bdk_nixx_lf_ras_w1s_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_11_63        : 53;
        uint64_t cint_poison           : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets NIX_LF_RAS[CINT_POISON]. */
        uint64_t qint_poison           : 1;  /**< [  9:  9](R/W1S/H) Reads or sets NIX_LF_RAS[QINT_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1S/H) Reads or sets NIX_LF_RAS[SEND_SG_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1S/H) Reads or sets NIX_LF_RAS[SEND_JUMP_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1S/H) Reads or sets NIX_LF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1S/H) Reads or sets NIX_LF_RAS[RSSE_POISON]. */
        uint64_t reserved_4            : 1;
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1S/H) Reads or sets NIX_LF_RAS[CQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1S/H) Reads or sets NIX_LF_RAS[RQ_CTX_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_LF_RAS[SQ_CTX_POISON]. */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_LF_RAS[SQB_POISON]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_poison            : 1;  /**< [  0:  0](R/W1S/H) Reads or sets NIX_LF_RAS[SQB_POISON]. */
        uint64_t sq_ctx_poison         : 1;  /**< [  1:  1](R/W1S/H) Reads or sets NIX_LF_RAS[SQ_CTX_POISON]. */
        uint64_t rq_ctx_poison         : 1;  /**< [  2:  2](R/W1S/H) Reads or sets NIX_LF_RAS[RQ_CTX_POISON]. */
        uint64_t cq_ctx_poison         : 1;  /**< [  3:  3](R/W1S/H) Reads or sets NIX_LF_RAS[CQ_CTX_POISON]. */
        uint64_t reserved_4            : 1;
        uint64_t rsse_poison           : 1;  /**< [  5:  5](R/W1S/H) Reads or sets NIX_LF_RAS[RSSE_POISON]. */
        uint64_t ipsec_dyno_poison     : 1;  /**< [  6:  6](R/W1S/H) Reads or sets NIX_LF_RAS[IPSEC_DYNO_POISON]. */
        uint64_t send_jump_poison      : 1;  /**< [  7:  7](R/W1S/H) Reads or sets NIX_LF_RAS[SEND_JUMP_POISON]. */
        uint64_t send_sg_poison        : 1;  /**< [  8:  8](R/W1S/H) Reads or sets NIX_LF_RAS[SEND_SG_POISON]. */
        uint64_t qint_poison           : 1;  /**< [  9:  9](R/W1S/H) Reads or sets NIX_LF_RAS[QINT_POISON]. */
        uint64_t cint_poison           : 1;  /**< [ 10: 10](R/W1S/H) Reads or sets NIX_LF_RAS[CINT_POISON]. */
        uint64_t reserved_11_63        : 53;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_ras_w1s_s cn; */
};
typedef union bdk_nixx_lf_ras_w1s bdk_nixx_lf_ras_w1s_t;

static inline uint64_t BDK_NIXX_LF_RAS_W1S(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_RAS_W1S(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400248ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_RAS_W1S", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_RAS_W1S(a) bdk_nixx_lf_ras_w1s_t
#define bustype_BDK_NIXX_LF_RAS_W1S(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_RAS_W1S(a) "NIXX_LF_RAS_W1S"
#define device_bar_BDK_NIXX_LF_RAS_W1S(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_RAS_W1S(a) (a)
#define arguments_BDK_NIXX_LF_RAS_W1S(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_rq_op_int
 *
 * NIX LF Receive Queue Interrupt Operation Register
 * A 64-bit atomic load-and-add to this register reads RQ interrupts and
 * interrupt enables.
 * A 64-bit write optionally sets or clears interrupts and interrupt enables.
 *
 * All other accesses to this register (e.g. reads, 128-bit accesses) are RAZ/WI.
 */
union bdk_nixx_lf_rq_op_int
{
    uint64_t u;
    struct bdk_nixx_lf_rq_op_int_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t rq                    : 20; /**< [ 63: 44](WO) RQ within LF. This field is present on a write, or in the write data
                                                                 of an atomic load-and-add. */
        uint64_t setop                 : 1;  /**< [ 43: 43](WO) Set Operation. Valid on a write. Indicates write-one-to-set when set,
                                                                 write-one-to-clear when clear. */
        uint64_t op_err                : 1;  /**< [ 42: 42](RO/H) Operation error. Remaining read data fields are not valid. One of the
                                                                 following (may not be exhaustive):
                                                                 * Memory fault on NIX_RQ_CTX_S read; also sets NIX_LF_ERR_INT[RQ_CTX_FAULT].
                                                                 * Poisoned data returned on NIX_RQ_CTX_S read; also sets
                                                                 NIX_LF_RAS[RQ_CTX_POISON] and NIX_AF_RAS[RQ_CTX_POISON] .
                                                                 * Access to out-of-range RQ ([RQ] \> NIX_AF_LF()_RQS_CFG[MAX_QUEUESM1]);
                                                                 also sets NIX_LF_ERR_INT[RQ_OOR].
                                                                 * Disabled RQ (NIX_RQ_CTX_S[ENA] = 0); also sets
                                                                 NIX_LF_ERR_INT[RQ_DISABLED]. */
        uint64_t reserved_16_41        : 26;
        uint64_t rq_int_ena            : 8;  /**< [ 15:  8](R/W/H) Returns NIX_RQ_CTX_S[RQ_INT_ENA] on an atomic load-and-add. On a write,
                                                                 write-one-to-set NIX_RQ_CTX_S[RQ_INT_ENA] if [SETOP] is set, write-one-to-clear
                                                                 otherwise. */
        uint64_t rq_int                : 8;  /**< [  7:  0](R/W/H) Returns NIX_RQ_CTX_S[RQ_INT] on an atomic load-and-add. On a write,
                                                                 write-one-to-set NIX_RQ_CTX_S[RQ_INT] if [SETOP] is set, write-one-to-clear
                                                                 otherwise. */
#else /* Word 0 - Little Endian */
        uint64_t rq_int                : 8;  /**< [  7:  0](R/W/H) Returns NIX_RQ_CTX_S[RQ_INT] on an atomic load-and-add. On a write,
                                                                 write-one-to-set NIX_RQ_CTX_S[RQ_INT] if [SETOP] is set, write-one-to-clear
                                                                 otherwise. */
        uint64_t rq_int_ena            : 8;  /**< [ 15:  8](R/W/H) Returns NIX_RQ_CTX_S[RQ_INT_ENA] on an atomic load-and-add. On a write,
                                                                 write-one-to-set NIX_RQ_CTX_S[RQ_INT_ENA] if [SETOP] is set, write-one-to-clear
                                                                 otherwise. */
        uint64_t reserved_16_41        : 26;
        uint64_t op_err                : 1;  /**< [ 42: 42](RO/H) Operation error. Remaining read data fields are not valid. One of the
                                                                 following (may not be exhaustive):
                                                                 * Memory fault on NIX_RQ_CTX_S read; also sets NIX_LF_ERR_INT[RQ_CTX_FAULT].
                                                                 * Poisoned data returned on NIX_RQ_CTX_S read; also sets
                                                                 NIX_LF_RAS[RQ_CTX_POISON] and NIX_AF_RAS[RQ_CTX_POISON] .
                                                                 * Access to out-of-range RQ ([RQ] \> NIX_AF_LF()_RQS_CFG[MAX_QUEUESM1]);
                                                                 also sets NIX_LF_ERR_INT[RQ_OOR].
                                                                 * Disabled RQ (NIX_RQ_CTX_S[ENA] = 0); also sets
                                                                 NIX_LF_ERR_INT[RQ_DISABLED]. */
        uint64_t setop                 : 1;  /**< [ 43: 43](WO) Set Operation. Valid on a write. Indicates write-one-to-set when set,
                                                                 write-one-to-clear when clear. */
        uint64_t rq                    : 20; /**< [ 63: 44](WO) RQ within LF. This field is present on a write, or in the write data
                                                                 of an atomic load-and-add. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_rq_op_int_s cn; */
};
typedef union bdk_nixx_lf_rq_op_int bdk_nixx_lf_rq_op_int_t;

static inline uint64_t BDK_NIXX_LF_RQ_OP_INT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_RQ_OP_INT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400900ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_RQ_OP_INT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_RQ_OP_INT(a) bdk_nixx_lf_rq_op_int_t
#define bustype_BDK_NIXX_LF_RQ_OP_INT(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_RQ_OP_INT(a) "NIXX_LF_RQ_OP_INT"
#define device_bar_BDK_NIXX_LF_RQ_OP_INT(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_RQ_OP_INT(a) (a)
#define arguments_BDK_NIXX_LF_RQ_OP_INT(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_rq_op_octs
 *
 * NIX LF Receive Queue Octets Operation Register
 * A 64-bit atomic load-and-add to this register reads NIX_RQ_CTX_S[OCTS]. The atomic
 * write data has format NIX_OP_Q_WDATA_S and selects the RQ within LF.
 *
 * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
 */
union bdk_nixx_lf_rq_op_octs
{
    uint64_t u;
    struct bdk_nixx_lf_rq_op_octs_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_RQ_OP_INT[OP_ERR]. */
        uint64_t reserved_48_62        : 15;
        uint64_t cnt                   : 48; /**< [ 47:  0](RO) Count. */
#else /* Word 0 - Little Endian */
        uint64_t cnt                   : 48; /**< [ 47:  0](RO) Count. */
        uint64_t reserved_48_62        : 15;
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_RQ_OP_INT[OP_ERR]. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_rq_op_octs_s cn; */
};
typedef union bdk_nixx_lf_rq_op_octs bdk_nixx_lf_rq_op_octs_t;

static inline uint64_t BDK_NIXX_LF_RQ_OP_OCTS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_RQ_OP_OCTS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400910ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_RQ_OP_OCTS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_RQ_OP_OCTS(a) bdk_nixx_lf_rq_op_octs_t
#define bustype_BDK_NIXX_LF_RQ_OP_OCTS(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_RQ_OP_OCTS(a) "NIXX_LF_RQ_OP_OCTS"
#define device_bar_BDK_NIXX_LF_RQ_OP_OCTS(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_RQ_OP_OCTS(a) (a)
#define arguments_BDK_NIXX_LF_RQ_OP_OCTS(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_rq_op_pkts
 *
 * NIX LF Receive Queue Packets Operation Register
 * A 64-bit atomic load-and-add to this register reads NIX_RQ_CTX_S[PKTS]. The atomic
 * write data has format NIX_OP_Q_WDATA_S and selects the RQ within LF.
 *
 * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
 */
union bdk_nixx_lf_rq_op_pkts
{
    uint64_t u;
    struct bdk_nixx_lf_rq_op_pkts_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_RQ_OP_INT[OP_ERR]. */
        uint64_t reserved_48_62        : 15;
        uint64_t cnt                   : 48; /**< [ 47:  0](RO) Count. */
#else /* Word 0 - Little Endian */
        uint64_t cnt                   : 48; /**< [ 47:  0](RO) Count. */
        uint64_t reserved_48_62        : 15;
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_RQ_OP_INT[OP_ERR]. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_rq_op_pkts_s cn; */
};
typedef union bdk_nixx_lf_rq_op_pkts bdk_nixx_lf_rq_op_pkts_t;

static inline uint64_t BDK_NIXX_LF_RQ_OP_PKTS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_RQ_OP_PKTS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400920ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_RQ_OP_PKTS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_RQ_OP_PKTS(a) bdk_nixx_lf_rq_op_pkts_t
#define bustype_BDK_NIXX_LF_RQ_OP_PKTS(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_RQ_OP_PKTS(a) "NIXX_LF_RQ_OP_PKTS"
#define device_bar_BDK_NIXX_LF_RQ_OP_PKTS(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_RQ_OP_PKTS(a) (a)
#define arguments_BDK_NIXX_LF_RQ_OP_PKTS(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_rx_secret#
 *
 * NIX LF Receive Secret Key Registers
 */
union bdk_nixx_lf_rx_secretx
{
    uint64_t u;
    struct bdk_nixx_lf_rx_secretx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t key                   : 64; /**< [ 63:  0](R/W) 64 bits of the 352-bit VF secret key for flow key hashing. Software must set
                                                                 this to a random secret value shared with the driver. If bidirectional flows are
                                                                 desired to land on the same queue, then the key must be selected to have the 32
                                                                 bits that start at each IP/port source bit key position match the 32 bits at the
                                                                 corresponding IP/port destination key bit position, in descending bit order.

                                                                 The FLOW_KEY\<319:0\> is generated for the packet as described in
                                                                 NIX_AF_RX_FLOW_KEY_ALG()_FIELD(). The following hash function computes FLOW_TAG\<31:0\>
                                                                 from FLOW_KEY:
                                                                 \<pre\>
                                                                   uint32_t nix_rx_flow_tag_hash(bit_array[319:0] FLOW_KEY) {
                                                                      uint32_t FLOW_TAG = 0;
                                                                      bit_array[351:0] secret;
                                                                      secret[351:288] = NIX_LF_RX_SECRET(0);
                                                                      secret[287:224] = NIX_LF_RX_SECRET(1);
                                                                      secret[223:160] = NIX_LF_RX_SECRET(2);
                                                                      secret[159:96] = NIX_LF_RX_SECRET(3);
                                                                      secret[95:32] = NIX_LF_RX_SECRET(4);
                                                                      secret[31:0] = NIX_LF_RX_SECRET(5)[KEY]\<63:32\>;
                                                                      // Note NIX_LF_RX_SECRET(5)[KEY]\<31:0\> bits are unused.
                                                                      secret_bit = 351;
                                                                      foreach bit_value (FLOW_KEY) { // MSB processed first
                                                                         assert(secret_bit\>=31);
                                                                         if (bit_value) {
                                                                            FLOW_TAG ^= secret[secret_bit -: 32];
                                                                         }
                                                                         secret_bit--;
                                                                      }
                                                                      return FLOW_TAG;
                                                                   }
                                                                 \</pre\> */
#else /* Word 0 - Little Endian */
        uint64_t key                   : 64; /**< [ 63:  0](R/W) 64 bits of the 352-bit VF secret key for flow key hashing. Software must set
                                                                 this to a random secret value shared with the driver. If bidirectional flows are
                                                                 desired to land on the same queue, then the key must be selected to have the 32
                                                                 bits that start at each IP/port source bit key position match the 32 bits at the
                                                                 corresponding IP/port destination key bit position, in descending bit order.

                                                                 The FLOW_KEY\<319:0\> is generated for the packet as described in
                                                                 NIX_AF_RX_FLOW_KEY_ALG()_FIELD(). The following hash function computes FLOW_TAG\<31:0\>
                                                                 from FLOW_KEY:
                                                                 \<pre\>
                                                                   uint32_t nix_rx_flow_tag_hash(bit_array[319:0] FLOW_KEY) {
                                                                      uint32_t FLOW_TAG = 0;
                                                                      bit_array[351:0] secret;
                                                                      secret[351:288] = NIX_LF_RX_SECRET(0);
                                                                      secret[287:224] = NIX_LF_RX_SECRET(1);
                                                                      secret[223:160] = NIX_LF_RX_SECRET(2);
                                                                      secret[159:96] = NIX_LF_RX_SECRET(3);
                                                                      secret[95:32] = NIX_LF_RX_SECRET(4);
                                                                      secret[31:0] = NIX_LF_RX_SECRET(5)[KEY]\<63:32\>;
                                                                      // Note NIX_LF_RX_SECRET(5)[KEY]\<31:0\> bits are unused.
                                                                      secret_bit = 351;
                                                                      foreach bit_value (FLOW_KEY) { // MSB processed first
                                                                         assert(secret_bit\>=31);
                                                                         if (bit_value) {
                                                                            FLOW_TAG ^= secret[secret_bit -: 32];
                                                                         }
                                                                         secret_bit--;
                                                                      }
                                                                      return FLOW_TAG;
                                                                   }
                                                                 \</pre\> */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_rx_secretx_s cn; */
};
typedef union bdk_nixx_lf_rx_secretx bdk_nixx_lf_rx_secretx_t;

static inline uint64_t BDK_NIXX_LF_RX_SECRETX(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_RX_SECRETX(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=5)))
        return 0x850200400000ll + 0x100000ll * ((a) & 0x0) + 8ll * ((b) & 0x7);
    __bdk_csr_fatal("NIXX_LF_RX_SECRETX", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_RX_SECRETX(a,b) bdk_nixx_lf_rx_secretx_t
#define bustype_BDK_NIXX_LF_RX_SECRETX(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_RX_SECRETX(a,b) "NIXX_LF_RX_SECRETX"
#define device_bar_BDK_NIXX_LF_RX_SECRETX(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_RX_SECRETX(a,b) (a)
#define arguments_BDK_NIXX_LF_RX_SECRETX(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_rx_stat#
 *
 * NIX LF Receive Statistics Registers
 * The last dimension indicates which statistic, and is enumerated by NIX_STAT_LF_RX_E.
 */
union bdk_nixx_lf_rx_statx
{
    uint64_t u;
    struct bdk_nixx_lf_rx_statx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t stat                  : 48; /**< [ 47:  0](RO/H) Statistic value. See also NIX_AF_LF()_RX_STAT() for a writable alias of this field. */
#else /* Word 0 - Little Endian */
        uint64_t stat                  : 48; /**< [ 47:  0](RO/H) Statistic value. See also NIX_AF_LF()_RX_STAT() for a writable alias of this field. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_rx_statx_s cn; */
};
typedef union bdk_nixx_lf_rx_statx bdk_nixx_lf_rx_statx_t;

static inline uint64_t BDK_NIXX_LF_RX_STATX(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_RX_STATX(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=11)))
        return 0x850200400400ll + 0x100000ll * ((a) & 0x0) + 8ll * ((b) & 0xf);
    __bdk_csr_fatal("NIXX_LF_RX_STATX", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_RX_STATX(a,b) bdk_nixx_lf_rx_statx_t
#define bustype_BDK_NIXX_LF_RX_STATX(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_RX_STATX(a,b) "NIXX_LF_RX_STATX"
#define device_bar_BDK_NIXX_LF_RX_STATX(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_RX_STATX(a,b) (a)
#define arguments_BDK_NIXX_LF_RX_STATX(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_send_err_dbg
 *
 * NIX LF Send Error Debug Register
 * This register captures debug info an error detected on packet send after a
 * meta-descriptor is granted by PSE.
 * Hardware sets [VALID] when the debug info is captured, and subsequent errors
 * are not captured until software clears [VALID] by writing a one to it.
 */
union bdk_nixx_lf_send_err_dbg
{
    uint64_t u;
    struct bdk_nixx_lf_send_err_dbg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_45_63        : 19;
        uint64_t valid                 : 1;  /**< [ 44: 44](R/W1C/H) Debug info valid. When set, remaining fields in this register are valid. */
        uint64_t sqe_id                : 16; /**< [ 43: 28](RO/H) SQE identifier from NIX_SEND_HDR_S[SQE_ID]. */
        uint64_t sq                    : 20; /**< [ 27:  8](RO/H) SQ within LF. */
        uint64_t errcode               : 8;  /**< [  7:  0](RO/H) Error code enumerated by NIX_SEND_STATUS_E. */
#else /* Word 0 - Little Endian */
        uint64_t errcode               : 8;  /**< [  7:  0](RO/H) Error code enumerated by NIX_SEND_STATUS_E. */
        uint64_t sq                    : 20; /**< [ 27:  8](RO/H) SQ within LF. */
        uint64_t sqe_id                : 16; /**< [ 43: 28](RO/H) SQE identifier from NIX_SEND_HDR_S[SQE_ID]. */
        uint64_t valid                 : 1;  /**< [ 44: 44](R/W1C/H) Debug info valid. When set, remaining fields in this register are valid. */
        uint64_t reserved_45_63        : 19;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_send_err_dbg_s cn; */
};
typedef union bdk_nixx_lf_send_err_dbg bdk_nixx_lf_send_err_dbg_t;

static inline uint64_t BDK_NIXX_LF_SEND_ERR_DBG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_SEND_ERR_DBG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400280ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_SEND_ERR_DBG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_SEND_ERR_DBG(a) bdk_nixx_lf_send_err_dbg_t
#define bustype_BDK_NIXX_LF_SEND_ERR_DBG(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_SEND_ERR_DBG(a) "NIXX_LF_SEND_ERR_DBG"
#define device_bar_BDK_NIXX_LF_SEND_ERR_DBG(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_SEND_ERR_DBG(a) (a)
#define arguments_BDK_NIXX_LF_SEND_ERR_DBG(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_int
 *
 * NIX LF Send Queue Interrupt Operation Register
 * A 64-bit atomic load-and-add to this register reads SQ interrupts,
 * interrupt enables and XOFF status.
 * A write optionally sets or clears interrupts, interrupt enables and XOFF
 * status.
 */
union bdk_nixx_lf_sq_op_int
{
    uint64_t u;
    struct bdk_nixx_lf_sq_op_int_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t sq                    : 20; /**< [ 63: 44](WO) SQ within LF. This field is present on a write, or in the write data
                                                                 of an atomic load-and-add. */
        uint64_t setop                 : 1;  /**< [ 43: 43](WO) Set operation. Valid on a write. Indicates write-one-to-set when set,
                                                                 write-one-to-clear when clear. */
        uint64_t op_err                : 1;  /**< [ 42: 42](RO/H) Operation error. Remaining read data fields are not valid. One of the
                                                                 following (may not be exhaustive):
                                                                 * Memory fault on NIX_SQ_CTX_S read; also sets NIX_LF_ERR_INT[SQ_CTX_FAULT].
                                                                 * Poisoned data returned on NIX_SQ_CTX_S read; also sets
                                                                 NIX_LF_RAS[SQ_CTX_POISON] and NIX_AF_RAS[SQ_CTX_POISON] .
                                                                 * Access to out-of-range SQ ([SQ] \> NIX_AF_LF()_SQS_CFG[MAX_QUEUESM1]);
                                                                 also sets NIX_LF_ERR_INT[SQ_OOR].
                                                                 * Disabled SQ (NIX_SQ_CTX_S[ENA] = 0); also sets
                                                                 NIX_LF_ERR_INT[SQ_DISABLED]. */
        uint64_t reserved_17_41        : 25;
        uint64_t xoff                  : 1;  /**< [ 16: 16](R/W/H) Returns NIX_SQ_CTX_S[XOFF] on an atomic load-and-add. On a write,
                                                                 write-one-to-set NIX_SQ_CTX_S[XOFF] if [SETOP] is set,
                                                                 write-one-to-clear otherwise. */
        uint64_t sq_int_ena            : 8;  /**< [ 15:  8](R/W/H) Returns NIX_SQ_CTX_S[SQ_INT_ENA] on an atomic load-and-add. On a write,
                                                                 write-one-to-set NIX_SQ_CTX_S[SQ_INT_ENA] if [SETOP] is set, write-one-to-clear
                                                                 otherwise. */
        uint64_t sq_int                : 8;  /**< [  7:  0](R/W/H) Returns NIX_SQ_CTX_S[SQ_INT] on an atomic load-and-add. On a write,
                                                                 write-one-to-set NIX_SQ_CTX_S[SQ_INT] if [SETOP] is set, write-one-to-clear
                                                                 otherwise. */
#else /* Word 0 - Little Endian */
        uint64_t sq_int                : 8;  /**< [  7:  0](R/W/H) Returns NIX_SQ_CTX_S[SQ_INT] on an atomic load-and-add. On a write,
                                                                 write-one-to-set NIX_SQ_CTX_S[SQ_INT] if [SETOP] is set, write-one-to-clear
                                                                 otherwise. */
        uint64_t sq_int_ena            : 8;  /**< [ 15:  8](R/W/H) Returns NIX_SQ_CTX_S[SQ_INT_ENA] on an atomic load-and-add. On a write,
                                                                 write-one-to-set NIX_SQ_CTX_S[SQ_INT_ENA] if [SETOP] is set, write-one-to-clear
                                                                 otherwise. */
        uint64_t xoff                  : 1;  /**< [ 16: 16](R/W/H) Returns NIX_SQ_CTX_S[XOFF] on an atomic load-and-add. On a write,
                                                                 write-one-to-set NIX_SQ_CTX_S[XOFF] if [SETOP] is set,
                                                                 write-one-to-clear otherwise. */
        uint64_t reserved_17_41        : 25;
        uint64_t op_err                : 1;  /**< [ 42: 42](RO/H) Operation error. Remaining read data fields are not valid. One of the
                                                                 following (may not be exhaustive):
                                                                 * Memory fault on NIX_SQ_CTX_S read; also sets NIX_LF_ERR_INT[SQ_CTX_FAULT].
                                                                 * Poisoned data returned on NIX_SQ_CTX_S read; also sets
                                                                 NIX_LF_RAS[SQ_CTX_POISON] and NIX_AF_RAS[SQ_CTX_POISON] .
                                                                 * Access to out-of-range SQ ([SQ] \> NIX_AF_LF()_SQS_CFG[MAX_QUEUESM1]);
                                                                 also sets NIX_LF_ERR_INT[SQ_OOR].
                                                                 * Disabled SQ (NIX_SQ_CTX_S[ENA] = 0); also sets
                                                                 NIX_LF_ERR_INT[SQ_DISABLED]. */
        uint64_t setop                 : 1;  /**< [ 43: 43](WO) Set operation. Valid on a write. Indicates write-one-to-set when set,
                                                                 write-one-to-clear when clear. */
        uint64_t sq                    : 20; /**< [ 63: 44](WO) SQ within LF. This field is present on a write, or in the write data
                                                                 of an atomic load-and-add. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_sq_op_int_s cn; */
};
typedef union bdk_nixx_lf_sq_op_int bdk_nixx_lf_sq_op_int_t;

static inline uint64_t BDK_NIXX_LF_SQ_OP_INT(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_SQ_OP_INT(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400a00ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_SQ_OP_INT", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_SQ_OP_INT(a) bdk_nixx_lf_sq_op_int_t
#define bustype_BDK_NIXX_LF_SQ_OP_INT(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_SQ_OP_INT(a) "NIXX_LF_SQ_OP_INT"
#define device_bar_BDK_NIXX_LF_SQ_OP_INT(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_SQ_OP_INT(a) (a)
#define arguments_BDK_NIXX_LF_SQ_OP_INT(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_octs
 *
 * NIX LF Send Queue Octets Operation Register
 * A 64-bit atomic load-and-add to this register reads NIX_SQ_CTX_S[OCTS]. The atomic
 * write data has format NIX_OP_Q_WDATA_S and selects the SQ within LF.
 *
 * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
 */
union bdk_nixx_lf_sq_op_octs
{
    uint64_t u;
    struct bdk_nixx_lf_sq_op_octs_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_SQ_OP_INT[OP_ERR]. */
        uint64_t reserved_48_62        : 15;
        uint64_t cnt                   : 48; /**< [ 47:  0](RO) Count. */
#else /* Word 0 - Little Endian */
        uint64_t cnt                   : 48; /**< [ 47:  0](RO) Count. */
        uint64_t reserved_48_62        : 15;
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_SQ_OP_INT[OP_ERR]. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_sq_op_octs_s cn; */
};
typedef union bdk_nixx_lf_sq_op_octs bdk_nixx_lf_sq_op_octs_t;

static inline uint64_t BDK_NIXX_LF_SQ_OP_OCTS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_SQ_OP_OCTS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400a10ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_SQ_OP_OCTS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_SQ_OP_OCTS(a) bdk_nixx_lf_sq_op_octs_t
#define bustype_BDK_NIXX_LF_SQ_OP_OCTS(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_SQ_OP_OCTS(a) "NIXX_LF_SQ_OP_OCTS"
#define device_bar_BDK_NIXX_LF_SQ_OP_OCTS(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_SQ_OP_OCTS(a) (a)
#define arguments_BDK_NIXX_LF_SQ_OP_OCTS(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_pkts
 *
 * NIX LF Send Queue Packets Operation Register
 * A 64-bit atomic load-and-add to this register reads NIX_SQ_CTX_S[PKTS]. The atomic
 * write data has format NIX_OP_Q_WDATA_S and selects the SQ within LF.
 *
 * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
 */
union bdk_nixx_lf_sq_op_pkts
{
    uint64_t u;
    struct bdk_nixx_lf_sq_op_pkts_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_RQ_OP_INT[OP_ERR]. */
        uint64_t reserved_48_62        : 15;
        uint64_t cnt                   : 48; /**< [ 47:  0](RO) Count. */
#else /* Word 0 - Little Endian */
        uint64_t cnt                   : 48; /**< [ 47:  0](RO) Count. */
        uint64_t reserved_48_62        : 15;
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_RQ_OP_INT[OP_ERR]. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_sq_op_pkts_s cn; */
};
typedef union bdk_nixx_lf_sq_op_pkts bdk_nixx_lf_sq_op_pkts_t;

static inline uint64_t BDK_NIXX_LF_SQ_OP_PKTS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_SQ_OP_PKTS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400a20ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_SQ_OP_PKTS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_SQ_OP_PKTS(a) bdk_nixx_lf_sq_op_pkts_t
#define bustype_BDK_NIXX_LF_SQ_OP_PKTS(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_SQ_OP_PKTS(a) "NIXX_LF_SQ_OP_PKTS"
#define device_bar_BDK_NIXX_LF_SQ_OP_PKTS(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_SQ_OP_PKTS(a) (a)
#define arguments_BDK_NIXX_LF_SQ_OP_PKTS(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_sq_op_status
 *
 * NIX LF Send Queue Status Operation Register
 * A 64-bit atomic load-and-add to this register reads status fields in
 * NIX_SQ_CTX_S. The atomic write data has format NIX_OP_Q_WDATA_S and
 * selects the SQ within LF.
 *
 * All other accesses to this register (e.g. reads and writes) are RAZ/WI.
 */
union bdk_nixx_lf_sq_op_status
{
    uint64_t u;
    struct bdk_nixx_lf_sq_op_status_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_SQ_OP_INT[OP_ERR]. */
        uint64_t reserved_32_62        : 31;
        uint64_t state                 : 12; /**< [ 31: 20](RO/H) Returns NIX_SQ_CTX_S[STATE]. */
        uint64_t sqb_count             : 20; /**< [ 19:  0](RO/H) Returns NIX_SQ_CTX_S[SQB_COUNT]. */
#else /* Word 0 - Little Endian */
        uint64_t sqb_count             : 20; /**< [ 19:  0](RO/H) Returns NIX_SQ_CTX_S[SQB_COUNT]. */
        uint64_t state                 : 12; /**< [ 31: 20](RO/H) Returns NIX_SQ_CTX_S[STATE]. */
        uint64_t reserved_32_62        : 31;
        uint64_t op_err                : 1;  /**< [ 63: 63](RO/H) Operation error. See NIX_LF_SQ_OP_INT[OP_ERR]. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_sq_op_status_s cn; */
};
typedef union bdk_nixx_lf_sq_op_status bdk_nixx_lf_sq_op_status_t;

static inline uint64_t BDK_NIXX_LF_SQ_OP_STATUS(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_SQ_OP_STATUS(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850200400a30ll + 0x100000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_LF_SQ_OP_STATUS", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_LF_SQ_OP_STATUS(a) bdk_nixx_lf_sq_op_status_t
#define bustype_BDK_NIXX_LF_SQ_OP_STATUS(a) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_SQ_OP_STATUS(a) "NIXX_LF_SQ_OP_STATUS"
#define device_bar_BDK_NIXX_LF_SQ_OP_STATUS(a) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_SQ_OP_STATUS(a) (a)
#define arguments_BDK_NIXX_LF_SQ_OP_STATUS(a) (a),-1,-1,-1

/**
 * Register (RVU_PFVF_BAR2) nix#_lf_tx_stat#
 *
 * NIX LF Transmit Statistics Registers
 * The last dimension indicates which statistic, and is enumerated by NIX_STAT_LF_TX_E.
 */
union bdk_nixx_lf_tx_statx
{
    uint64_t u;
    struct bdk_nixx_lf_tx_statx_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_48_63        : 16;
        uint64_t stat                  : 48; /**< [ 47:  0](RO/H) Statistic value. See also NIX_AF_LF()_TX_STAT() for a writable alias of this field. */
#else /* Word 0 - Little Endian */
        uint64_t stat                  : 48; /**< [ 47:  0](RO/H) Statistic value. See also NIX_AF_LF()_TX_STAT() for a writable alias of this field. */
        uint64_t reserved_48_63        : 16;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_lf_tx_statx_s cn; */
};
typedef union bdk_nixx_lf_tx_statx bdk_nixx_lf_tx_statx_t;

static inline uint64_t BDK_NIXX_LF_TX_STATX(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_LF_TX_STATX(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=4)))
        return 0x850200400300ll + 0x100000ll * ((a) & 0x0) + 8ll * ((b) & 0x7);
    __bdk_csr_fatal("NIXX_LF_TX_STATX", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_LF_TX_STATX(a,b) bdk_nixx_lf_tx_statx_t
#define bustype_BDK_NIXX_LF_TX_STATX(a,b) BDK_CSR_TYPE_RVU_PFVF_BAR2
#define basename_BDK_NIXX_LF_TX_STATX(a,b) "NIXX_LF_TX_STATX"
#define device_bar_BDK_NIXX_LF_TX_STATX(a,b) 0x2 /* BAR2 */
#define busnum_BDK_NIXX_LF_TX_STATX(a,b) (a)
#define arguments_BDK_NIXX_LF_TX_STATX(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_priv_af_int_cfg
 *
 * NIX Privileged Admin Function Interrupt Configuration Register
 */
union bdk_nixx_priv_af_int_cfg
{
    uint64_t u;
    struct bdk_nixx_priv_af_int_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_20_63        : 44;
        uint64_t msix_size             : 8;  /**< [ 19: 12](RO) Number of interrupt vectors enumerated by NIX_AF_INT_VEC_E. */
        uint64_t reserved_11           : 1;
        uint64_t msix_offset           : 11; /**< [ 10:  0](R/W) MSI-X offset. Offset of AF interrupt vectors enumerated by
                                                                 NIX_AF_INT_VEC_E in RVU PF(0)'s MSI-X table. This offset is added to each
                                                                 enumerated value to obtain the corresponding MSI-X vector index. The
                                                                 highest enumerated value plus [MSIX_OFFSET] must be less than or equal to
                                                                 RVU_PRIV_PF(0)_MSIX_CFG[PF_MSIXT_SIZEM1]. */
#else /* Word 0 - Little Endian */
        uint64_t msix_offset           : 11; /**< [ 10:  0](R/W) MSI-X offset. Offset of AF interrupt vectors enumerated by
                                                                 NIX_AF_INT_VEC_E in RVU PF(0)'s MSI-X table. This offset is added to each
                                                                 enumerated value to obtain the corresponding MSI-X vector index. The
                                                                 highest enumerated value plus [MSIX_OFFSET] must be less than or equal to
                                                                 RVU_PRIV_PF(0)_MSIX_CFG[PF_MSIXT_SIZEM1]. */
        uint64_t reserved_11           : 1;
        uint64_t msix_size             : 8;  /**< [ 19: 12](RO) Number of interrupt vectors enumerated by NIX_AF_INT_VEC_E. */
        uint64_t reserved_20_63        : 44;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_priv_af_int_cfg_s cn; */
};
typedef union bdk_nixx_priv_af_int_cfg bdk_nixx_priv_af_int_cfg_t;

static inline uint64_t BDK_NIXX_PRIV_AF_INT_CFG(unsigned long a) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_PRIV_AF_INT_CFG(unsigned long a)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && (a==0))
        return 0x850048000000ll + 0x10000000ll * ((a) & 0x0);
    __bdk_csr_fatal("NIXX_PRIV_AF_INT_CFG", 1, a, 0, 0, 0);
}

#define typedef_BDK_NIXX_PRIV_AF_INT_CFG(a) bdk_nixx_priv_af_int_cfg_t
#define bustype_BDK_NIXX_PRIV_AF_INT_CFG(a) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_PRIV_AF_INT_CFG(a) "NIXX_PRIV_AF_INT_CFG"
#define device_bar_BDK_NIXX_PRIV_AF_INT_CFG(a) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_PRIV_AF_INT_CFG(a) (a)
#define arguments_BDK_NIXX_PRIV_AF_INT_CFG(a) (a),-1,-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_priv_lf#_cfg
 *
 * NIX Privileged Local Function Configuration Registers
 * These registers allow each NIX local function (LF) to be provisioned to a VF/PF
 * for RVU. See also NIX_PRIV_PF_CFG_DEBUG.
 *
 * [SLOT] must be zero.
 *
 * Internal:
 * Hardware ignores [SLOT] and always assumes 0x0.
 */
union bdk_nixx_priv_lfx_cfg
{
    uint64_t u;
    struct bdk_nixx_priv_lfx_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t ena                   : 1;  /**< [ 63: 63](R/W) Enable. When set, the LF is enabled and provisioned to the VF/PF slot
                                                                 selected by [PF_FUNC] and [SLOT]. When clear, the LF is not provisioned.

                                                                 LF to slot mapping must be 1-to-1. Thus, each enabled LF must be provisioned
                                                                 to a unique {[PF_FUNC], [SLOT]} combination. */
        uint64_t reserved_24_62        : 39;
        uint64_t pf_func               : 16; /**< [ 23:  8](R/W) RVU VF/PF to which the LF is provisioned. Format defined by RVU_PF_FUNC_S.
                                                                 Interrupts from the LF are delivered to the selected PF/VF. */
        uint64_t slot                  : 8;  /**< [  7:  0](R/W) Slot within the VF/PF selected by [PF_FUNC] to which the LF is
                                                                 provisioned. */
#else /* Word 0 - Little Endian */
        uint64_t slot                  : 8;  /**< [  7:  0](R/W) Slot within the VF/PF selected by [PF_FUNC] to which the LF is
                                                                 provisioned. */
        uint64_t pf_func               : 16; /**< [ 23:  8](R/W) RVU VF/PF to which the LF is provisioned. Format defined by RVU_PF_FUNC_S.
                                                                 Interrupts from the LF are delivered to the selected PF/VF. */
        uint64_t reserved_24_62        : 39;
        uint64_t ena                   : 1;  /**< [ 63: 63](R/W) Enable. When set, the LF is enabled and provisioned to the VF/PF slot
                                                                 selected by [PF_FUNC] and [SLOT]. When clear, the LF is not provisioned.

                                                                 LF to slot mapping must be 1-to-1. Thus, each enabled LF must be provisioned
                                                                 to a unique {[PF_FUNC], [SLOT]} combination. */
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_priv_lfx_cfg_s cn; */
};
typedef union bdk_nixx_priv_lfx_cfg bdk_nixx_priv_lfx_cfg_t;

static inline uint64_t BDK_NIXX_PRIV_LFX_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_PRIV_LFX_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850048000010ll + 0x10000000ll * ((a) & 0x0) + 0x100ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_PRIV_LFX_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_PRIV_LFX_CFG(a,b) bdk_nixx_priv_lfx_cfg_t
#define bustype_BDK_NIXX_PRIV_LFX_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_PRIV_LFX_CFG(a,b) "NIXX_PRIV_LFX_CFG"
#define device_bar_BDK_NIXX_PRIV_LFX_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_PRIV_LFX_CFG(a,b) (a)
#define arguments_BDK_NIXX_PRIV_LFX_CFG(a,b) (a),(b),-1,-1

/**
 * Register (RVU_PF_BAR0) nix#_priv_lf#_int_cfg
 *
 * NIX Privileged LF Interrupt Configuration Registers
 */
union bdk_nixx_priv_lfx_int_cfg
{
    uint64_t u;
    struct bdk_nixx_priv_lfx_int_cfg_s
    {
#if __BYTE_ORDER == __BIG_ENDIAN /* Word 0 - Big Endian */
        uint64_t reserved_20_63        : 44;
        uint64_t msix_size             : 8;  /**< [ 19: 12](RO) Number of interrupt vectors enumerated by NIX_LF_INT_VEC_E. */
        uint64_t reserved_11           : 1;
        uint64_t msix_offset           : 11; /**< [ 10:  0](R/W) MSI-X offset. Offset of LF interrupt vectors enumerated by the block's
                                                                 NIX_LF_INT_VEC_E in the MSI-X table of the corresponding RVU VF/PF (see
                                                                 NIX_PRIV_LF()_CFG[PF_FUNC]). This offset is added to each enumerated value
                                                                 to obtain the corresponding MSI-X vector index. The highest enumerated
                                                                 value plus [MSIX_OFFSET] must be less than or equal to
                                                                 RVU_PRIV_PF()_MSIX_CFG[PF_MSIXT_SIZEM1,VF_MSIXT_SIZEM1]. */
#else /* Word 0 - Little Endian */
        uint64_t msix_offset           : 11; /**< [ 10:  0](R/W) MSI-X offset. Offset of LF interrupt vectors enumerated by the block's
                                                                 NIX_LF_INT_VEC_E in the MSI-X table of the corresponding RVU VF/PF (see
                                                                 NIX_PRIV_LF()_CFG[PF_FUNC]). This offset is added to each enumerated value
                                                                 to obtain the corresponding MSI-X vector index. The highest enumerated
                                                                 value plus [MSIX_OFFSET] must be less than or equal to
                                                                 RVU_PRIV_PF()_MSIX_CFG[PF_MSIXT_SIZEM1,VF_MSIXT_SIZEM1]. */
        uint64_t reserved_11           : 1;
        uint64_t msix_size             : 8;  /**< [ 19: 12](RO) Number of interrupt vectors enumerated by NIX_LF_INT_VEC_E. */
        uint64_t reserved_20_63        : 44;
#endif /* Word 0 - End */
    } s;
    /* struct bdk_nixx_priv_lfx_int_cfg_s cn; */
};
typedef union bdk_nixx_priv_lfx_int_cfg bdk_nixx_priv_lfx_int_cfg_t;

static inline uint64_t BDK_NIXX_PRIV_LFX_INT_CFG(unsigned long a, unsigned long b) __attribute__ ((pure, always_inline));
static inline uint64_t BDK_NIXX_PRIV_LFX_INT_CFG(unsigned long a, unsigned long b)
{
    if (CAVIUM_IS_MODEL(CAVIUM_CN9XXX) && ((a==0) && (b<=127)))
        return 0x850048000020ll + 0x10000000ll * ((a) & 0x0) + 0x100ll * ((b) & 0x7f);
    __bdk_csr_fatal("NIXX_PRIV_LFX_INT_CFG", 2, a, b, 0, 0);
}

#define typedef_BDK_NIXX_PRIV_LFX_INT_CFG(a,b) bdk_nixx_priv_lfx_int_cfg_t
#define bustype_BDK_NIXX_PRIV_LFX_INT_CFG(a,b) BDK_CSR_TYPE_RVU_PF_BAR0
#define basename_BDK_NIXX_PRIV_LFX_INT_CFG(a,b) "NIXX_PRIV_LFX_INT_CFG"
#define device_bar_BDK_NIXX_PRIV_LFX_INT_CFG(a,b) 0x0 /* BAR0 */
#define busnum_BDK_NIXX_PRIV_LFX_INT_CFG(a,b) (a)
#define arguments_BDK_NIXX_PRIV_LFX_INT_CFG(a,b) (a),(b),-1,-1

#endif /* __BDK_CSRS_NIX_H__ */
